# --- START OF REVISED app_utils.py ---
"""
Utilities for the Streamlit Simulation Results Dashboard. (Optimized v2.3.3)
Includes functions for loading data, preparing data structures, and animation.
Focuses on returning raw or minimally processed data structures.
Heavy aggregation/processing for plots is triggered in app.py.
"""

import os
import json
import pandas as pd
import numpy as np
import streamlit as st
from typing import Dict, List, Optional, Tuple, Any, Union
import matplotlib.pyplot as plt
import matplotlib.animation as animation
import logging
from datetime import datetime
import glob # To find summary files and simulation files
import traceback # For detailed error logging
import gc # Garbage collector

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# =============================================================================
# == Data Loading Functions ==
# =============================================================================

@st.cache_data # Cache the folder structure
def load_folder_structure(base_folder: str) -> List[str]:
    """Loads and returns a sorted list of folder names within the base folder."""
    try:
        folders = [f for f in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, f))]
        # Sort by date/time embedded in folder name (YYYYMMDD-HHMM)
        def sort_key(folder_name):
            parts = folder_name.split('-')
            if len(parts) == 2:
                try: return datetime.strptime(f"{parts[0]}{parts[1]}", "%Y%m%d%H%M").timestamp()
                except ValueError: return 0 # Fallback for non-matching format (sorts first)
            return 0 # Fallback for unexpected format

        # Sort descending by time (most recent first)
        folders.sort(key=sort_key, reverse=True)
        logging.info(f"Found {len(folders)} folders in {base_folder}.")
        return folders
    except FileNotFoundError:
        st.error(f"Base results folder '{base_folder}' not found.")
        logging.error(f"Base folder not found: {base_folder}")
        return []
    except Exception as e:
        st.error(f"Error reading folder structure in '{base_folder}': {e}")
        logging.error(f"Error listing folders in {base_folder}: {e}\n{traceback.format_exc()}")
        return []

@st.cache_data # Cache JSON file loading based on path
def load_json_file(file_path: str) -> Optional[Union[Dict, List]]:
    """Loads and returns the content of a JSON file."""
    if not os.path.exists(file_path):
        logging.warning(f"JSON file not found: {file_path}")
        return None
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
            return data
    except json.JSONDecodeError as e:
        logging.error(f"Error decoding JSON file: {file_path}. Error: {e}")
        st.warning(f"Error reading file: {os.path.basename(file_path)}. It might be corrupted or empty.")
        return None
    except Exception as e:
        logging.error(f"Error loading JSON file {file_path}: {e}\n{traceback.format_exc()}")
        st.error(f"An unexpected error occurred while reading {os.path.basename(file_path)}.")
        return None

@st.cache_data # Cache agent state loading based on folder and episode
def load_agent_state_file(folder_path: str, episode_number: int) -> Optional[Dict]:
    """
    Loads the agent state JSON file for a specific episode.
    Expects the format generated by agent_state_manager.py (list-of-dicts for tables).
    """
    file_path = os.path.join(folder_path, f'agent_state_episode_{episode_number}.json')
    if not os.path.exists(file_path):
        logging.warning(f"Agent state file not found for episode {episode_number} at {file_path}")
        return None

    logging.info(f"Attempting to load agent state from: {file_path}")
    agent_state = load_json_file(file_path)

    if isinstance(agent_state, dict):
        # Basic validation for expected top-level keys
        expected_keys = ['episode', 'q_tables', 'visit_counts']
        missing_keys = [key for key in expected_keys if key not in agent_state]
        if missing_keys:
             logging.warning(f"Agent state file {file_path} missing expected keys: {missing_keys}.")
             # Continue if some data is present

        # Validate internal structure (expects lists of dicts for tables)
        valid_structure = True
        for table_key in ['q_tables', 'visit_counts']:
            table_data = agent_state.get(table_key, {})
            if not isinstance(table_data, dict):
                logging.warning(f"Agent state {file_path}: '{table_key}' is not a dictionary.")
                valid_structure = False
                continue
            for gain, data_list in table_data.items():
                if not isinstance(data_list, list):
                    logging.warning(f"Agent state {file_path}: '{table_key}':'{gain}' is not a list.")
                    valid_structure = False
                # Optional: Check if list contains dicts? Can be slow.

        if not valid_structure:
             st.warning(f"Agent state file {os.path.basename(file_path)} has an unexpected internal structure. Comparison might fail.")

        logging.info(f"Successfully loaded agent state for episode {episode_number}.")
        return agent_state
    else:
        logging.error(f"Loaded agent state from {file_path} is not a dictionary.")
        return None


@st.cache_data # Cache summary data based on folder path
def load_summary_data(folder_path: str) -> Optional[pd.DataFrame]:
    """Loads the summary data file (summary.xlsx preferred, summary.csv fallback)."""
    summary_excel_path = os.path.join(folder_path, 'summary.xlsx')
    summary_csv_path = os.path.join(folder_path, 'summary.csv')
    file_to_load = None
    read_func = None
    kwargs = {}

    if os.path.exists(summary_excel_path):
        file_to_load = summary_excel_path
        try:
            pd.read_excel(file_to_load, engine='openpyxl', nrows=0) # Test read capability
            read_func = pd.read_excel
            kwargs = {'engine': 'openpyxl'}
            logging.info(f"Found summary file: {summary_excel_path}")
        except ImportError:
            st.warning("`openpyxl` not installed, cannot read `.xlsx`. Trying `.csv`. Install with: `pip install openpyxl`")
            file_to_load = None # Force fallback
        except Exception as e_test:
             logging.warning(f"Could not test read Excel file {summary_excel_path}: {e_test}. Trying CSV.")
             file_to_load = None # Force fallback

    if file_to_load is None and os.path.exists(summary_csv_path):
        file_to_load = summary_csv_path
        read_func = pd.read_csv
        kwargs = {}
        logging.info(f"Found summary file: {summary_csv_path}")

    if not file_to_load:
        logging.info(f"Summary file (summary.xlsx or summary.csv) not found in {folder_path}")
        return None

    try:
        df_summary = read_func(file_to_load, **kwargs)
        if 'episode' not in df_summary.columns:
            st.error(f"Summary file {os.path.basename(file_to_load)} missing required 'episode' column.")
            logging.error(f"Summary missing 'episode' column: {file_to_load}")
            return None

        # Clean 'episode' column
        initial_len = len(df_summary)
        df_summary['episode'] = pd.to_numeric(df_summary['episode'], errors='coerce')
        df_summary = df_summary.dropna(subset=['episode'])
        df_summary['episode'] = df_summary['episode'].astype('Int64') # Use nullable integer type
        dropped_rows = initial_len - len(df_summary)
        if dropped_rows > 0:
            logging.warning(f"Dropped {dropped_rows} rows from summary due to non-numeric/missing episode numbers.")

        logging.info(f"Loaded and processed summary from {os.path.basename(file_to_load)} ({len(df_summary)} valid episodes).")
        return df_summary

    except Exception as e:
        st.error(f"Error loading or processing summary file {os.path.basename(file_to_load)}: {e}")
        logging.error(f"Failed to load/process summary {file_to_load}: {e}\n{traceback.format_exc()}")
        return None


@st.cache_data # Cache metadata based on folder path
def load_metadata(folder_path: str) -> Optional[Dict]:
    """Loads the metadata.json file."""
    metadata_path = os.path.join(folder_path, 'metadata.json')
    metadata = load_json_file(metadata_path) # Uses the cached JSON loader
    if metadata:
        logging.info(f"Loaded metadata from {metadata_path}")
    else:
        # Warning/error is handled by load_json_file
        logging.warning(f"Metadata file not found or failed load: {metadata_path}")
    return metadata

# Use a tuple for selected_episode_numbers for cache key stability
@st.cache_data(max_entries=10) # Cache recent trajectory loads
def load_selected_episodes(folder_path: str, selected_episode_numbers_tuple: Tuple[int, ...]) -> Dict[str, List[Dict]]:
    """
    Loads trajectory data ONLY for selected episodes from simulation_data_*.json files.
    Input `selected_episode_numbers_tuple` must be a tuple for caching.
    """
    episode_set_to_load = set(selected_episode_numbers_tuple)
    loaded_data: Dict[str, List[Dict]] = {'simulation_data': []}

    if not episode_set_to_load or not folder_path:
        return loaded_data

    simulation_files_pattern = os.path.join(folder_path, 'simulation_data_*.json')
    potential_files = sorted(glob.glob(simulation_files_pattern))

    if not potential_files:
        logging.warning(f"No simulation data files found matching pattern in {folder_path}")
        return loaded_data

    files_to_read = []
    # Identify which files *might* contain the required episodes
    for file_path in potential_files:
        filename = os.path.basename(file_path)
        try:
            # simulation_data_100_to_199.json
            parts = filename.replace('simulation_data_', '').replace('.json', '').split('_to_')
            if len(parts) == 2:
                start_ep = int(parts[0])
                end_ep = int(parts[1])
                # Check if the file range intersects with the requested episodes
                if any(start_ep <= ep_num <= end_ep for ep_num in episode_set_to_load):
                    files_to_read.append(file_path)
            else:
                # If parsing fails, assume it might contain data (less efficient but safe)
                logging.debug(f"Could not parse episode range from filename '{filename}'. Adding to read list.")
                files_to_read.append(file_path)
        except (ValueError, IndexError):
            logging.warning(f"Error parsing episode range from filename '{filename}'. Adding to read list.")
            files_to_read.append(file_path)

    if not files_to_read:
        logging.warning(f"No data files found potentially covering episodes: {sorted(list(episode_set_to_load))}")
        return loaded_data

    logging.info(f"Identified {len(files_to_read)} data file(s) to check for {len(episode_set_to_load)} requested episodes.")

    episodes_actually_found = set()
    # Read identified files and filter episodes
    # Note: load_json_file is cached, so reading the same file multiple times is cheap
    for file_path in files_to_read:
        file_content = load_json_file(file_path)
        if isinstance(file_content, list):
            for episode_data in file_content:
                if isinstance(episode_data, dict):
                    try:
                        ep_number = int(episode_data.get('episode', -1))
                        if ep_number in episode_set_to_load and ep_number not in episodes_actually_found:
                            # Minimal processing: Ensure basic structure if needed later
                            processed_episode = {'episode': ep_number}
                            for key, value in episode_data.items():
                                # Store lists/arrays directly, convert others safely
                                if isinstance(value, (list, np.ndarray)):
                                    processed_episode[key] = value
                                elif isinstance(value, (int, float, bool, str, np.number)):
                                     processed_episode[key] = value
                                # Skip complex nested objects like raw qtables if they exist here
                            loaded_data['simulation_data'].append(processed_episode)
                            episodes_actually_found.add(ep_number)
                    except (ValueError, TypeError):
                        logging.debug(f"Skipping record in {os.path.basename(file_path)} due to invalid episode number.")
                else:
                     logging.debug(f"Skipping non-dict item in {os.path.basename(file_path)}.")
        # else: # Error loading file already logged by load_json_file

    total_episodes_loaded = len(loaded_data['simulation_data'])
    logging.info(f"Finished loading. Found data for {total_episodes_loaded} episodes.")

    # Sort the loaded data by episode number
    if total_episodes_loaded > 0:
        loaded_data['simulation_data'].sort(key=lambda x: x.get('episode', -1))

    # Report missing episodes (optional, can be done in UI)
    episodes_not_found = episode_set_to_load - episodes_actually_found
    if episodes_not_found:
        logging.warning(f"Could not find data for {len(episodes_not_found)} requested episodes: {sorted(list(episodes_not_found))[:20]}...")

    return loaded_data


# =============================================================================
# == Data Preparation / Extraction Functions ==
# =============================================================================

def extract_trajectory_data(episode_data: Dict, param_name: str) -> Optional[np.ndarray]:
    """
    Safely extracts, converts to numeric (float64), and returns a specific
    trajectory parameter from episode data as a NumPy array. Returns None if
    data is missing, not list/array, or cannot be converted.
    """
    if not isinstance(episode_data, dict): return None
    raw_data = episode_data.get(param_name)

    if not isinstance(raw_data, (list, np.ndarray)) or len(raw_data) == 0:
        # logging.debug(f"Trajectory data missing or empty for parameter '{param_name}' in episode {episode_data.get('episode', 'N/A')}")
        return None

    try:
        # Convert to numeric, coercing errors to NaN, ensure float64 for consistency
        numeric_data = pd.to_numeric(raw_data, errors='coerce').astype(np.float64)
        # If all values become NaN after conversion, treat as invalid
        # if np.isnan(numeric_data).all():
        #     logging.debug(f"All values became NaN for parameter '{param_name}' in episode {episode_data.get('episode', 'N/A')}")
        #     return None
        return numeric_data
    except Exception as e:
        logging.warning(f"Error converting trajectory data for parameter '{param_name}' in episode {episode_data.get('episode', 'N/A')}: {e}")
        return None

def align_trajectory_data(x_data: Optional[np.ndarray], y_data: Optional[np.ndarray]) -> Optional[Tuple[np.ndarray, np.ndarray]]:
    """
    Aligns two NumPy arrays (trajectory data) to the same length (shortest)
    and filters out pairs where either value is NaN.
    Returns a tuple (aligned_x, aligned_y) or None if alignment fails or results in empty arrays.
    """
    if x_data is None or y_data is None:
        # logging.debug("Alignment failed: Input data is None.")
        return None

    min_len = min(len(x_data), len(y_data))
    if min_len == 0:
        # logging.debug("Alignment failed: Input data has zero length.")
        return None

    x_aligned = x_data[:min_len]
    y_aligned = y_data[:min_len]

    # Create a mask for valid (finite) pairs
    valid_mask = np.isfinite(x_aligned) & np.isfinite(y_aligned)

    x_final = x_aligned[valid_mask]
    y_final = y_aligned[valid_mask]

    if len(x_final) == 0:
        # logging.debug("Alignment resulted in zero valid data points after NaN filtering.")
        return None

    return x_final, y_final


def extract_heatmap_xy_data(episode_data_list: List[Dict], x_param: str, y_param: str) -> Optional[Tuple[np.ndarray, np.ndarray]]:
    """
    Extracts and concatenates data for specific X and Y variables from a list
    of episodes, handling alignment and NaN filtering internally using helpers.

    Args:
        episode_data_list: List of dictionaries, each an episode.
        x_param: Name of the variable for the X-axis.
        y_param: Name of the variable for the Y-axis.

    Returns:
        Tuple (x_data_concatenated, y_data_concatenated), or None if error or no valid data.
    """
    if not episode_data_list or not x_param or not y_param:
        logging.warning("Heatmap extraction called with empty data or missing params.")
        return None

    all_x_aligned: List[np.ndarray] = []
    all_y_aligned: List[np.ndarray] = []

    logging.info(f"Extracting heatmap data for X='{x_param}', Y='{y_param}' from {len(episode_data_list)} episodes.")

    for idx, episode in enumerate(episode_data_list):
        ep_num = episode.get('episode', f"Index {idx}")
        x_data_raw = extract_trajectory_data(episode, x_param)
        y_data_raw = extract_trajectory_data(episode, y_param)

        aligned_pair = align_trajectory_data(x_data_raw, y_data_raw)

        if aligned_pair:
            all_x_aligned.append(aligned_pair[0])
            all_y_aligned.append(aligned_pair[1])
        # else: # Debug message handled within align_trajectory_data or extract_trajectory_data
            # logging.debug(f"No valid aligned data for X='{x_param}', Y='{y_param}' in episode {ep_num}.")

    if not all_x_aligned: # Check if the list is empty
        logging.warning(f"No valid data points found across all episodes for heatmap X='{x_param}', Y='{y_param}'.")
        return None

    try:
        x_concat = np.concatenate(all_x_aligned)
        y_concat = np.concatenate(all_y_aligned)
        logging.info(f"Successfully extracted and concatenated data for heatmap: X shape {x_concat.shape}, Y shape {y_concat.shape}")
        if len(x_concat) == 0:
             logging.warning("Concatenation resulted in empty arrays for heatmap.")
             return None
        return x_concat, y_concat
    except Exception as e:
        logging.error(f"Error concatenating extracted heatmap data: {e}", exc_info=True)
        return None

# =============================================================================
# == Styling & Helpers ==
# =============================================================================
def resaltar_maximo(s: pd.Series) -> List[str]:
    """Applies background style to the maximum value in a Pandas Series (numeric only)."""
    if pd.api.types.is_numeric_dtype(s):
        try:
            max_val = s.max()
            # Handle cases where max_val might be NaN (e.g., all NaNs in series)
            if pd.isna(max_val):
                return [''] * len(s)
            # Check for non-NaN values before comparison
            is_max = (s == max_val) & (~s.isna())
            return ['background-color: lightgreen' if v else '' for v in is_max]
        except Exception:
            # Fallback for any unexpected errors during max calculation/comparison
            return [''] * len(s)
    else:
        # Return empty style for non-numeric columns
        return [''] * len(s)

# =============================================================================
# == Animation Class ==
# =============================================================================
class PendulumAnimator:
    """Handles the creation and display of the pendulum animation."""

    def __init__(self, episode_data: Dict, anim_config: Dict):
        """
        Initializes the animator.

        Args:
            episode_data: Dictionary containing the trajectory data for one episode.
                          Must include 'time', 'cart_position', 'pendulum_angle'.
            anim_config: Dictionary with animation settings (fps, speed, limits, etc.).
        """
        self.episode_data = episode_data
        # Merge provided config with defaults
        self.config = {
            'fps': 30, 'speed': 1.0, 'x_lim': [-5, 5], 'y_lim': [-3, 3],
            'dpi': 100, 'pendulum_length': 2.0, **anim_config
        }
        self.fig, self.ax = plt.subplots(figsize=(10, 6)) # Smaller default?

        # Check for essential data immediately
        essential_keys = ['time', 'cart_position', 'pendulum_angle']
        missing_keys = [k for k in essential_keys if extract_trajectory_data(episode_data, k) is None]
        if missing_keys:
            plt.close(self.fig) # Close the figure if data is missing
            raise ValueError(f"Animator missing essential data: {', '.join(missing_keys)}")

        # Extract and store essential data (already validated by extract_trajectory_data)
        self.time_data = extract_trajectory_data(episode_data, 'time')
        self.cart_pos_data = extract_trajectory_data(episode_data, 'cart_position')
        self.pendulum_angle_data = extract_trajectory_data(episode_data, 'pendulum_angle')

        # Align all essential data to the shortest length and filter NaNs pairwise (implicitly)
        min_len = min(len(self.time_data), len(self.cart_pos_data), len(self.pendulum_angle_data))
        self.time_data = self.time_data[:min_len]
        self.cart_pos_data = self.cart_pos_data[:min_len]
        self.pendulum_angle_data = self.pendulum_angle_data[:min_len]

        valid_mask = np.isfinite(self.time_data) & np.isfinite(self.cart_pos_data) & np.isfinite(self.pendulum_angle_data)
        self.time_data = self.time_data[valid_mask]
        self.cart_pos_data = self.cart_pos_data[valid_mask]
        self.pendulum_angle_data = self.pendulum_angle_data[valid_mask]

        self.num_frames = len(self.time_data)
        if self.num_frames == 0:
             plt.close(self.fig)
             raise ValueError("No valid, aligned data frames for animation after processing.")

        # Store optional data, aligned to the number of valid frames
        self.optional_data = {}
        optional_fields_fmt = {
             'force': '.2f', 'error': '.4f', 'reward': '.3f', 'kp': '.2f', 'ki': '.2f', 'kd': '.2f',
             'pendulum_velocity': '.4f', 'cart_velocity': '.3f', 'epsilon': '.4f', 'learning_rate': '.4f',
             'action_kp': '.0f', 'action_ki': '.0f', 'action_kd': '.0f'
        }
        self.optional_formats = optional_fields_fmt
        for field in optional_fields_fmt:
             data = extract_trajectory_data(episode_data, field)
             if data is not None and len(data) >= min_len:
                 # Align with essential data mask *before* storing
                 aligned_optional_data = data[:min_len][valid_mask]
                 if len(aligned_optional_data) == self.num_frames:
                     self.optional_data[field] = aligned_optional_data
                 else:
                      logging.warning(f"Optional data '{field}' length mismatch after masking for animation.")

        # Setup plot elements
        self.cart_width, self.cart_height = 0.5, 0.25
        self.pendulum_length = self.config.get('pendulum_length', 2.0)
        self.cart: Optional[plt.Rectangle] = None
        self.pendulum: Optional[plt.Line2D] = None
        self.time_text: Optional[plt.Text] = None
        self.info_text: Optional[plt.Text] = None

        self._setup_plot_elements()

    def _setup_plot_elements(self):
        """Sets up the static elements of the animation plot."""
        try:
            self.ax.set_xlim(self.config['x_lim'])
            self.ax.set_ylim(self.config['y_lim'])
            self.ax.set_aspect('equal', adjustable='box')
            self.ax.grid(True, linestyle='--', alpha=0.6)

            # Create plot objects
            initial_x = self.cart_pos_data[0] if self.num_frames > 0 else 0
            self.cart = self.ax.add_patch(
                plt.Rectangle((initial_x - self.cart_width / 2, -self.cart_height / 2),
                              self.cart_width, self.cart_height,
                              fc='cornflowerblue', ec='black', zorder=3)
            )
            self.pendulum, = self.ax.plot([], [], 'o-', color='black', lw=2, markersize=6, markerfacecolor='grey', zorder=2) # Comma unpacks line object
            self.time_text = self.ax.text(0.02, 0.95, '', transform=self.ax.transAxes, fontsize=10, va='top', ha='left')
            self.info_text = self.ax.text(0.98, 0.95, '', transform=self.ax.transAxes, fontsize=9, va='top', ha='right', linespacing=1.3)

            self.ax.set_xlabel('Cart Position (m)')
            self.ax.set_ylabel('Height (m)')
            ep_num = self.episode_data.get('episode', 'N/A')
            self.ax.set_title(f'Cart-Pendulum Simulation - Episode {ep_num}')
            self.fig.tight_layout(pad=0.5) # Adjust padding
            logging.debug("Animation plot elements setup complete.")

        except Exception as e:
            logging.error(f"Error setting up animation plot elements: {e}\n{traceback.format_exc()}")
            if self.fig: plt.close(self.fig) # Ensure figure is closed on error
            raise # Re-raise the exception

    def _init_animation(self) -> Tuple[plt.Artist, ...]:
        """Initializes the plot elements for the animation start."""
        # Reset pendulum line
        self.pendulum.set_data([], [])
        # Reset cart position (redundant if _setup_plot_elements uses first frame, but safe)
        initial_x = self.cart_pos_data[0] if self.num_frames > 0 else 0
        self.cart.set_xy((initial_x - self.cart_width / 2, -self.cart_height / 2))
        # Clear text
        self.time_text.set_text('')
        self.info_text.set_text('')
        # Return all animated artists
        return self.pendulum, self.cart, self.time_text, self.info_text

    def _animate_frame(self, i: int) -> Tuple[plt.Artist, ...]:
        """Updates the plot elements for a specific frame `i`."""
        if i >= self.num_frames: # Safety check
            return self.pendulum, self.cart, self.time_text, self.info_text

        try:
            # Get data for the current frame (already validated and aligned)
            t = self.time_data[i]
            x = self.cart_pos_data[i]
            theta_rad = self.pendulum_angle_data[i] # Assuming angle is in radians

            # Update cart position
            cart_x_base = x - self.cart_width / 2
            self.cart.set_xy((cart_x_base, -self.cart_height / 2))

            # Update pendulum position
            pendulum_pivot_x = x
            pendulum_pivot_y = 0 # Assuming pivot is at y=0 on the cart
            pendulum_end_x = pendulum_pivot_x + self.pendulum_length * np.sin(theta_rad)
            pendulum_end_y = pendulum_pivot_y + self.pendulum_length * np.cos(theta_rad) # cos for vertical component from top
            self.pendulum.set_data([pendulum_pivot_x, pendulum_end_x], [pendulum_pivot_y, pendulum_end_y])

            # Update time text
            self.time_text.set_text(f'Time: {t:.3f} s')

            # Update info text with optional data
            info_items = []
            for field, fmt in self.optional_formats.items():
                if field in self.optional_data:
                    value = self.optional_data[field][i]
                    if np.isfinite(value): # Check if value is finite for this frame
                         try: info_items.append(f"{field.replace('_', ' ').title()}: {value:{fmt}}")
                         except (ValueError, TypeError): info_items.append(f"{field.replace('_', ' ').title()}: {value}") # Fallback format

            self.info_text.set_text("\n".join(info_items))

            # Return all animated artists
            return self.pendulum, self.cart, self.time_text, self.info_text

        except Exception as e:
            logging.error(f"Error animating frame {i}: {e}\n{traceback.format_exc()}")
            # Return current state without crashing
            return self.pendulum, self.cart, self.time_text, self.info_text

    def create_animation(self) -> Optional[animation.FuncAnimation]:
        """Creates the FuncAnimation object."""
        if self.num_frames == 0:
            st.error("Cannot create animation: No valid data frames.")
            logging.error("Animation creation failed due to zero valid frames.")
            return None

        fps = self.config.get('fps', 30)
        speed = self.config.get('speed', 1.0)
        # Ensure interval is at least 1ms
        interval_ms = max(1, int(1000 / (fps * speed)))

        logging.info(f"Creating animation: {self.num_frames} frames, interval={interval_ms}ms (fps={fps}, speed={speed}).")

        try:
            # Create the animation object
            anim = animation.FuncAnimation(
                fig=self.fig,
                func=self._animate_frame,
                frames=self.num_frames,
                init_func=self._init_animation,
                blit=True,          # Use blitting for performance
                interval=interval_ms,
                repeat=False        # Do not repeat the animation
            )
            logging.info("FuncAnimation object created successfully.")
            return anim

        except Exception as e:
            st.error(f"Failed to create animation object: {e}")
            logging.error(f"FuncAnimation creation failed: {e}\n{traceback.format_exc()}")
            if self.fig: plt.close(self.fig) # Clean up figure on error
            return None

    def close_figure(self):
         """Closes the matplotlib figure to release memory."""
         if self.fig and plt.fignum_exists(self.fig.number):
             plt.close(self.fig)
             logging.debug(f"Closed animation figure (Number: {self.fig.number}).")
             gc.collect() # Explicit garbage collection

