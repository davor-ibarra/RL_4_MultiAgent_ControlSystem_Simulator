# ========================================
# Environment Configuration
# ========================================
environment:
  type: pendulum_environment
  dt: 0.001                      # Simulation time step (s)
  total_time: 5.0                # Total simulation time per episode (s)
  decision_interval: 0.01        # Time between agent decisions (s) - must be multiple of dt
  max_episodes: 10000            # Maximum number of episodes to run
  results_folder: 'results_history' # Base folder for saving results

  # --- Dynamic System ---
  system:
    type: inverted_pendulum
    params:
      m1: 5.0                    # Mass of the cart (kg)
      m2: 1.0                    # Mass of the pendulum bob (kg)
      l: 2.0                     # Length of the pendulum rod (m)
      g: 9.81                    # Acceleration due to gravity (m/s^2)
      cr: 0.0                    # Damping coefficient of the cart (friction)
      ca: 0.0                    # Damping coefficient of the pendulum pivot (friction)

  # --- Controller ---
  controller:
    type: pid
    params:
      kp: 10.0                   # Initial Proportional gain
      ki: 0.0                    # Initial Integral gain
      kd: 0.0                    # Initial Derivative gain
      setpoint: 0.0              # Target value for the controlled variable (pendulum angle = 0 rad)

  # --- Agent ---
  agent:
    type: pid_qlearning
    params:
      num_actions: 3             # Number of actions per gain: 0=decrease, 1=maintain, 2=increase
      discount_factor: 0.98      # Q-learning discount factor (gamma)
      # Epsilon (Exploration Rate) Settings
      epsilon: 1.0               # Initial exploration rate
      epsilon_decay: 0.9997122  # Decay factor per episode | 0.9997122 (8000)| 0.999846506108527 (15000)
      epsilon_min: 0.1           # Minimum exploration rate
      use_epsilon_decay: true    # Enable/disable epsilon decay
      # Learning Rate (Alpha) Settings
      learning_rate: 1.0         # Initial learning rate for Q-updates
      learning_rate_decay: 0.999425 # Decay factor per episode | 0.999425 (8000) | 0.999693035777428 (15000)
      learning_rate_min: 0.01    # Minimum learning rate
      use_learning_rate_decay: true # Enable/disable learning rate decay
      # Agent State Discretization Configuration
      state_config:
        # Base Gain State Variables
        kp:                      {enabled: true, min: 0.0, max: 100.0, bins: 21, state_vars: ['ki', 'kd']} # state_vars: ['angle','angular_velocity','ki', etc.]
        ki:                      {enabled: false, min: 0.0, max: 100.0, bins: 21, state_vars: []}
        kd:                      {enabled: false, min: 0.0, max: 100.0, bins: 21, state_vars: []}
        # Additional State Variables
        angle:                   {enabled: false, min: -0.03491, max: 0.03491, bins: 3}
        angular_velocity:        {enabled: false, min: -0.5, max: 0.5, bins: 3}
        cart_position:           {enabled: false, min: -1.0, max: 1.0, bins: 3}
        cart_velocity:           {enabled: false, min: -1.0, max: 1.0, bins: 3}

  # --- Reward Configuration ---
  reward_setup:

    # --- 1. Agent Learning Strategy Configuration ---
    # Defines how the reward R_learn used for the agent's update is derived.
    learning_strategy:
      type: echo_baseline # Type of learning strategy. Options: 'global', 'shadow_baseline', 'echo_baseline'
      # Parameters specific to the chosen strategy:
      strategy_params:
        global: {}
        echo_baseline: {}
        shadow_baseline: # Used if type is 'shadow_baseline'
          beta: 0.2 # Learning rate for the baseline B(s) update
          baseline_init_value: 0.0 # Initial value for B(s) entries

    # --- 2. Reward Calculation Configuration ---
    # Defines how the instantaneous reward R(s,a,s') is calculated per step.
    calculation:
      method: gaussian # Method for reward calculation. Options: 'gaussian', 'stability_calculator'
      gaussian_params: # Used if method is 'gaussian'
        weights:
          angle: 1.0
          angular_velocity: 0.5
          cart_position: 0.0
          cart_velocity: 0.5
          force: 0.5
          time: 0.0
        scales:
          angle: 0.1
          angular_velocity: 0.1
          cart_position: 1.0
          cart_velocity: 1.0
          force: 10.0
          time: 1.0
      # Note: If method is 'stability_calculator', it uses the 'lambda' parameter from the *enabled* stability_calculator below.

    # --- 3. Stability Score (w_stab) Configuration ---
    # Defines how the instantaneous stability score (w_stab) is calculated.
      stability_calculator:
        enabled: true # Calculate w_stab? (Required if learning_strategy.type is 'shadow_baseline')
        type: simple_exponential # Type of stability calculator. Options: 'ira_instantaneous', 'simple_exponential'
        simple_exponential_params: # Used if type is 'simple_exponential'
        # Note: This calculator does not have a separate lambda for reward. If calculation.method is 'stability_calculator' with this type, the reward will be equal to w_stab.
          lambda_weights: # Used for w_stab calculation
              angle: 1.0
              angular_velocity: 1.0
              cart_position: 0.0
              cart_velocity: 0.5
          scales: # Used for w_stab calculation
              angle: 0.1
              angular_velocity: 0.1
              cart_position: 1.0
              cart_velocity: 1.0
        ira_params: # Used if type is 'ira_instantaneous'
          lambda: 1.0 # lambda: Sensitivity factor ONLY used if calculation.method is 'stability_calculator'
          weights:
            angle: 0.1
            angular_velocity: 0.1
            cart_position: 0.0
            cart_velocity: 0.0
          z_score_epsilon: 1.0e-6
          adaptive_stats:
            enabled: false
            min_episode: 1000
            min_sigma: 1.0e-4
          initial_reference_stats:
            angle:            { mu: 0.0, sigma: 0.05 }
            angular_velocity: { mu: 0.0, sigma: 0.05 }
            cart_position:    { mu: 0.0, sigma: 0.5 }
            cart_velocity:    { mu: 0.0, sigma: 0.5 }

  # --- End of Reward Configuration ---

# ========================================
# Simulation Control & Termination
# ========================================
simulation:
  episodes_per_file: 250
  use_cart_limit: true
  cart_limit: 5.0
  use_angle_limit: true
  angle_limit: 1.0472
  save_agent_state: true
  agent_state_save_frequency: 2000

# ========================================
# Initial Conditions for Episodes
# ========================================
initial_conditions:
  x0: [0.0, 0.0, 0.0349066, 0.0] # 0.0349066(2°) 0.0873 (5°) 0.1746 (10°)

# ========================================
# PID Adaptation Specifics
# ========================================
pid_adaptation:
  gain_step: 5.0
  # gain_step: {kp: 5.0, ki: 1.0, kd: 0.5} # Example if variable_step=true
  variable_step: false
  reset_gains_each_episode: true

# ========================================
# Stabilization Criteria
# ========================================
stabilization_criteria:
  angle_threshold: 0.001
  velocity_threshold: 0.005

# ========================================
# Visualization Settings
# ========================================
visualization:
  enabled: true
  config_file: sub_config_visualization.yaml

# ========================================
# Logging Configuration
# ========================================
logging:
  log_to_file: false                 # Enable/disable logging to file
  filename: log_simulation_run.log      # Base name for the log file (will be placed in results folder)
  # List of levels to include in the file (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL)
  # The lowest level specified determines the overall minimum level processed.
  levels: #[DEBUG, INFO, ERROR, CRITICAL]
  # How often (in episodes) to save the current log buffer to the file and clear the buffer
  # Set to 0 or negative to only save at the very end.
  log_save_frequency: 100