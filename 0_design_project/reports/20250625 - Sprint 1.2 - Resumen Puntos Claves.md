# **Puntos Clave de Integración entre Componentes Principales**

Este mapa de dependencias resalta los "enchufes" y "contratos" cruciales entre los componentes de simulación.
## **SimulationManager**

Orquesta el ciclo de vida de los episodios.
- **Puntos de Conexión Cruciales:**
    - **Llama a env.reset():** Inicia cada episodio, pasando las condiciones iniciales desde el config.
    - **Llama a agent.build_agent_state():** Pide al agente que construya su estado (S) a partir del estado del entorno.
    - **Llama a agent.select_action():** Pide al agente que elija una acción (A) basada en S.
    - **Llama a _apply_actions_to_controller():** Modifica el Controller con los deltas de ganancia de la acción A.
    - **Orquesta el Intervalo:**
        - Llama a env.step() repetidamente para avanzar la simulación. Recibe (next_state_vec, reward, stability, info).
        - **Si reward_strategy.needs_virtual_simulation es True:** Llama a virtual_sim.run_interval_simulation() para obtener recompensas contrafactuales.
    - **Construye reward_info (dict):** Empaqueta la recompensa real (interval_reward), la estabilidad (avg_stability) y, opcionalmente, las recompensas diferenciales (differential_rewards) en un único diccionario.
    - **Llama a agent.learn():** Pasa la transición completa (S, A, reward_info, S') para que el agente aprenda.
    - **Llama a env.check_termination() y agent.should_episode_terminate_early():** Decide si el episodio debe terminar.

## **Environment**

Abstrae la física y la interacción directa, sirviendo de puente.
- **Puntos de Conexión Cruciales:**
    - **Método _create_state_dict(state_vector):** **CONTRATO FUNDAMENTAL.** Traduce el vector de estado interno y crudo del sistema dinámico a un diccionario con claves nombradas ({'angle': 0.1, ...}). Este diccionario es la unidad de comunicación para todos los componentes de análisis.
    - **Método step():**
        - Llama a controller.compute_action(), pasándole el vector de estado crudo.
        - Llama a system.apply_action(), pasando el estado y la acción del controlador.
        - Llama a reward_function.calculate(), pasándole el state_dict, la acción, next_state_dict y contexto temporal.
        - Llama a stability_calculator.calculate_instantaneous_stability(), pasándole el next_state_dict.
    - **Método reset():**
        - Llama a system.reset().
        - Llama a controller.reset_policy().
        - Llama a agent.reset_agent().
        - Llama a reward_function.reset().

#### **RLAgent

Encapsula la lógica de aprendizaje y decisión.
- **Puntos de Conexión Cruciales:**
    - **Método build_agent_state(env_state_dict, controller):** Su única responsabilidad es tomar el diccionario de estado del entorno y los parámetros del controlador para construir el diccionario de estado que usará internamente para sus tablas Q.
    - **Método learn(s, a, r_info, s_prime, ...):**
        - Llama a self.reward_strategy.compute_reward_for_learning() para obtener la R_learn. Este es el punto de desacoplamiento de la estrategia de recompensa. Le pasa todo el contexto necesario (self, controller, s, a, etc.).
        - Actualiza sus tablas Q (self.q_tables) usando la R_learn calculada.
    - **Método set_reward_strategy(strategy):** Permite la inyección tardía de la estrategia para romper dependencias circulares y es donde se inicializan las tablas auxiliares que la estrategia pueda requerir.
    - **Métodos de Tabla Auxiliar (get/update_auxiliary_table_value)**: Permiten a una RewardStrategy (como ShadowBaseline) leer y escribir en el estado interno del agente (ej. tablas de baseline) de forma controlada.

#### **RewardStrategy (ej. ShadowBaseline, EchoBaseline)**

Define CÓMO se interpreta la recompensa del entorno para el aprendizaje.
- **Puntos de Conexión Cruciales:**
    - **Atributo needs_virtual_simulation: bool:** Informa al SimulationManager si debe ejecutar simulaciones contrafactuales.
    - **Método compute_reward_for_learning(...):** Es su única función. Recibe el contexto completo del paso de aprendizaje y devuelve un único valor flotante: R_learn.
        - Puede llamar a agent.get_auxiliary_table_value() y agent.update_auxiliary_table_value() si necesita interactuar con el estado interno del agente (ej. leer/escribir un baseline).

#### **RewardFunction

Define QUÉ es la recompensa en un instante de tiempo.
- **Puntos de Conexión Cruciales:**
    - **Método calculate(state_dict, ...):** Recibe el estado contextualizado del entorno y calcula la recompensa instantánea R_real. Es agnóstica a cómo se usará esta recompensa.
    - **En el __init__:** Almacena una instancia de stability_calculator para poder llamarla si su método de cálculo se basa en la estabilidad.

#### **StabilityCalculator

Define QUÉ es la estabilidad en un instante de tiempo.
- **Puntos de Conexión Cruciales:**
    - **Método calculate_instantaneous_stability(state_dict):** Su función principal. Recibe el estado contextualizado del entorno y devuelve una métrica de estabilidad w_stab.