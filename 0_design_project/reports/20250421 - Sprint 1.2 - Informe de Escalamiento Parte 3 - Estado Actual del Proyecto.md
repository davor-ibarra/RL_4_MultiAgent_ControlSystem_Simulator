---
created: 20250414 06:04
update: 20250423-16:54
summary: 
status: done
link: 
tags:
  - content
---
		## Documento de Actualización del Proyecto **(v5.1.0_DI)**

**1. Objetivo del Sprint 1.2 (re‑afirmado)**  
Implementar recompensas diferenciales (`echo‑baseline`, `shadow‑baseline`), **_y_** migrar la orquestación a un contenedor de Inyección de Dependencias para aislar módulos, facilitar pruebas y preparar la futura extensibilidad del framework.

**2. Cambios clave desde v3.0.0 → v5.1.0_DI**

- **Contenedor DI** (`di_container.py`) con *providers* singleton/transient y detección de ciclos.  
- **main.py** reducido a **orquestación**, delegando creación de componentes al contenedor.  
- **Factorías ampliadas**: ahora inyectan `RewardStrategy`, `BaseStabilityCalculator`, `VirtualSimulator` y registran pasos `dt`, `gain_step`, `variable_step`.  
- **`SimulationManager`** consume dependencias desde el contenedor; manejo explícito de _logging flush_, batches y guardado incremental.  
- **`ResultHandler`** ya no mantiene estado global; todo método recibe `results_folder`.  
- **`HeatmapGenerator`** desacoplado; búsqueda automática del último `simulation_data_*.json`.  
- **Interfaces nuevas** (`RewardStrategy`, `BaseStabilityCalculator`) y **implementaciones concretas** (`Global`, `ShadowBaseline`, `EchoBaseline`, `IRA`, `SimpleExponential`).  
- **`PIDQLearningAgent`** ahora **inyecta** `RewardStrategy`; internamente resuelve `R_learn` mediante `compute_reward_for_learning`.  
- **Logging configurable por YAML** (`logging_configurator.py`), con filtro dinámico de niveles a archivo.

**3. Arquitectura y Componentes (snapshot)**  

```text
RL-Agent-Factory/
├── main.py                    # Orquestador + DI bootstrap
├── config_loader.py           # Configuraciones simulación
├── logging_configurator.py    # Configuraciones Logging
├── di_container.py            # Contenedor de dependencias
├── result_handler.py          # Persistencia de artefactos
├── simulation_manager.py      # Bucle de entrenamiento
├── heatmap_generator.py       # Datos para heatmaps
├── visualization_runner.py    # Contenedor de dependencias
│
├── components/
│   ├── agents/
│   │   └── pid_qlearning_agent.py
│   ├── analysis/
│	│	├── extended_metrics_collector.py
│	│	├── ira_stability_calculator.py
│	│	├── simple_exponential_stability_calculator.py
│   ├── controllers/
│   │   └── pid_controller.py
│   ├── environments/
│   │   └── pendulum_environment.py
│   ├── rewards/
│   │   └── gaussian_reward.py
│   ├── reward_strategies/
│   │   ├── global_reward_strategy.py
│   │   ├── shadow_baseline_reward_strategy.py
│   │   └── echo_baseline_reward_strategy.py
│   ├── simulators/
│   │    └── pendulum_virtual_simulator.py
│   └── systems/
│       └── inverted_pendulum_system.py
│
├── factories/
│   ├── environment_factory.py        
│   ├── reward_factory.py
│   └── agent_factory.py
│   ├── controller_factory.py 
│   └── system_factory.py 
│
├── interfaces/
│   ├── controller.py
│   ├── dynamic_system.py
│   ├── environment.py
│   ├── metrics_collector.py
│   ├── rl_agent.py
│   ├── reward_function.py
│   ├── reward_strategy.py
│   ├── stability_calculator.py
│   └── virtual_simulator.py
│
├── utils/
│   ├── data_processing.py
│   ├── numpy_encoder.py
│   └── plot_generator.py
```

**4. Estado actual y funcionalidad**

| **Funcionalidad**                              | **Estado**                                          |
| ---------------------------------------------- | --------------------------------------------------- |
| Modo global                                    | ✅ estable                                           |
| Modo shadow‑baseline                           | ✅ estable<br>(B(s) actualizable con β configurable) |
| Modo echo‑baseline                             | ✅ funcional; sobrecarga ≈ +50 % CPU                 |
| Guardado incremental de episodios              | ✅ (episodes_per_file)                               |
| heatmap_generator / visualization_runner       | ✅ scripts independientes                            |
| Carga de configuración & validación de Q‑table | ✅ con warnings de tamaño                            |


**5. Puntos clave y riesgos**

- Sobre‑carga en Echo Mitigación: parámetro episodes_per_file + flush de logs.
- Tamaño de tablas Q El estimador en config_loader emite warning > 1 M de entradas/ganancia.
- Complejidad DI Añade curva de aprendizaje; se documenta tabla de tokens registrados para debug.

**6. Próximos pasos (Sprint 2.1)**

- Extraer núcleo de framework (orquestador genérico + registro de plugins).
- Benchmark: comparar Global vs Shadow vs Echo en conjunto de seeds.


## Flujo Detallado de Datos Estructurado (v5.1.0_DI)

**Fase 1 – Inicialización**  
_Todo orquestado por `main.py`, apalancando el contenedor DI._

1. **main.py**  
   - Llama a `load_and_validate_config()` → devuelve `config`, `vis_config`, `logging_config`.  
   - Crea la carpeta de resultados vía `ResultHandler.setup_results_folder()`.  
   - Configura logging a fichero con `configure_file_logger()`.

2. **DI Container (`build_container`)**  
   - Registra factorías, servicios y singletons.  
   - Inyecta **`results_folder`** como singleton `str`.  
   - Resuelve [`ResultHandler`, `SimulationManager`, *etc.*].

3. **ResultHandler.save_metadata()**  
   - Guarda `metadata.json` (config, versión, comando de ejecución, *etc.*) en la carpeta de resultados.

---

**Fase 2 – Ejecución de la Simulación**  
_Ejecutada por `SimulationManager.run()`._

1. **Resolución de dependencias**  
   - `Environment`, `RLAgent`, `Controller`, `RewardFunction`, `RewardStrategy`, `MetricsCollector`, `VirtualSimulator*` (opcional).

2. **Bucle de episodios** (`max_episodes`)  
3. 1. `environment.reset()` → inicializa estado, controlador y agente.  
4. 2. `metrics_collector.reset()` → limpia buffers y asigna `episode_id`.  
5. 3. **Bucle de pasos `dt`**  
      - `environment.step()` → devuelve `next_state`, `(reward, w_stab)`, `force`.  
      - `metrics_collector.log()` registra métricas físicas, de control y del agente.  
      - **Cada `decision_interval` o `done`:**  
        1. Computa `interval_reward`, `avg_w_stab`.  
        2. _(Si `echo-baseline`)_: `virtual_simulator.run_interval_simulation()` → obtiene recompensas contrafactuales.  
        3. `agent.learn()` → pasa `RewardStrategy.compute_reward_for_learning()` (inyectado).  
        4. `agent.select_action()` → aplica nueva acción A′ vía `controller.update_params()`.  
        5. `metrics_collector.log_q_values / log_baselines / log_td_errors`.  
6. 4. Cierre de episodio  
      - `summarize_episode()` produce dict resumido.  
      - `ResultHandler.save_episode_batch()` guarda lote en `simulation_data_*`.json.  
      - `ResultHandler.save_agent_state()` según `agent_state_save_frequency`.

7. **Flush de logs**  
   - Cada `log_flush_frequency` pasos se sincronizan `FileHandler`s a disco.

---

**Fase 3 – Finalización**

1. `ResultHandler.finalize()`  
   - Genera `summary.xlsx`, convierte el último `agent_state_*.json` a Excel y dispara `generate_heatmap_data()` según `vis_config`.  
   - Copia artefactos clave al directorio de resultados.

2. **run_visualizations()** (opcional)  
   - Resuelve `PlotGenerator` desde DI y crea gráficas definidas en `vis_config`.

---

**Flujo de dependencias clave**

```text
config.yaml ─▶ build_container()
            │
            ├─▶ RewardFactory ──▶ GaussianReward  ─┐
            │                                      │
            ├─▶ SystemFactory  ───▶ InvertedPendulumSystem
            ├─▶ ControllerFactory ─▶ PIDController │   (comparten dt)
            ├─▶ AgentFactory ────▶ PIDQLearningAgent ◀─┘
            │
            └─▶ EnvironmentFactory ─▶ PendulumEnvironment
                                          ▲
                                          │ (opcional)
                                  VirtualSimulator (Echo)
```
