
Esta guía proporciona instrucciones prescriptivas para extender el sistema con nuevos componentes, asegurando que cada adición respete los principios de diseño de simplicidad, desacoplamiento y configuración declarativa.

# **1. Cómo Añadir un Nuevo Sistema Dinámico**

(Ej: GearedMotorSystem, QuadcopterSystem)

**Objetivo:** Integrar un nuevo modelo físico en el simulador.

**Pasos de Implementación:**

1. **Crear el Archivo de la Clase del Sistema Dinámico:**
    
    - **Ubicación:** components/systems/
        
    - **Archivo:** nuevo_sistema_system.py
        
    - **Contenido:**
        
        - Crea una clase NuevoSistemaSystem que herede de interfaces.dynamic_system.DynamicSystem.
            
        - Implementa el constructor __init__ para que acepte todos los parámetros físicos del nuevo sistema (ej: inercia_J, resistencia_R, etc.). Debe lanzar ValueError si los parámetros son físicamente inválidos.
            
        - Implementa el método _dynamics(self, state_vector, t, control_input). Este método contendrá las ecuaciones diferenciales de estado del nuevo sistema. state_vector es un np.ndarray.
            
        - Implementa el método apply_action(self, current_state, ...) que use scipy.integrate.odeint para resolver _dynamics durante un paso de tiempo dt.
            
        - Implementa el método reset(self, initial_conditions: Dict). Este método debe tomar un diccionario de condiciones iniciales (ej: {'posicion_angular': 0.1, 'velocidad_angular': 0.0}) y devolver el vector de estado inicial como un np.ndarray.
            
2. **Crear el Archivo de la Clase del Entorno:**
    
    - **Ubicación:** components/environments/
        
    - **Archivo:** nuevo_sistema_environment.py
        
    - **Contenido:**
        
        - Crea una clase NuevoSistemaEnvironment que herede de interfaces.environment.Environment.
            
        - Implementa el constructor __init__ para que acepte los componentes estándar (system, controller, agent, etc.) y la config global.
            
        - **Implementa el método _create_state_dict(self, state_vector):** **Este es el contrato clave.** Traduce el state_vector crudo de tu nuevo sistema a un diccionario con nombres explícitos (ej: {'posicion_angular': state_vector[0], ...}).
            
        - Implementa el método step() para orquestar la interacción, asegurándote de llamar a los componentes de recompensa y estabilidad con el state_dict generado.
            
        - Implementa check_termination() con la lógica de finalización específica para tu nuevo sistema (límites físicos, consecución de objetivos).
            
        - Implementa _evaluate_if_state_is_goal(self, state_dict) con los criterios de estabilización para el nuevo sistema.
            
3. **Actualizar la Configuración (config.yaml):**
    
    - Modifica la sección environment:
        
        - type: nuevo_sistema_environment
            
        - module_path: components.environments.nuevo_sistema_environment
            
        - class_name: NuevoSistemaEnvironment
            
    - Modifica la sección environment.system:
        
        - type: nuevo_sistema
            
        - module_path: components.systems.nuevo_sistema_system
            
        - class_name: NuevoSistemaSystem
            
        - params: Añade todos los parámetros físicos que tu NuevoSistemaSystem necesita en su __init__.
            
    - Modifica la sección environment.initial_conditions: Define las condiciones iniciales para el nuevo sistema.


#### **2. Cómo Añadir un Nuevo Tipo de Agente de RL**

(Ej: ActorCriticAgent, DDPGAgent)

**Objetivo:** Integrar un nuevo algoritmo de aprendizaje por refuerzo.

**Pasos de Implementación:**

1. **Crear el Archivo de la Clase del Agente:**
    
    - **Ubicación:** components/agents/
        
    - **Archivo:** nuevo_agente.py
        
    - **Contenido:**
        
        - Crea una clase NuevoAgente que herede de interfaces.rl_agent.RLAgent.
            
        - Implementa todos los métodos abstractos de la interfaz, incluyendo select_action, learn, reset_agent, build_agent_state, set_reward_strategy, etc.
            
        - El método learn debe llamar a self.reward_strategy.compute_reward_for_learning() para obtener la R_learn, manteniendo el desacoplamiento.
            
2. **Actualizar la Configuración (config.yaml):**
    
    - Modifica la sección environment.agent:
        
        - type: nuevo_agente_qlearning (o el nombre que elijas)
            
        - module_path: components.agents.nuevo_agente
            
        - class_name: NuevoAgente
            
        - params: Añade todos los hiperparámetros que tu nuevo agente necesita (ej: actor_learning_rate, critic_learning_rate, etc.).

#### **3. Cómo Añadir una Nueva Estrategia de Recompensa**

(Ej: PotentialBasedShapingStrategy, PPOClipStrategy)

**Objetivo:** Implementar una nueva forma de calcular la recompensa de aprendizaje (R_learn) a partir de la recompensa del entorno.

**Pasos de Implementación:**

1. **Crear el Archivo de la Clase de la Estrategia:**
    - **Ubicación:** components/reward_strategies/
    - **Archivo:** nueva_estrategia_reward.py
    - **Contenido:**
        - Crea una clase NuevaEstrategiaReward que herede de interfaces.reward_strategy.RewardStrategy.
        - Define los atributos de clase:
            - needs_virtual_simulation: bool: Ponlo a True si tu estrategia requiere simulaciones contrafactuales.
            - required_auxiliary_tables: List[str]: Lista los nombres de las tablas auxiliares que el agente debe gestionar para tu estrategia (ej: ['potential_values']). Si no necesita ninguna, déjalo como [].
        - Implementa el método compute_reward_for_learning(...). Esta es la lógica central. Puede usar el agent_instance para leer/escribir en tablas auxiliares. Debe devolver un único float.
2. **Registrar la Nueva Estrategia en el Contenedor DI:**
    - **Archivo:** di_container.py
    - **Acción:**
        - Importa tu nueva clase: from components.reward_strategies.nueva_estrategia_reward import NuevaEstrategiaReward.
        - Dentro de build_container, localiza la instanciación de RewardFactory y añade una nueva línea de registro:
            Generated python
```
reward_f.register_reward_strategy_type('nombre_estrategia_en_yaml', NuevaEstrategiaReward)
```
1. **Actualizar la Configuración (config.yaml):**
    - Modifica la sección environment.reward_setup.reward_strategy:
        - type: nombre_estrategia_en_yaml
        - strategy_params: Añade una nueva clave nombre_estrategia_en_yaml con todos los parámetros que el __init__ de tu nueva estrategia necesita.

#### **4. Cómo Añadir un Nuevo Componente de Cálculo (Recompensa o Estabilidad)**

(Ej: un nuevo método para RewardFunction o StabilityCalculator)

**Objetivo:** Introducir una nueva forma de calcular la recompensa instantánea (R_real) o la estabilidad (w_stab).

**Pasos de Implementación:**

1. **Crear el Archivo de la Clase del Calculador:**
    
    - **Ubicación:** components/rewards/ o components/analysis/
        
    - **Archivo:** nuevo_calculador.py
        
    - **Contenido:**
        
        - Crea una clase NuevoCalculador que herede de RewardFunction o BaseStabilityCalculator.
            
        - Implementa el constructor __init__(self, config: Dict[str, Any]). El constructor debe ser autosuficiente y extraer sus propios parámetros del config global.
            
        - Implementa los métodos de la interfaz (calculate, calculate_instantaneous_stability, etc.), asegurándote de que operen sobre un state_dict.
            
2. **Registrar el Nuevo Calculador:**
    
    - **Para una RewardFunction:**
        
        - **Archivo:** di_container.py
            
        - **Acción:** Importa la nueva clase y regístrala en la RewardFactory en build_container:
```
reward_f.register_reward_function_type('nombre_calculador_en_yaml', NuevoCalculador)
```
- **Para un StabilityCalculator:**
	- **Archivo:** di_container.py
	- **Acción:** Modifica el helper _create_stability_calculator_helper. Añade un nuevo elif para instanciar tu NuevoCalculador cuando type coincida:
```
# Dentro de _create_stability_calculator_helper 
elif calc_type == 'nombre_calculador_en_yaml': return NuevoCalculador(config=config)
```
1. **Actualizar la Configuración (config.yaml):**
    - **Para una RewardFunction:** Modifica environment.reward_setup.calculation.reward_function_type al nombre_calculador_en_yaml.
    - **Para un StabilityCalculator:** Modifica environment.reward_setup.calculation.stability_measure.type al nombre_calculador_en_yaml y añade su bloque de parámetros (nuevos_params: {...}).

---

### **Guía de Escalamiento Avanzado (Versión Rigurosa)**

#### **1. Cómo Escalar a un Framework de Múltiples Sistemas Dinámicos Interconectados**

**Objetivo:** Permitir la simulación de un "Sistema de Sistemas" donde múltiples `DynamicSystem` coexisten e interactúan.

**Impacto Arquitectónico:** Este cambio introduce un patrón `Composite` en varios niveles de la arquitectura. Afecta cómo se definen y orquestan los sistemas, entornos, controladores y agentes.

**Pasos Detallados de Implementación:**

1.  **Definir la Interfaz `CompositeSystem`:**
    *   **Archivo a Crear:** `interfaces/composite_system.py`
    *   **Contenido:**
        *   Crear clase `CompositeSystem(DynamicSystem)` para actuar como contenedor.
        *   `__init__(self, subsystems: Dict[str, DynamicSystem], coupling_logic: Callable)`: Almacena los subsistemas y la función de acoplamiento.
        *   `apply_action(self, actions: Dict[str, float], ...)`: Itera sobre el diccionario `actions`. Para cada `(name, action)` en `actions`, llama a `self.subsystems[name].apply_action(action, ...)`. Después de que todos los subsistemas avanzan, ejecuta `coupling_logic` para sincronizar estados.
        *   `reset(self, initial_conditions: Dict[str, Dict])`: Itera sobre `initial_conditions` y llama a `self.subsystems[name].reset(conditions)`.
        *   El estado que devuelve es un diccionario agregado: `{name: state_vector for name, subsystem in self.subsystems.items()}`.

2.  **Crear la Clase `CompositeEnvironment`:**
    *   **Archivo a Crear:** `components/environments/composite_environment.py`
    *   **Contenido:**
        *   Crear clase `CompositeEnvironment(Environment)`.
        *   `__init__(self, system: CompositeSystem, controllers: Dict[str, Controller], agents: Dict[str, RLAgent], ...)`: Acepta diccionarios de componentes, uno por cada subsistema.
        *   `step()`:
            1.  Crea un diccionario de acciones vacío `actions_to_apply = {}`.
            2.  Itera sobre los `controllers`. Llama a `controller.compute_action()` para cada uno y puebla `actions_to_apply`.
            3.  Llama a `self.system.apply_action(actions_to_apply, ...)`. El `system` aquí es una instancia de `CompositeSystem`.
            4.  Recibe el nuevo `composite_state_dict` del sistema.
            5.  Itera sobre los subsistemas. Para cada uno, extrae su `sub_state_dict` y calcula su recompensa y estabilidad.
            6.  Devuelve un estado y recompensas agregados (ej: la suma de recompensas o un diccionario de ellas).
        *   `_create_state_dict()`: Devuelve el diccionario de estados anidado directamente desde el `CompositeSystem`.

3.  **Adaptar `SystemFactory` y `EnvironmentFactory`:**
    *   **Archivos a Modificar:** `factories/system_factory.py`, `factories/environment_factory.py`.
    *   **Modificaciones:**
        *   En `create_system` de `SystemFactory`, si `system_type == 'composite'`, la lógica debe cambiar. En lugar de instanciar una clase directamente, debe:
            1.  Iterar sobre la configuración de `subsystems`.
            2.  Llamarse a sí misma (`self.create_system(...)`) de forma recursiva para crear cada instancia de subsistema.
            3.  Importar dinámicamente la `coupling_logic`.
            4.  Instanciar `CompositeSystem` con el diccionario de subsistemas creados y la lógica de acoplamiento.
        *   Se aplica un patrón similar a `EnvironmentFactory` y `ControllerFactory`/`AgentFactory`. Estas factorías necesitarán ser capaces de manejar una configuración que especifique un diccionario de componentes en lugar de uno solo.

4.  **Adaptar `DI Container`:**
    *   **Archivo a Modificar:** `di_container.py`.
    *   **Modificaciones:**
        *   La lógica de instanciación de `DynamicSystem`, `Controller`, `RLAgent` y `Environment` debe ser modificada. En lugar de resolver una única instancia, debe detectar si la configuración es para un sistema compuesto.
        *   Si es así, el contenedor deberá orquestar la creación de los diccionarios de componentes (controladores, agentes) que el `CompositeEnvironment` necesita, posiblemente iterando y llamando a las factorías para cada subsistema definido en la configuración.

5.  **Adaptar `SimulationManager`:**
    *   **Archivo a Modificar:** `simulation_manager.py`.
    *   **Modificaciones:**
        *   El `SimulationManager` en sí mismo debería requerir muy pocos cambios, ya que opera a través de la interfaz `Environment`.
        *   Sin embargo, cómo maneja los datos para el `learn` del agente podría necesitar ajustes. Si el `CompositeEnvironment` devuelve un diccionario de recompensas (una por subsistema), el `SimulationManager` deberá iterar y llamar a `agent.learn()` para cada agente con su recompensa correspondiente. Esto implica que el `agent` resuelto por el DI podría ser un `Dict[str, RLAgent]`.

---

#### **2. Cómo Añadir Sistemas de Múltiples Agentes (MARL) para Optimización Multi-Objetivo**

**Objetivo:** Permitir que múltiples agentes de RL colaboren o compitan para optimizar objetivos, que pueden ser compartidos, individuales o contrapuestos.

**Impacto Arquitectónico:** Este cambio introduce una capa de coordinación explícita (`MARLCoordinator`) que se sitúa entre el cálculo de recompensas del entorno y el proceso de aprendizaje de los agentes. Modifica fundamentalmente el flujo de la señal de recompensa.

**Pasos Detallados de Implementación:**

1.  **Crear la Interfaz `MARLCoordinator`:**
    *   **Archivo a Crear:** `interfaces/marl_coordinator.py`
    *   **Contenido:**
        *   Crear la clase abstracta `MARLCoordinator(ABC)`.
        *   `__init__(self, agent_ids: List[str], config: Dict)`: Se inicializa con los identificadores de los agentes que va a coordinar y su configuración específica.
        *   `process_rewards(self, individual_rewards: Dict[str, float], global_state: Dict, actions: Dict) -> Dict[str, float]`: Método abstracto. Recibe las recompensas individuales y el contexto global, y devuelve las recompensas procesadas que cada agente usará para aprender.

2.  **Crear Implementaciones Concretas del Coordinador:**
    *   **Archivos a Crear:** `components/coordinators/vdn_coordinator.py`, `components/coordinators/qmix_coordinator.py`, etc.
    *   **Contenido:**
        *   Cada archivo implementará una estrategia de coordinación específica. Por ejemplo, `VDNCoordinator` (Value-Decomposition Networks) simplemente sumaría las recompensas. `QMIXCoordinator` implementaría una red de mezcla más compleja.

3.  **Refactorizar `RewardStrategy` y `RLAgent`:**
    *   **Archivos a Modificar:** `interfaces/reward_strategy.py`, `interfaces/rl_agent.py`.
    *   **Modificaciones:**
        *   El método `compute_reward_for_learning` en `RewardStrategy` no es el lugar adecuado para la coordinación multi-agente, ya que opera a nivel de un solo agente. Su rol se mantiene: transformar la recompensa *final* que recibe un agente.
        *   El método `learn` en la interfaz `RLAgent` se mantiene sin cambios en su firma. Seguirá recibiendo una recompensa escalar.

4.  **Adaptar `DI Container` para Inyectar el Coordinador:**
    *   **Archivo a Modificar:** `di_container.py`.
    *   **Modificaciones:**
        *   Se añadirá un nuevo registro para resolver el `MARLCoordinator`.
            ```python
            container.register(Optional[MARLCoordinator], _create_marl_coordinator_helper, singleton=True)
            ```
        *   Se creará un `_create_marl_coordinator_helper` que leerá la sección `marl_coordinator` de la configuración, importará dinámicamente la clase del coordinador especificado y la instanciará.

5.  **Modificar `SimulationManager` para Incorporar la Capa de Coordinación:**
    *   **Archivo a Modificar:** `simulation_manager.py`.
    *   **Modificaciones:** Este es el cambio más crítico.
        *   En `_resolve_dependencies`, el `SimulationManager` ahora resolverá la instancia opcional del `MARLCoordinator`.
            ```python
            marl_coordinator = self.container.resolve(Optional[MARLCoordinator])
            ```
        *   En el bucle principal de `_run_episode`, la lógica después de `env.step()` cambiará. Asumiendo que el entorno ahora devuelve un diccionario de recompensas `individual_rewards = {'kp_agent': r1, 'ki_agent': r2}`.
        *   El flujo será:
            1.  Llamar a `env.step()` -> obtener `individual_rewards`.
            2.  **Si `marl_coordinator` existe:**
                ```python
                processed_rewards = marl_coordinator.process_rewards(
                    individual_rewards, 
                    global_state=current_agent_state, 
                    actions=actions
                )
                ```
            3.  **Si no:** `processed_rewards = individual_rewards`.
            4.  Iterar sobre cada agente y llamar a `agent.learn()` con su recompensa específica de `processed_rewards`.
                ```python
                for agent_id, agent_instance in agents.items():
                    reward_for_agent = processed_rewards[agent_id]
                    # ... construir reward_info para este agente ...
                    agent_instance.learn(..., reward_info={'interval_reward': reward_for_agent}, ...)
                ```
        *   Esto implica que `RLAgent` resuelto desde el DI ya no es una sola instancia, sino un `Dict[str, RLAgent]`. La lógica del `DI Container` y las factorías deben adaptarse para construir este diccionario de agentes.

6.  **Refactorizar la Configuración (`config.yaml`) para Soportar MARL:**
    *   La sección `agent` se convertirá en una lista o diccionario, donde cada elemento define un agente individual.
        ```yaml
        agent:
          kp_agent:
            type: pid_qlearning
            module_path: ...
            params: { ... }
          ki_agent:
            type: pid_qlearning
            module_path: ...
            params: { ... }
        ```
    *   Se añadirá la sección `marl_coordinator` para definir la estrategia de coordinación, como se describió anteriormente.

Esta guía detallada asegura que cada paso del escalamiento se realice de manera informada, considerando todas las dependencias y manteniendo la integridad de la arquitectura.

---

Entendido. Mi error fue aterrizar los conceptos en ejemplos demasiado concretos. La guía debe ser un "meta-recetario" que enseñe a los desarrolladores a pensar en cómo escalar, independientemente de la aplicación específica.

Aquí presento la versión revisada de la guía de escalamiento avanzado, redactada con el nivel de abstracción y generalidad que solicitaste.

---

### **Guía de Escalamiento Arquitectónico Avanzado**

Esta guía detalla los patrones y modificaciones estructurales necesarios para evolucionar el framework hacia dos escenarios de alta complejidad: la simulación de sistemas compuestos y la coordinación de múltiples agentes de RL (MARL). Las instrucciones son agnósticas al dominio específico (química, robótica, etc.) y al algoritmo de RL utilizado.

#### **1. Framework para Múltiples Sistemas Dinámicos Interconectados**

**Objetivo General:** Evolucionar la arquitectura para simular un único "sistema de sistemas", donde múltiples componentes `DynamicSystem` individuales coexisten, son orquestados de forma centralizada y pueden intercambiar información entre ellos (acoplamiento).

**Patrón Arquitectónico Clave:** Implementación del patrón `Composite` a través de toda la pila de simulación. Se introducen componentes "contenedores" que gestionan colecciones de componentes base.

**Pasos Metodológicos de Implementación:**

1.  **Generalizar la Noción de "Sistema": El `CompositeSystem`**
    *   **Acción:** Crear una nueva implementación de `DynamicSystem` llamada `CompositeSystem`.
    *   **Responsabilidad:** Esta clase no implementará ecuaciones diferenciales. Su rol es ser un contenedor que gestiona un `Dict[str, DynamicSystem]`.
    *   **Contrato Clave (`apply_action`):** Su método `apply_action` recibirá un diccionario de acciones (`actions: Dict[str, float]`). Iterará sobre este diccionario, delegando cada acción a su subsistema correspondiente.
    *   **Mecanismo de Acoplamiento:** El `__init__` del `CompositeSystem` aceptará una `coupling_function` inyectada. Esta función es la única que contiene la lógica de negocio sobre cómo los estados de los subsistemas se afectan mutuamente. Será invocada por `apply_action` después de que todos los subsistemas hayan avanzado un paso.
    *   **Estado Agregado:** El estado del `CompositeSystem` es un diccionario que mapea los nombres de los subsistemas a sus respectivos vectores de estado.

2.  **Generalizar la Noción de "Entorno": El `CompositeEnvironment`**
    *   **Acción:** Crear una nueva implementación de `Environment` llamada `CompositeEnvironment`.
    *   **Responsabilidad:** Orquestar la interacción de un `CompositeSystem` con una colección de componentes de control (`Controllers`, `RLAgents`, `RewardFunctions`).
    *   **Estructura de Componentes:** Su `__init__` aceptará `system: CompositeSystem` y diccionarios de componentes de control, por ejemplo: `controllers: Dict[str, Controller]`.
    *   **Orquestación en `step()`:**
        1.  Delegará a cada `Controller` el cálculo de su acción.
        2.  Agregará las acciones en un diccionario y lo pasará al `CompositeSystem`.
        3.  Recibirá el estado compuesto y lo usará para calcular un diccionario de recompensas, delegando a la `RewardFunction` de cada subsistema.
    *   **Contrato de Estado (`_create_state_dict`):** Este método es fundamental. Transformará el estado vectorial de cada subsistema en un diccionario de características nombradas, devolviendo un diccionario anidado (`{'subsystem_A': {'feature1': val}, 'subsystem_B': {'feature2': val}}`).

3.  **Habilitar la Creación de Estructuras Anidadas: Refactorización de Factorías y DI**
    *   **Acción:** Modificar las factorías (`SystemFactory`, `ControllerFactory`, etc.) y el `DI Container`.
    *   **Lógica de Factoría Recursiva:** Los métodos `create_*` de las factorías deben ser capaces de detectar una configuración de tipo "compuesto". Si la detectan, deben iterar sobre la definición de los sub-componentes y llamarse a sí mismas recursivamente para construir cada uno, finalmente ensamblando un diccionario de instancias en lugar de una sola.
    *   **Lógica de Resolución del DI:** El `DI Container` debe ser adaptado para manejar la resolución de estos diccionarios de componentes. Cuando se resuelva `DynamicSystem`, por ejemplo, el resultado podría ser un `Dict[str, DynamicSystem]` si la configuración es compuesta. Las llamadas subsecuentes para construir el entorno deben manejar este tipo de dato.

---

#### **2. Framework para Sistemas Multi-Agente (MARL) con Objetivos de Optimización Múltiples**

**Objetivo General:** Implementar una estructura que permita a múltiples agentes de RL coexistir y aprender dentro del mismo entorno. Esta estructura debe ser lo suficientemente flexible para soportar diferentes paradigmas de interacción (cooperación, competición, consenso) a través de un mecanismo de coordinación explícito y conectable ("plug-and-play").

**Patrón Arquitectónico Clave:** Introducción de una capa de "Coordinación de Recompensa" (`MARLCoordinator`) que se sitúa entre la generación de recompensas del entorno y el proceso de aprendizaje de los agentes. Esta capa modifica la señal de recompensa que cada agente recibe, implementando así la lógica de interacción multi-agente.

**Pasos Metodológicos de Implementación:**

1.  **Abstraer la Lógica de Interacción: La Interfaz `MARLCoordinator`**
    *   **Acción:** Crear una nueva interfaz `interfaces/marl_coordinator.py`.
    *   **Responsabilidad:** Definir el contrato para cualquier estrategia de coordinación multi-agente.
    *   **Contrato Clave (`process_rewards`):** El método abstracto `process_rewards(self, individual_rewards: Dict, global_context: Dict) -> Dict[str, float]` será su función principal.
        *   **Entradas:** Recibe las recompensas "egoístas" que cada agente habría obtenido y el contexto global (que puede incluir el estado de todos los agentes y sus acciones).
        *   **Salida:** Devuelve un diccionario de recompensas "sociales" o procesadas. Cada agente usará su valor correspondiente de este diccionario para su actualización de `learn`.
    *   **Implementaciones Concretas:** Se pueden crear diferentes coordinadores (`VDNCoordinator`, `QMIXCoordinator`, `ConsensusCoordinator`) que implementen esta interfaz, cada uno con una lógica distinta en `process_rewards`.

2.  **Integrar la Capa de Coordinación en el Flujo de Simulación: Modificar `SimulationManager`**
    *   **Acción:** `SimulationManager` es el lugar natural para insertar esta capa, ya que es el orquestador que tiene visibilidad sobre todos los agentes y la recompensa total.
    *   **Modificaciones en `_run_episode`:**
        1.  El `SimulationManager` resolverá una instancia opcional de `MARLCoordinator` desde el DI.
        2.  Después de que el entorno (`env.step()`) devuelva un diccionario de recompensas individuales (`individual_rewards`).
        3.  **Punto de Inserción:** Si el `MARLCoordinator` está presente, se invocará `marl_coordinator.process_rewards(individual_rewards, ...)`. La salida de esta llamada (`processed_rewards`) reemplazará al `individual_rewards` original.
        4.  El bucle de aprendizaje (`agent.learn(...)`) procederá como antes, pero utilizando las recompensas del diccionario `processed_rewards`.

3.  **Adaptar el Ecosistema para Soportar Colecciones de Agentes:**
    *   **Acción:** Las factorías y el DI deben ser capaces de construir y gestionar un `Dict[str, RLAgent]` en lugar de un único agente.
    *   **Factorías (`AgentFactory`):** El método `create_agent` se refactorizará para poder ser llamado en un bucle y construir un diccionario de agentes si la configuración lo especifica.
    *   **DI Container (`di_container.py`):** La lógica de resolución para `RLAgent` debe ser adaptada para manejar esta nueva estructura de "colección de agentes". El resultado de `c.resolve(RLAgent)` podría ser un `Dict` en un escenario MARL.
    *   **Exposición de Estado para Coordinación:** La interfaz `RLAgent` debe garantizar que expone métodos para que un coordinador pueda consultar su estado interno si es necesario (ej: `get_q_values_for_state`), permitiendo estrategias de coordinación más sofisticadas que solo la recompensa.

4.  **Habilitar la Configuración Declarativa de Escenarios MARL:**
    *   **Acción:** Extender el `config.yaml` para definir estos nuevos conceptos.
    *   **Sección `agent`:** Se transformará de un objeto único a un diccionario, donde cada clave es un identificador de agente y cada valor es la configuración completa de ese agente.
    *   **Nueva Sección `marl_coordinator`:** Se añadirá una sección opcional de primer nivel para definir qué coordinador se debe usar, su `module_path`, `class_name` y sus `params` específicos. Si esta sección está ausente, el sistema opera en modo de agente único o multi-agente no coordinado.


---


¡Excelente pregunta! Este escenario es un paso intermedio perfecto y muy práctico entre el sistema de agente único y el framework MARL completo. Aborda la optimización multi-objetivo dentro de un único sistema dinámico.

Aquí te presento la guía generalizada primero, seguida por la aplicación específica y detallada al sistema CartPole.

---

### **Guía de Escalamiento: Múltiples Controladores PID con Objetivos y Recompensas Diferenciadas**

#### **Parte 1: Guía de Escalamiento Generalizada**

**Objetivo General:** Adaptar la arquitectura para soportar múltiples controladores PID operando sobre el mismo `DynamicSystem`. Cada PID tendrá su propio `setpoint` (objetivo) y será optimizado por un `RLAgent` independiente que recibe una señal de recompensa personalizada, calculada por una `RewardFunction` específica para su objetivo.

**Patrón Arquitectónico Clave:** Transición de componentes singulares (`Controller`, `RLAgent`, `RewardFunction`) a colecciones (diccionarios) de estos componentes. El `Environment` se convierte en el orquestador principal que gestiona estas colecciones y distribuye la información relevante a cada "bucle de control" individual.

**Pasos Metodológicos de Implementación:**

1.  **Evolucionar de Componente Único a Colección de Componentes:**
    *   **Acción:** Refactorizar el `DI Container` y las `Factorías` (`ControllerFactory`, `AgentFactory`, `RewardFactory`) para que puedan construir diccionarios de componentes (`Dict[str, Controller]`, `Dict[str, RLAgent]`, etc.) cuando la configuración lo especifique.
    *   **Lógica de Factoría:** Los métodos `create_*` deben poder manejar una configuración que, en lugar de ser un objeto único, sea un diccionario. Iterarán sobre las claves de este diccionario (ej: `pid_objetivo_A`, `pid_objetivo_B`), crearán una instancia para cada una y devolverán un diccionario de componentes.
    *   **Lógica del DI:** El `DI Container` debe ser capaz de resolver estos diccionarios. Por ejemplo, `c.resolve(Controller)` ahora podría devolver un `Dict[str, Controller]`.

2.  **Centralizar la Orquestación en el `Environment`:**
    *   **Acción:** El `Environment` se convierte en el núcleo que gestiona los múltiples bucles de control.
    *   **Modificaciones al `__init__`:** Aceptará `controllers: Dict[str, Controller]`, `agents: Dict[str, RLAgent]`, `reward_functions: Dict[str, RewardFunction]`.
    *   **Modificaciones al `step()`:**
        1.  **Agregación de Acciones:** Iterará sobre el `controllers` dict, llamará a `compute_action` en cada uno y sumará o combinará las acciones resultantes en una única acción final que se aplicará al `DynamicSystem`. La lógica de combinación (ej: suma simple, promedio ponderado) debe ser definida dentro del `Environment`.
        2.  **Llamada al Sistema:** Llamará a `system.apply_action()` con la acción final agregada.
        3.  **Cálculo de Recompensas Diferenciadas:** Iterará sobre el `reward_functions` dict. Para cada una (`rf_A`, `rf_B`), llamará a `rf.calculate()` pasándole el contexto del sistema (`state_dict`, `next_state_dict`). Esto genera un diccionario de recompensas: `rewards = {'objetivo_A': r_A, 'objetivo_B': r_B}`.
        4.  **Devolución Agregada:** El `step()` del entorno devolverá este `rewards` dict (o una suma, según el diseño) al `SimulationManager`.

3.  **Adaptar `SimulationManager` para el Aprendizaje Multi-Agente:**
    *   **Acción:** `SimulationManager` debe poder manejar el diccionario de recompensas y delegar el aprendizaje a cada agente.
    *   **Modificaciones en `_run_episode`:**
        1.  Después de `env.step()`, recibirá el `rewards_dict`.
        2.  Iterará sobre el `agents` dict. Para cada `agent_id` y `agent_instance`:
        3.  Extraerá la recompensa correspondiente: `reward_for_agent = rewards_dict[agent_id]`.
        4.  Llamará a `agent_instance.learn()` con esa recompensa específica.

4.  **Configuración Declarativa de Múltiples Bucles:**
    *   **Acción:** El `config.yaml` debe ser reestructurado para reflejar esta arquitectura de colecciones.
    *   **Estructura del `config.yaml`:** Las secciones `controller`, `agent` y `reward_setup` (o una subsección de esta) se convertirán en diccionarios. Cada clave representará un bucle de control con su objetivo.

    ```yaml
    # Ejemplo de estructura en config.yaml
    environment:
      # ...
      controller:
        control_loop_A:
          type: pid
          params: { setpoint: X, ... }
        control_loop_B:
          type: pid
          params: { setpoint: Y, ... }
      agent:
        control_loop_A:
          type: pid_qlearning
          params: { ... }
        control_loop_B:
          type: pid_qlearning
          params: { ... }
      reward_setup:
        reward_functions: # Nueva sub-sección
          control_loop_A:
            method: weighted_exponential
            params: { ... } # Pesos para el objetivo A
          control_loop_B:
            method: weighted_exponential
            params: { ... } # Pesos para el objetivo B
    ```

---

#### **Parte 2: Guía Específica para el Sistema CartPole Multi-Objetivo**

**Objetivo Específico:** Implementar dos controladores PID sobre el `InvertedPendulumSystem`:
1.  **`PID_Angle`:** Su objetivo es mantener el péndulo vertical (`angle = 0`). Es optimizado por `AngleAgent`.
2.  **`PID_Position`:** Su objetivo es mantener el carro en el centro (`cart_position = 0`). Es optimizado por `PositionAgent`.

Ambos controladores generan una fuerza, y la fuerza neta aplicada al carro es la suma de ambas.

**Pasos Detallados de Implementación:**

1.  **Refactorizar `PendulumEnvironment` para ser Multi-Controlador:**
    *   **Archivo a Modificar:** `components/environments/pendulum_environment.py`.
    *   **Modificaciones en `__init__`:**
        *   Ya no aceptará `controller: Controller`, `agent: RLAgent`, etc., sino `controllers: Dict[str, Controller]`, `agents: Dict[str, RLAgent]` y `reward_functions: Dict[str, RewardFunction]`.
        *   Almacenará estos diccionarios internamente (ej: `self.controllers`).
    *   **Modificaciones en `step()`:**
        1.  `total_force = 0.0`
        2.  Itera `for name, ctrl in self.controllers.items()`: `total_force += ctrl.compute_action(current_state_vec)`.
        3.  Llama a `self.system.apply_action(total_force, ...)`.
        4.  Crea el `next_state_dict`.
        5.  `rewards = {}`
        6.  Itera `for name, rf in self.reward_functions.items()`: `rewards[name] = rf.calculate(next_state_dict, ...)`.
        7.  Devuelve el `rewards` dict como parte de la tupla de retorno (en lugar de un float).

2.  **Adaptar `DI Container` y Factorías:**
    *   **Archivo a Modificar:** `di_container.py`.
    *   **Modificaciones:**
        *   El helper `_create_reward_function_helper` debe ser refactorizado. Ya no creará una sola `RewardFunction`, sino que podría necesitar crear un diccionario de ellas. Una forma simple es que el `PendulumEnvironment` reciba la `RewardFactory` completa y construya las funciones que necesita internamente basándose en la configuración. Alternativamente, el `DI Container` puede construir el diccionario.
        *   La lógica en `build_container` para registrar `Controller` y `RLAgent` debe cambiar. En lugar de registrar una única instancia, leerá el diccionario de configuración y registrará un `Dict[str, Controller]` y un `Dict[str, RLAgent]`.
            ```python
            # Lógica conceptual en build_container
            controllers_cfg = main_config.get('environment', {}).get('controller', {})
            controllers_dict = {name: c.resolve(ControllerFactory).create_controller(cfg['type'], cfg['params']) for name, cfg in controllers_cfg.items()}
            container.register(dict, lambda c: controllers_dict, singleton=True) # Registrar bajo un token específico, ej: "ControllersDict"
            ```

3.  **Adaptar `SimulationManager`:**
    *   **Archivo a Modificar:** `simulation_manager.py`.
    *   **Modificaciones en `_run_episode`:**
        1.  La llamada a `env.step()` ahora devuelve un `rewards_dict`.
        2.  El `SimulationManager` ya no puede pasar una única recompensa al método `learn`. Debe iterar sobre los agentes.
        3.  `agents_dict = c.resolve(Dict[str, RLAgent])`
        4.  `rewards_dict = env.step()`
        5.  `for agent_id, agent_instance in agents_dict.items()`:
            *   `reward_for_this_agent = rewards_dict.get(agent_id, 0.0)`
            *   `reward_info = {'interval_reward': reward_for_this_agent, ...}`
            *   `agent_instance.learn(s, a, reward_info, s_prime, ...)`

4.  **Actualizar `config.yaml` para el Escenario Específico:**
    ```yaml
    environment:
      type: pendulum_environment # Mantenemos el mismo, pero ahora es multi-controlador
      # ...
      controller:
        angle_control:
          type: pid
          module_path: ...
          class_name: PIDController
          params:
            setpoint: 0.0
            process_variable_index: 2 # El índice del ángulo en el vector de estado
            # Kp, Ki, Kd iniciales para el ángulo
        position_control:
          type: pid
          module_path: ...
          class_name: PIDController
          params:
            setpoint: 0.0
            process_variable_index: 0 # El índice de la posición del carro
            # Kp, Ki, Kd iniciales para la posición
            
      agent:
        angle_control:
          type: pid_qlearning
          params: { ... } # Configuración del agente que optimiza el PID de ángulo
        position_control:
          type: pid_qlearning
          params: { ... } # Configuración del agente que optimiza el PID de posición
          
      reward_setup:
        # La sección de cálculo ahora es un diccionario
        calculation_functions: 
          angle_control:
            method: weighted_exponential
            weighted_exponential_params:
              features:
                angle: { weight: 1.0, scaled: 0.15 } # Foco principal en el ángulo
                angular_velocity: { weight: 1.0, scaled: 0.25 }
                cart_position: { weight: 0.1, scaled: 1.0 } # Penalización leve por moverse
                cart_velocity: { weight: 0.1, scaled: 1.0 }
          position_control:
            method: weighted_exponential
            weighted_exponential_params:
              features:
                cart_position: { weight: 1.0, scaled: 0.5 } # Foco principal en la posición
                cart_velocity: { weight: 1.0, scaled: 0.5 }
                angle: { weight: 0.2, scaled: 0.5 } # Penalización leve por desestabilizar el péndulo
                angular_velocity: { weight: 0.2, scaled: 0.5 }
    ```

Este enfoque permite una optimización multi-objetivo real y abre la puerta a investigar el fascinante problema de cómo las políticas de los agentes interfieren entre sí mientras intentan controlar el mismo sistema subyacente.




recuerda que la programación debe ser directa, explicita y lo más parecido a mi estilo, adicionalmente, la idea es que el sistema sea escalable y flexible a nuevas configuraciones y adición de componentes