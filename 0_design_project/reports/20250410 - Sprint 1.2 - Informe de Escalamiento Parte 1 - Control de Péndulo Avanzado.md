---
created: 20250410 11:04
update: 20250410-11:47
summary: 
status: done
link: 
tags:
  - content
---

# Documento del Estado Actual del Proyecto
## 1. Objetivo del Escalamiento

Entrenar un agente basado en Q-Learning que adapte en línea las ganancias de un controlador PID (`Kp`, `Ki`, `Kd`) para estabilizar un péndulo invertido montado sobre un carro. El objetivo final es maximizar una función de recompensa diseñada para reflejar la estabilidad del sistema, evaluando el impacto de cada acción individual de cada agente sobre la recompensa global mediante estrategias diferenciadas de evaluación.

---

## 2. Arquitectura Modular y Componentes Principales

La arquitectura se estructura en torno a un flujo modular, altamente desacoplado mediante el uso de **interfaces (ABCs)** y **fábricas configurables**, integrando:

- **main.py**: Orquestador de simulación.
- **PendulumEnvironment**: Medio que conecta sistema, agente y controlador.
- **PIDQLearningAgent**: Controladores separados por ganancia. Aprenden mediante recompensa global, echo o shadow.
- **GaussianReward + Stability Calculators (IRA / Exponential)**: Generadores de recompensa.
- **PendulumVirtualSimulator**: Simulador interno que permite estimar recompensas contrafactuales.
- **SimpleMetricsCollector**: Recolección de métricas clave durante la ejecución.

Configuración completa vía `config.yaml`.

---

## 3. Flujo de Datos y Episodio

El flujo de cada episodio incluye:

1. Reset del entorno (`PendulumEnvironment`) y del agente (`PIDQLearningAgent`).
2. Aplicación del controlador PID al sistema físico (ODEs).
3. Cálculo de recompensa y estabilidad.
4. Acumulación de métricas por intervalo (`decision_interval`).
5. Llamada a `learn()` por parte del agente para actualizar sus Q-tables.

Según el `reward_mode`, la recompensa se determina:

- `global`: $R_{real}$
- `echo-baseline`: $R_{dif}^{i} = R_{real} - R_{cf}^{i}$
- `shadow-baseline`: $R_{dif}^{i} = R_{real} - B(s)$

---

# 4. Detalle de Estrategias de Recompensa Diferencial

## Echo Baseline Agent

- Evalúa simulaciones virtuales contrafactuales por ganancia.
- Para cada agente $i$  en $\{k_p, k_i, k_d\}$, se calcula:
  $$R_{cf}^{i} = {reward \ with}\ K_i \ {revertido}$$
  $$R_{dif}^{i} = R_{real} - R_{cf}^{i}$$
## Shadow Baseline Agent

- Mantiene una tabla `B(s)` para cada agente.
- Si se elige acción `"mantener"`, se actualiza:
  $$B(s) \leftarrow B(s) + \beta \cdot w_{stab} \cdot (R_{real} - B(s))$$
- Para otras acciones, se usa:
  $$R_{dif}^{i} = R_{real} - B(s)$$

---

# 5. Métrica de Estabilidad (w_stab)

## Exponential:
$$w_{stab} = \exp(-\sum \lambda_i (\frac{s_i}{scale_i})^2)$$
## IRA:
$$z_i(t) = \frac{s_i(t) - \mu_i}{\sigma_i + \epsilon}
$$$$w_{stab} = \exp(-\sum w_i z_i^2)$$

---

# 6. Validaciones y Métricas

- **Métricas en tiempo real:** `pendulum_angle`, `cart_position`, `force`, `reward`, `stability_score`, `action_k*`, `epsilon`, `learning_rate`, `baseline_value`.
- **Recolección:** cada paso y acumulado por episodio.
- **Resumen:** Excel y JSON por lote, con visualizaciones de trayectoria, recompensa y estabilidad.

---

# 7. Puntos Críticos Observados

- `decision_interval` mal definido puede invalidar las simulaciones `echo`.
- `w_stab` debe balancear sensibilidad y robustez. IRA con stats adaptativas mejora estabilidad.
- `state_config` impacta exponencialmente el tamaño de tablas y la capacidad de generalización.

---

# 8. Estado Actual

✅ Simulador funcional con lógica diferenciada de recompensa  
✅ Integración completa de modos `echo-baseline` y `shadow-baseline`  
✅ Métricas detalladas y trazabilidad del aprendizaje  
⚠️ Costo computacional relevante en modo `echo`, estabiliza rápido pero sobreamortiguado
⚠️ Requiere integración con control dual (próximo sprint)

---

# 9. Posibles Implementaciones Futuras

- Evaluar control dual (`pendulum_angle` y `cart_position`) con dos agentes coordinados.
- Habilitar aprendizaje adaptativo de hiperparámetros (ej: $\epsilon$, $\alpha$).
- Añadir visualización específica de contribuciones de cada ganancia sobre la recompensa.
- Automatizar diagnóstico de convergencia de Q-tables.