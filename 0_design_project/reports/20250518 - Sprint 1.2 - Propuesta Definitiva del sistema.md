# Escalamiento Definitivo

---

## Instrucciones Técnicas (alto nivel)

Declaración de intenciones y conjunto de instrucciones direccionales de alto nivel para la refactorización final. Mediante los _Principios Fundamentales de Diseño_ definidos anteriormente (SRP reforzado, composición, configuración declarativa, fail-fast, etc.) y en asegurar la habilitación para la creación plug-and-play de **nuevos algoritmos, controladores, sistemas, calculadoras y estrategias** sin tocar el núcleo (`main`, `di_container`, factorías, `simulation_manager`).

---

### 1. `simulation_manager.py`

**Objetivo operativo**  
Seguir orquestando episodios y decisiones pero completamente agnóstica a los componentes, es decir, sin ramificaciones por tipo concreto, permitiendo la orquestación de simulaciones de cualquier sistema dinámico, controlador y agentes declarados. Su responsabilidad final es: _“ejecutar la simulación completa con los componentes que el contenedor le entregue”_.

**Puntos de mejora para escalar**

- La inicialización de todo el sistema debe ser realizado una sola vez; instancias, estructuras, llamadas y variables de comunicación deben ser creadas al inicio y no cada vez que se toma una decisión o que se resetea el episodio, ya que la idea es que una vez corrida la simulación solo se itere sobre las métodos directos y explícitos del sistema establecido, asegurando así la velocidad de la simulación de cada episodio.
- `agent_instance_decision.build_agent_state` debe ser utilizado solo para inicializar la estructura, no para construirla en cada `_handle_decision_boundary`
- Eliminar cualquier `isinstance` o import directo de estrategias concretas. Sustituye la comprobación de `EchoBaselineRewardStrategy` por la lectura de un atributo estándar (`needs_virtual_simulation`) ya presente en la interfaz `RewardStrategy`.
- Extraer la lógica de _decision interval loop_ y _episodio_ a métodos auxiliares privados para que distintos `Environment` u otros dominios la puedan sobreescribir vía composición si fuera necesario.
- Simplificar y generalizar la firma de métodos `_run_standard_interval_steps` y `_run_echo_baseline_interval_steps` aceptando un único objeto “runner” (inyectado) que implemente el contrato _IntervalRunner_. Así la aparición de futuras variantes (p.ej. _offline rollouts_) se limita a crear otra clase y declararla en config.
- La evaluación del check termination es responsabilidad del PendulumEnvironment, por lo que solo se debería recibir el estado de terminación y el motivo, no procesar su propio check termination y menos recibir información del agente.

**Consideraciones de integración**  
Mantén la misma firma pública de `run()` para no romper `main.py`. Anota exhaustivamente en los logs cuándo se resuelve cada componente y delega cualquier validación de parámetros a las clases instanciadas.

---

### 2. `pid_qlearning_agent.py`

**Propósito**  
Ser un _orquestador jerárquico_ de tablas Q y estructuras auxiliares requerido por la estrategia de recompensa, sin conocer detalles de la misma.

**Escalabilidad**

- La clase gestiona dinámicamente la creación de tablas Q, auxiliares, la selección de acciones mediante método en función a la política establecida, y la gestión del aprendizaje mediante un orquestador directo y legible.
- `build_agent_state()` debe ser utilizado solo para inicializar la estructura, no para construirla en cada `_handle_decision_boundary`
- Capacidad del agente para gestionar su paciencia (lógica de early_termination); y en función de esta, penalizar las recompensas deben ser debidamente modularizadas como métodos privados.

**Integración**  
Asegúrate de exponer métodos públicos neutros (`get_auxiliary_table_value`, `update_auxiliary_table_value`, etc.) y documentar que cualquier nueva estrategia debe usarlos; evita tocar su firma más adelante.

---

### 3. `pendulum_environment.py`

**Propósito**  
Encapsular la interacción “sistema físico ↔ controlador ↔ reward” sin lógica de aprendizaje.

**Escalabilidad**

- Mantener capa abstracta `Environment` cuyo contrato cubra `step`, `reset`, `check_termination`, `update_reward_calculator_stats`, etc. Los entornos futuros (por ejemplo, “MountainCarEnvironment”) sólo implementan este contrato y se registran en `EnvironmentFactory`.
- Es responsabilidad del Environment gestionar todos los Check termination, por lo que debería recibir el early termination del agente, y manejar los criterios de detención.
- Es responsabilidad del Environment gestionar la conexión con `BaseStabilityCalculator` para calcular la estabilidad del sistema en cada step, así como de entregar su estado al `PendulumEnvironment`.

---

### 4. `pendulum_virtual_simulator.py`

**Intención**  
Ejecutar _rollouts contrafactuales_ totalmente aislados del entorno real.

**Sistema Actual**

- Clase genérica `VirtualSimulator` con un método `run_interval_simulation` basado en un **`DynamicSystem` clonable** y un **`Controller` clonable**.
- Exige que cualquier controlador que pretenda usarse en virtualizaciones implemente `clone()` o sea _deep-copy-safe_; documentarlo en la interfaz.

**Consideraciones**
El sistema actual asegura integridad de las simulaciones, pero se encuentra efectivamente optimizada? es necesario hacer una deepcopy en cada `run_interval_simulation()` o sería mejor solo al inicializar, y luego se le entrega la información necesaria para que realice las simulaciones virtuales?

---

### 5. `instantaneous_reward_calculator.py` 

**Propósito**  
Implementar una _estrategia de cálculo de recompensa_ parametrizada (gaussiana, by-stability, etc.).

**Escalabilidad**

- Mantener desacoplado internamente cada método de cálculo en sub-clases o _policy objects_ y seleccionarlas en el constructor. Así nuevas fórmulas se añaden declarativamente.
- Estandarizar la salida como solo `calculated_reward_value` y no como`(reward, w_stab)` ya que la responsabilidad de evaluar la estabilidad del sistema es de `PendulumEnvironment` mediante el nuevo componente `StabilityCalculator`, por lo tanto, con esta clase es que se debe proponer una conexión directa para utilizar la estabilidad del sistema si es que se ha determinado una recompensa basada en la estabilidad.
- Es responsabilidad de instantaneous_reward_calculator el calcular el valor de la recompensa instantánea (calculated_reward_value) en cada paso, basándose en el método configurado (e.g., weighted_exponential, stability_measure_based).

---

### 6. `ira_stability_calculator.py` 

### 7. `simple_exponential_stability_calculator.py` 

_(juntos por similitud)_

**Propósito conjunto**  
Traducir un estado continuo a un escalar `w_stab ∈ [0, 1]` o a una recompensa basada en estabilidad.

**Escalabilidad**

- Limpiar validaciones y mención a variables específicas del sistema del péndulo, la idea es cuente con la lógica de cálculo y que maneje dinámicamente las variables declaradas en el config (las que podrían ser para cualquier tipo de sistema o cualquier variable que venga de `PendulumEnvironment` ).

---

### 8. `global_reward_strategy.py` 

### 9. `shadow_baseline_reward_strategy.py` 

### 10. `echo_baseline_reward_strategy.py` 

**Rol común**  
Traducir la “información cruda” del intervalo (recompensa real, `w_stab`, tablas auxiliares, simulaciones virtuales…) a la _recompensa que el agente debe usar para actualizar Q_.

**Escalabilidad**

| Requisito                               | Ajuste propuesto                                                                                                                                                                                   |
| --------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Declarar necesidades de infraestructura | Cada estrategia expone `needs_virtual_simulation` y `required_auxiliary_tables`. Esto elimina toda lógica condicional en `SimulationManager` y en el agente.                                       |
| Composición sobre herencia              | Extraer el cálculo “delta B”, “R_diff”, etc. a _helper policies_ que se inyectan mediante el constructor, evitando que cada clase crezca con casos especiales.                                     |
| Config-driven                           | Todos los hiper-parámetros (`beta`, `baseline_init_value`, etc.) deben leerse sin validación cruzada: la clase asume que ya fueron verificados por una función de esquema al cargar `config.yaml`. |
| Acceso a tablas auxiliares              | Usar únicamente los getters/setters genéricos del agente para que la misma estrategia funcione con agentes futuros que no sean PID-centric o para cualquier parámetro entregado por el agente.     |

**Asegurar que**  
La estrategia sigue controlando la actualización, pero la creación la maneja el agente de forma agnóstica.

---

## Consideraciones generales (aplican a todos)

- **Configuración declarativa total**: toda variación (como por ejemplo nuevos agentes DQN, controladores LQR, sistemas “MountainCar”, estrategias “Advantage plus Replay”, etc) debe poder referenciarse sólo añadiendo un bloque en `config.yaml`
- **Factorías como “service locators acotados”**: cada factoría mantiene un registro extensible y no contiene _if-elif_ enormes; las clases se autodetectan.
- **Interfaces estables y mínimas**: cuando sea necesario un nuevo método, introdúcelo en la interfaz y ofrécele una implementación por defecto (p.ej. en una `Mixin`) para no romper clases existentes.
- **Validación fail-fast en constructores**: todo parámetro crítico se verifica tan pronto como la clase se crea. El `SimulationManager` asume que las instancias y parámetros son válidos y no son re-validados en cada llamada. Esto además permite dejar cada método y lógica específica de forma directa y explicita, y así mejorar la legibilidad del código.
- **Logging**: módulos core (manager, factorías) generan _logs de alto nivel_; los componentes especializados sólo registran logs de salida de sus parámetros y métodos claves.
- **Metric collector**: Se gestiona únicamente en el Environment y el SimulationManager , asegurando de colectar de manera eficiente en cada etapa los parámetros pertinentes, como al inicializar el episodio, finalizar un step, intervalo de decisión y episodio.
- **Gestión de Errores**: módulos core (manager, factorías) generan _logs de alto nivel_; los componentes especializados sólo contarán en sus métodos claves un solo `try-except` que permita rastrear el motivo del fallo. Por lo que, se acabaron la gestión de valores específicos y la asignación a valores corruptos que no corresponden a la simulación.
- **Composición interna**: antes de optar por herencia, preferir que cada clase tenga atributos _strategy_ o _policy_ intercambiables declarados en config (p.ej. un `PIDController` podría recibir una política de anti-windup).
