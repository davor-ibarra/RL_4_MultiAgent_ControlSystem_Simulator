# Objetivo

- Escalar el sistema para que **cualquier** componente nuevo se integre ⊂ _core_ **sin tocar** `main`, `di_container`, _interfaces_ ni _factorías_, por lo tanto, en adelante las mejoras se aplican una sola vez; solo con **(a)** añadir la clase nueva en `components/…` y **(b)** declararla en `config.yaml`.
- Incluir un parámetro adaptativo de paciencia que evite recorrer aprendizaje erróneo luego de un determinado número de decisiones sin mejorar el sistema.
## 1. Escalamiento definitivo

Refactorizar Factorías para que solo se validen parámetros en los componentes y no en la fábrica, permitiendo la inclusión de nuevos componentes sin necesidad de modificar el core
Para garantizar que el proyecto pueda recibir a futuro, nuevos cálculos de estabilidad, nuevas funciones de recompensa instantánea y estrategias de asignación de recompensa sin tocar los archivos **core**, cada pieza del código debe cumplir requisitos muy concretos.

El **`simulation_manager.py`** debe operar siempre con las interfaces genéricas que recibe desde la inyección de dependencias y no contener referencias directas a implementaciones concretas ni comprobaciones de tipo condicionales. Todas las rutas de ejecución —incluidas las que hoy distinguen entre modos Echo, Shadow u otros— tienen que basarse únicamente en la información que exponga la estrategia a través de atributos o de un contexto que ella misma devuelva. De esta forma, el _manager_ seguirá orquestando episodios y fronteras de decisión sin modificaciones cuando aparezcan estrategias que todavía no existen.

En **`pid_qlearning_agent.py`** la responsabilidad de decidir cómo se actualizan tablas auxiliares, baselines o cualquier estado interno extra debe recaer en la estrategia de recompensa que se le inyecta. El agente, por tanto, sólo puede conservar estructuras genéricas (p. ej., diccionarios o tensores etiquetados) y ofrecer métodos de acceso neutros; nunca debe crear ni gestionar buffers específicos para una estrategia concreta. Cualquier ajuste de exploración, tasas de aprendizaje u otras políticas adaptativas deberá solicitarse de forma declarativa a través de la estrategia para que el agente permanezca inalterado ante la llegada de nuevas variantes.

El archivo **`instantaneous_reward_calculator.py`** debe exponer un orquestador claro que permita declarar mediante configuración, tantos modos de cálculo como se desee. Será imprescindible que exista un registro verificable de nombres de método, que la selección del cálculo se realice en tiempo de carga a partir de ese registro y que cualquier método adicional pueda incorporarse tan solo sumando su nombre en la configuración y definiendo la lógica en un módulo de componentes o método privado provisorio, sin editar el archivo central.

Cada clase ubicada bajo **`components/reward_strategies`** ha de ajustarse estrictamente a la interfaz de estrategia única. Debe proporcionar un método unificado de cálculo de la recompensa de aprendizaje y, opcionalmente, atributos declarativos —por ejemplo, indicadores de si la estrategia necesita un simulador virtual o qué forma de memoria suplementaria ocupará en el agente. Toda la información adicional que el _Simulation Manager_ o el agente requieran debe viajar a través de estos atributos para prevenir dependencias explícitas entre archivos.

Las distintas implementaciones de **calculadoras de estabilidad** deben garantizar que cualquier valor de estabilidad instantánea se devuelva en un rango normalizado y que, cuando precisen estadísticos adaptativos, ofrezcan puntos de actualización independientes del resto de la lógica. Todos sus parámetros han de recibirse exclusivamente desde la configuración y validarse de modo estricto en el constructor, evitando que la aplicación arranque con combinaciones inválidas.

El **`pendulum_virtual_simulator.py`** tiene que ofrecer a las estrategias un método genérico de simulación contrafactual que acepte controladores, listas arbitrarias de ganancias y número de pasos, sin presumir cuántos contrafactuales se van a lanzar ni qué métricas se extraerán de ellos. De este modo, cualquier estrategia futura que exija réplicas virtuales distintas de las actuales podrá reutilizar el simulador sin ajustes.

Por último, el archivo **`config.yaml`** debe reflejar con claridad la correspondencia uno‑a‑uno entre cada clave declarada y la implementación concreta que se cargará en tiempo de ejecución. Todas las secciones de configuración dentro de `calculation`en `reward_setup` o en `stability_calculator` tienen que admitir `enabled_*`, `type`, `*_name`, bloques `params` independientes para que los nuevos componentes reciban sus propios hiperparámetros sin colisionar con los de versiones anteriores. Con este diseño declarativo, la simple adición de nuevas claves y parámetros en el YAML bastará para activar componentes inéditos, manteniendo intactos los módulos principales del sistema.


## 2. Early Termination Adaptativo con Penalización Suave

### 1. Definiciones de Variables

- $K^g_t$: valor de la ganancia $(g\in {p,i,d})$ en el paso $t$.  
- $r^g_t = r^g_{ins}(t)$: recompensa instantánea observada tras aplicar la acción en el paso $t$ para cada $g$.  
- $w^g_t = w^g_{\mathrm{stab}}(s_t)$: métrica de “estabilidad” o señal de recompensa utilizada para valorar mejora de cada $g$.
- $\Delta w^g_t = w^g_t - w^g_{t-1} \vee \Delta r^g_t​=r^g_t​−r^g_{t−1​}$: cambio de la métrica de mejoría entre pasos consecutivos para cada $g$.
- $\vec{m^g_{c}}(n)$: vector de mejoría del intervalo $n$ para cada $g$.
- $\bar{m^g_{e}}$: último promedio registrado de la mejoría en el episodio $e$ para cada $g$.
- $c_n \in\mathbb{N}$: contador de pasos de decisión.
- $\hat{c}^g_n\in\mathbb{N}$: contador de pasos **sin mejora** para el agente $g$.  
- $M^g\in\mathbb{N}$: **paciencia** o número máximo de pasos sin mejora antes de permitir terminación para cada $g$. 
- $\beta^g_{n}\in[0,1]$: magnitud de la **penalización suave** al no mejorar entre decisiones por cada $g$.
- $\alpha$: tasa de aprendizaje de Q-learning.  
- $\gamma$: factor de descuento de Q-learning.
- $\epsilon$: Epsilon Greedy de Q-learning.
- $\delta_n$: TD target en la decisión $n$ de Q-learning.

### 2. Inicialización de Parámetros desde config

1. Inicializar paciencia inicial de cada $g$:  
   $$
     M^g_{0}
   $$
2. Inicializar penalización suave de cada $g$:  
   $$
     \beta^g_{n} = \beta^g_{0}
   $$
3. inicializar tasas de Q-learning:  
   $$
     \alpha_{0} \ ,\ \gamma \ ,\ \epsilon_{0}
   $$
4. Resetear parámetros relevantes del episodio anterior.

### 3. Lógica del Paso de Tiempo

Para cada agente \(g\) y paso \(t\):

1. Ejecutar acción  
   $$
     a^g_t \;\in\;\{-\Delta,\,0,\,+\Delta\}.
   $$
2. Observar nuevo estado $$s_{t+1}$$ y recompensa $$r_{ins}(t)$$
### 4. Cálculo de Mejoría y Actualización del Contador

1. Calcular la métrica de estabilidad o recompensa:  
   $$
     w^g_t = w^g_{\mathrm{stab}}(s_t)
     \quad \lor \quad r^g_t = r^g_{ins}(t)
   $$
2. Calcular el cambio:  
   $$
     \Delta w^g_t = w^g_t - w^g_{t-1}.
   $$
O 
$$\Delta r^g_t​=r^g_t​−r^g_{t−1​}$$
Y guardar en vector de mejoría del intervalo $n$.
$$\vec{m}^g_{c}(n) = [\Delta w^g_t\ , \Delta w^g_{t+1}\ , \ \dots \ , \Delta w^g_n]$$
O
$$\vec{m}^g_{c}(n) = [\Delta r^g_t\ , \Delta r^g_{t+1}\ , \ \dots \ , \Delta r^g_n]$$
3. Luego, si se encuentra en `_handle_decision_boundary` ($t=n$):
- Actualiza el último promedio de mejoría registrado para el episodio y registrar en $\vec{m}^g_{e}$ :
$$\bar{m}^g_{e} = \bar{m}^g(c_{n})$$
- Calcular recompensa acumulada del intervalo de decisión:
$$r^g_{n} = \sum_{t=0}^{n} r^g_{t}$$
- Calcular el promedio del intervalo:
$$\bar{m}^g(c_{n}) = mean(\vec{m}^g_{c}(n))$$
- Actualizar contador si:
   $$
   \hat{c}^g_n =
   \begin{cases}
     0, & \bar{m}^g(c_{n}) > \bar{m}^g(c_{n-1}) \\
     \hat{c}^g_{n-1} + 1, & \bar{m}^g(c_{n}) \le \bar{m}^g(c_{n-1})
   \end{cases}
   $$

### 5. Evaluación de Early Termination

- **Condición**: si 
$$\hat{c}^g_n \ge M^g \ \Rightarrow early\_termination = True$$
El episodio se prepara para **terminar** y comenzar uno nuevo, de lo contrario `continue`.

### 6. Penalización Suave y Recompensa Modificada

Si $early\_termination = False \ \land \ \bar{m}^g(c_{n}) < \bar{m}^g(c_{n-1})$ entonces:
1. Se actualiza el coeficiente de penalización suave por no mejoría:
$$\beta^g_{n}\ = \beta^g_{n-1} * \frac{M^g - \hat{c}^g_n}{M^g}$$
2. Recompensa modificada dentro del episodio:
   $$
     R_{learn}^{'g} = r^g_n * \beta^g_{n}
   $$
(Coeficiente de penalización suave vuelve a $\beta^g_{0}$ cuando $\hat{c}^g_n = 0$)
### 7. Actualización de Q-Learning

1. **Paso actualización normal**:  
   $$
   {\delta}^g_n = R^{'g}_{learn} + \gamma\,\max_{a'}Q^g(s_{n+1},a_{n+1}) - Q^g(s_n,a_n),
   $$
   $$
   Q^g(s_n,a_n) \leftarrow Q^g(s_n,a_n) + \alpha\,\delta_n^g.
   $$
2. **Terminación temprana** (cualquiera sea el motivo):  
   $$
   {\delta}^g_n = R_{learn}^{'g} - Q^g(s_n,a_n),
   $$
   $$
   Q^g(s_n,a_n) \leftarrow Q^g(s_n,a_n) + \alpha\,\delta_n^g.
   $$

### 8. Ajuste de la Paciencia

Antes de terminar el episodio si es que el parámetro de paciencia en `config.yaml` se encuentra en `type:adaptative` se debe ajustar la paciencia $M^g$ de la siguiente forma :

1. **Comparar mejoría entre episodios consecutivos**:
$$\Delta \bar{m}^g_e = \bar{m}^g_{e} - \bar{m}^g_{e-1}$$​
donde $\bar{m}^g_{e}$​ representa el promedio de mejoría del episodio actual para el agente $g$.

2. **Actualizar paciencia en función de la tendencia**:
$$M^g \leftarrow \mathrm{clip}\bigl( M^g + \eta \cdot \mathrm{sign}(\Delta \bar{m}^g_e), M^g_{\min},\,M^g_{\max} \bigl)$$
donde $\eta \ge 1$ es la tasa de ajuste, $\mathrm{sign}(\cdot)$ indica la dirección del cambio (+1 o -1), y $\mathrm{clip}(\cdot)$ asegura que $M^g$ se mantenga dentro de los límites definidos.

3. **Consideración del progreso global (opcional)**:  

Considerar la etapa del entrenamiento según $\epsilon_{n}-greddy$​, se puede definir el valor base de $M^g$ como:

$$M^g_{base} = M^g_{\max} \cdot \epsilon_t + M^g_{\min} \cdot (1 - \epsilon_t)$$
y aplicar la actualización como:
$$M^g \leftarrow \mathrm{clip}\bigl( M^g_{base} + \eta \cdot \mathrm{sign}(\Delta \bar{m}^g_e), M^g_{\min},\,M^g_{\max} \bigr)$$


