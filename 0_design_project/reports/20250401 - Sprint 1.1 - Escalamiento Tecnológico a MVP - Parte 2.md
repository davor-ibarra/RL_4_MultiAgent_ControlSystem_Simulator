---
created: 
update: 20250401-08:17
summary: 
status: 
link: 
tags: 
---
## Documento del Estado Actual del Proyecto

**1. Objetivo del Proyecto:**

- El objetivo principal es entrenar un agente de Aprendizaje por Refuerzo (RL), específicamente usando Q-learning, para adaptar dinámicamente las ganancias (Kp, Ki, Kd) de un controlador PID.
    
- El sistema objetivo es un péndulo invertido sobre un carro, donde el controlador PID busca estabilizar el péndulo en la posición vertical (ángulo 0).
    
- El agente aprende a ajustar las ganancias PID basándose en la retroalimentación recibida del entorno en forma de recompensas, con el objetivo de maximizar la recompensa acumulada (que está diseñada para correlacionarse con la estabilidad y eficiencia del control).
    

**2. Arquitectura y Componentes:**

- **Diseño Modular:** El proyecto utiliza un diseño modular robusto basado en el patrón **Factory** para crear componentes (Environment, System, Controller, Agent, RewardFunction) a partir de la configuración (config.yaml).
    
- **Interfaces (ABCs):** Se definen Interfaces (Clases Base Abstractas) para los componentes clave (interfaces/), lo que promueve la consistencia y facilita la extensibilidad (por ejemplo, añadir nuevos tipos de agentes, sistemas o funciones de recompensa).
    
- **Componentes Principales:**
    
    - main.py: Orquestador principal. Carga la configuración, inicializa el entorno y componentes, ejecuta el bucle de simulación (episodios y pasos), gestiona la recolección de métricas, guarda resultados y estado del agente, y genera visualizaciones.
        
    - PendulumEnvironment: Implementa la interfaz Environment. Coordina la interacción entre el sistema, el controlador y el agente. Aplica las acciones del agente (cambios en ganancias), ejecuta un paso de simulación, calcula la recompensa y verifica las condiciones de terminación.
        
    - InvertedPendulumSystem: Implementa DynamicSystem. Modela la física del péndulo invertido usando ecuaciones diferenciales ordinarias (ODEs) y las integra (odeint) para simular la dinámica.
        
    - PIDController: Implementa Controller. Calcula la fuerza de control PID basándose en el estado actual y sus ganancias. Permite la actualización de ganancias por parte del agente.
        
    - PIDQLearningAgent: Implementa RLAgent. El "cerebro" del sistema.
        
        - Utiliza Q-learning para aprender políticas de ajuste de ganancias PID.
            
        - **Actualización Clave:** Internamente, ahora utiliza **arrays de NumPy** para almacenar las tablas Q y los contadores de visitas por eficiencia. Mantiene una tabla Q y un contador de visitas separados para cada ganancia habilitada (kp, ki, kd).
            
        - El estado que utiliza para tomar decisiones y aprender depende de state_config en config.yaml. Actualmente, según config.yaml, el estado para cada tabla Q de ganancia solo incluye las propias ganancias (kp, ki, kd), lo cual es una simplificación importante (no considera directamente el estado físico como ángulo/velocidad para decidir el cambio de ganancia).
            
        - Implementa la lógica de selección de acción (epsilon-greedy), aprendizaje (actualización Q), y decaimiento de epsilon/learning rate.
            
        - **Nueva Funcionalidad:** Incluye un método (get_agent_state_for_saving) que transforma los arrays internos de NumPy a **diccionarios anidados** (con claves de string representando valores discretizados) para poder guardarlos en formato JSON.
            
    - GaussianReward: Implementa RewardFunction. Calcula la recompensa basándose en la estabilidad del péndulo, el uso de fuerza y (opcionalmente) la estabilidad del carro, usando funciones gaussianas.
        
    - SimpleMetricsCollector: Implementa MetricsCollector. Almacena métricas de series temporales durante cada episodio.
        
- **Utilidades:** Módulos auxiliares para:
    
    - data_processing.py: Calcular resúmenes por episodio y guardar una tabla de resumen (.xlsx).
        
    - visualization.py: Generar gráficos (recompensa/tiempo acumulado, heatmaps de trayectoria).
        
    - episode_saver.py: Guardar datos detallados de lotes de episodios y metadatos de simulación (.json).
        
    - agent_state_manager.py: **Nueva adición**. Guarda el estado interno transformado del agente (tablas Q, contadores de visita) periódicamente.
        
    - numpy_encoder.py: Codificador JSON personalizado para manejar tipos de datos NumPy al guardar.
        
- **Configuración:** Centralizada en config.yaml. Define todos los parámetros del entorno, sistema, controlador, agente, simulación, condiciones iniciales, adaptación PID y criterios de estabilización.
    

**3. Estado Actual y Funcionalidad:**

- El código parece **completo y funcional** para el flujo de trabajo definido.
    
- La simulación se ejecuta episodio por episodio, paso a paso.
    
- El agente toma decisiones sobre las ganancias PID a intervalos definidos (decision_interval).
    
- El aprendizaje (actualización de tablas Q) ocurre en esos mismos intervalos.
    
- Se implementa la **discretización del espacio de estados** (basado en state_config) para las tablas Q. El agente mapea los valores continuos del estado (actualmente solo las ganancias según config.yaml) a índices de bins enteros para acceder a los arrays NumPy.
    
- Se gestiona el **decaimiento de Epsilon y Learning Rate** para equilibrar exploración y explotación.
    
- Se implementa el **guardado periódico del estado del agente** (Tablas Q y contadores de visita) en formato JSON, lo que permite reanudar entrenamientos o analizar el aprendizaje. La transformación de NumPy a dict es crucial aquí.
    
- Se realiza el **guardado de datos detallados** por lotes y un resumen general al final.
    
- Se generan **visualizaciones** para analizar el rendimiento (recompensa, duración, heatmaps).
    
- El manejo de **errores y logging** parece robusto en main.py y otros módulos.
    

**4. Puntos Clave y Consideraciones (Basado en Código y Config):**

- **Representación del Estado del Agente:** La configuración actual (state_config con enabled: false para ángulo/velocidad) implica que la decisión de ajustar Kp, Ki, o Kd solo depende de los valores actuales de Kp, Ki, y Kd. Esto podría ser insuficiente. ¿Debería el error actual del ángulo, o la velocidad angular, influir directamente en si se debe aumentar/disminuir una ganancia específica? El código (build_agent_state, get_discrete_state_indices_tuple) está preparado para incluir estas variables si se habilitan en el config.yaml, pero la configuración actual no lo hace. Este es el punto de diseño más crítico a evaluar.
    
- **Eficiencia vs. Almacenamiento:** El uso de NumPy para las tablas Q es bueno para la velocidad de acceso durante el aprendizaje. La transformación a diccionarios para guardar es necesaria para JSON pero puede generar archivos grandes si el espacio de estados es grande (muchas variables habilitadas o muchos bins).
    
- **Inicialización de Ganancias:** reset_gains_each_episode: true significa que cada episodio empieza desde cero en términos de ganancias PID. Esto es bueno para explorar desde el principio en cada episodio, pero si se quisiera que el agente aprendiera una sintonización "final" persistente entre episodios, esto debería ser false.
    
- **Función de Recompensa:** La GaussianReward es flexible, pero los pesos (weights) y escalas (scales) probablemente requieran ajuste fino para guiar al agente de manera efectiva.
    
- **Hiperparámetros:** Las tasas de decaimiento de epsilon/LR, el factor de descuento, y los propios gain_step son hiperparámetros clave que afectan el aprendizaje. Los valores actuales parecen ajustados para 10k episodios, pero su rendimiento debe ser monitorizado.
    

**5. Próximos Pasos / Posibles Mejoras:**

- **Evaluar la Representación del Estado:** Experimentar habilitando variables de estado físico (ángulo, velocidad angular) en state_config para ver si mejora el aprendizaje del agente.
    
- **Ajuste Fino:** Ajustar los pesos/escalas de la recompensa y los hiperparámetros de RL.
    
- **Análisis del Estado Guardado:** Desarrollar herramientas o scripts para cargar y analizar los archivos agent_state_episode_*.json para entender mejor cómo evolucionan las tablas Q.
    
- **Pruebas de Robustez:** Evaluar el rendimiento del controlador sintonizado con diferentes condiciones iniciales o perturbaciones.
    

## Flujo Detallado de Datos Estructurado

Aquí se describe cómo fluyen los datos a través de los archivos y funciones clave durante una ejecución típica:

**Fase 1: Inicialización (en main.py)**

1. **main.py**: Lee config.yaml -> config (dict).
    
2. **main.py -> EnvironmentFactory.create_environment(config)**:
    
    - **EnvironmentFactory**: Pasa partes relevantes de config a otras fábricas.
        
    - **SystemFactory.create_system(...)**: Crea InvertedPendulumSystem (objeto con parámetros físicos).
        
    - **ControllerFactory.create_controller(...)**: Crea PIDController (objeto con ganancias iniciales, setpoint, dt).
        
    - **AgentFactory.create_agent(...)**:
        
        - Crea PIDQLearningAgent (objeto).
            
        - **PIDQLearningAgent.__init__**: Usa state_config de config para determinar dimensiones. Inicializa q_tables_np y visit_counts_np como **arrays NumPy** llenos de q_init_value / visit_init_value para cada ganancia habilitada. Define ordered_state_vars_for_gain.
            
    - **RewardFactory.create_reward_function(...)**: Crea GaussianReward (objeto con pesos y escalas).
        
    - **EnvironmentFactory**: Crea PendulumEnvironment (objeto env que contiene System, Controller, Agent, RewardFunction, dt, etc.).
        
    - Retorna el objeto env a main.py.
        
3. **main.py**: Crea results_folder.
    
4. **main.py -> episode_saver.save_metadata(config, results_folder)**: Guarda config en metadata.json.
    
5. **main.py**: Extrae parámetros de simulación (max_episodes, dt, decision_interval, etc.) de config.
    

**Fase 2: Bucle de Episodios (en main.py)**

Para cada episode de 0 a max_episodes-1:

1. **main.py -> env.reset(initial_state_vector)**:
    
    - **PendulumEnvironment.reset**:
        
        - Llama a self.system.reset(initial_conditions) -> retorna initial_state (array NumPy). Lo guarda en self.state.
            
        - Resetea self.t = 0.0.
            
        - Si self.reset_gains es True, llama a self.controller.reset() (restaura Kp, Ki, Kd iniciales).
            
        - Llama a self.agent.reset_agent() (actualiza self.epsilon y self.learning_rate según decaimiento).
            
    - Retorna state_vector (array NumPy del estado inicial) a main.py.
        
2. **main.py**: Inicializa metrics_collector, cumulative_reward, interval_reward, next_decision_time.
    
3. **main.py**: Loguea estado inicial (tiempo=0, componentes de state_vector, ganancias del env.controller, epsilon/lr del env.agent) usando metrics_collector.log(nombre, valor).
    
4. **main.py -> `env.agent.build_agent_state(state_vector, env.controller, agent_params_cfg['state_config']`)**:
    
    - **PIDQLearningAgent.build_agent_state**: Construye un **diccionario** (current_agent_state_dict) con los valores continuos de las variables habilitadas en state_config (actualmente, solo {'kp': controller.kp, 'ki': controller.ki, 'kd': controller.kd}).
        
    - Retorna current_agent_state_dict a main.py.
        
5. **main.py -> env.agent.select_action(current_agent_state_dict)**:
    
    - **PIDQLearningAgent.select_action**:
        
        - Para cada ganancia g habilitada:
            
            - Llama a self.get_discrete_state_indices_tuple(current_agent_state_dict, g) -> obtiene state_indices (tupla de **índices enteros**) para esa ganancia.
                
            - Genera rand(). Si < self.epsilon (exploración): elige action_index aleatorio (0, 1, o 2).
                
            - Si > self.epsilon (explotación): Accede a q_values = self.q_tables_np[g][state_indices]. Encuentra action_index = np.argmax(q_values).
                
        - Retorna actions (dict: {'kp': action_idx, 'ki': action_idx, 'kd': action_idx}) a main.py.
            
6. **main.py**: Loguea las acciones iniciales seleccionadas (action_kp, etc.) usando metrics_collector.log.
    

**Fase 3: Bucle de Pasos Dentro del Episodio (en main.py)**

Para cada t_step de 0 a max_steps-1:

1. **main.py**: Calcula current_time = (t_step + 1) * dt.
    
2. **main.py -> env.step(actions)**:
    
    - **PendulumEnvironment.step**:
        
        - Obtiene kp, ki, kd actuales de self.controller.
            
        - Interpreta actions (dict de índices 0, 1, 2) para modificar kp, ki, kd usando self.gain_step.
            
        - Aplica clipping a las nuevas ganancias usando los límites min/max de self.agent.state_config.
            
        - Llama a self.controller.update_params(new_kp, new_ki, new_kd).
            
        - Llama a self.controller.compute_action(self.state) -> calcula force (float).
            
        - Llama a self.system.apply_action(self.state, force, self.t, self.dt):
            
            - **InvertedPendulumSystem.apply_action**: Llama a `odeint(self.dynamics, state, [t, t+dt], args=(force,))` -> retorna next_state (array NumPy). Normaliza el ángulo.
                
        - Llama a self.reward_function.calculate(self.state, force, next_state):
            
            - **GaussianReward.calculate**: Usa state, force, next_state, self.weights, self.scales -> retorna reward (float).
                
        - Actualiza self.state = next_state y self.t += self.dt.
            
    - Retorna next_state_vector (array NumPy), reward (float), force (float) a main.py.
        
3. **main.py**: Actualiza cumulative_reward += reward, interval_reward += reward.
    
4. **main.py**: Loguea métricas del paso actual (tiempo, componentes de next_state_vector, Kp/Ki/Kd aplicadas, epsilon/lr, reward, cumulative_reward, force) usando metrics_collector.log.
    
5. **main.py -> env.check_termination(config)**:
    
    - **PendulumEnvironment.check_termination**: Compara self.state (ángulo, posición) con límites de config. Compara self.state (ángulo, vel angular) con umbrales de estabilización de config.
        
    - Retorna angle_exceeded, cart_exceeded, stabilized (booleanos) a main.py.
        
6. **main.py**: Verifica si angle_exceeded o cart_exceeded o stabilized o time_limit_reached. Si alguna es True, setea done = True y termination_reason (string).
    
7. **main.py**: Comprueba si current_time >= next_decision_time o si done.
    
    - **Si es momento de decisión/aprendizaje o fin de episodio:**
        
        - **main.py -> env.agent.build_agent_state(next_state_vector, env.controller, ...)**: Obtiene next_agent_state_dict (dict continuo).
            
        - **main.py -> env.agent.learn(current_agent_state_dict, actions, interval_reward, next_agent_state_dict, done)**:
            
            - **PIDQLearningAgent.learn**:
                
                - Obtiene current_state_indices (tupla int) de current_agent_state_dict.
                    
                - Obtiene next_state_indices (tupla int) de next_agent_state_dict.
                    
                - Para cada ganancia g:
                    
                    - Obtiene action_taken_idx de `actions[g]`.
                        
                    - Si done, td_target = interval_reward.
                        
                    - Si no done, next_q_values = `self.q_tables_np[g][next_state_indices]`, max_next_q = np.max(next_q_values), td_target = interval_reward + self.discount_factor * max_next_q.
                        
                    - current_q = `self.q_tables_np[g][current_state_indices + (action_taken_idx,)]`.
                        
                    - td_error = td_target - current_q.
                        
                    - new_q = current_q + self.learning_rate * td_error.
                        
                    - Actualiza **`self.q_tables_np[g][current_state_indices + (action_taken_idx,)] = new_q**`.
                        
                    - Incrementa **`self.visit_counts_np[g][current_state_indices + (action_taken_idx,)]` += 1**.
                        
        - **Si done es False:**
            
            - **main.py -> env.agent.select_action(next_agent_state_dict)**: Selecciona las nuevas actions (dict de índices) para el siguiente intervalo.
                
            - **main.py**: Loguea las nuevas actions seleccionadas (action_kp, etc.).
                
        - **Si done es True:** Loguea acciones NaN.
            
        - **main.py**: Resetea interval_reward = 0. Calcula next_decision_time.
            
        - **main.py**: Actualiza current_agent_state_dict = next_agent_state_dict.
            
    - **Si NO es momento de decisión:** Loguea las acciones actualmente activas (action_kp, etc.).
        
8. **main.py**: Si done, loguea razón de terminación y break del bucle de pasos.
    
9. **main.py**: Actualiza state_vector = next_state_vector para el siguiente paso.
    

**Fase 4: Fin del Episodio (en main.py)**

1. **main.py**: Si el bucle terminó por max_steps, asegura que termination_reason sea "time_limit".
    
2. **main.py -> metrics_collector.get_metrics()**: Obtiene episode_data (dict: {'metric_name': `[lista_valores]`}).
    
3. **main.py**: Añade episode y termination_reason a episode_data.
    
4. **main.py**: Appendea episode_data a episode_batch (lista de dicts) y all_episodes_data (lista de dicts).
    
5. **main.py -> data_processing.summarize_episode(episode_data)**:
    
    - **summarize_episode**: Calcula agregados (mean, std, min, max, total, final) -> retorna summary (dict de escalares).
        
6. **main.py**: Appendea summary a summary_data (lista de dicts).
    
7. **main.py**: Si (episode + 1) % episodes_per_file == 0 o es el último episodio:
    
    - **main.py -> episode_saver.save_episode_batch(episode_batch, results_folder, episode)**:
        
        - **save_episode_batch**: Guarda episode_batch (lista de dicts detallados) en simulation_data_X_to_Y.json usando NumpyEncoder.
            
    - **main.py**: Limpia episode_batch = [].
        
8. **main.py**: Si save_agent_state_flag y (episode + 1) % agent_state_save_frequency == 0:
    
    - **main.py -> agent_state_manager.save_agent_state(env.agent, episode, results_folder)**:
        
        - **save_agent_state**: Llama a agent.get_agent_state_for_saving():
            
            - **PIDQLearningAgent.get_agent_state_for_saving**:
                
                - Itera sobre las ganancias habilitadas.
                    
                - Llama a self._build_nested_dict_from_np(`self.q_tables_np[g]`, ordered_vars) -> Transforma el array NumPy `Q[g]` a un **diccionario anidado** q_dict_g.
                    
                - Llama a `self._build_nested_dict_from_np(self.visit_counts_np[g]`, ordered_vars) -> Transforma el array NumPy `V[g]` a un **diccionario anidado** v_dict_g.
                    
                - (**_build_nested_dict_from_np** usa recursión y **_get_representative_value_from_index** para crear las claves string).
                    
            - Retorna agent_state_data = {'q_tables': {g: q_dict_g, ...}, 'visit_counts': {g: v_dict_g, ...}}.
                
        - **save_agent_state**: Añade episode al dict. Guarda data_to_save en agent_state_episode_{episode}.json usando NumpyEncoder.
            

**Fase 5: Fin del Entrenamiento (en main.py)**

1. **main.py**: Si summary_data no está vacío:
    
    - Crea summary_df = pd.DataFrame(summary_data).
        
    - **main.py -> data_processing.save_summary_table(summary_data, summary_file_path)**:
        - **save_summary_table**: Convierte summary_data (lista de dicts) a DataFrame. Guarda el DataFrame en summary.xlsx.
    - **main.py -> Funciones de visualization**:
        - Llama a plot_cumulative_reward(summary_df, ...)
        - Llama a plot_cumulative_time(summary_df, ...)
        - Llama a plot_time_angle_heatmap(all_episodes_data, ...)
        - Llama a plot_time_angle_vel_heatmap(all_episodes_data, ...)
        - Llama a plot_angle_angle_vel_heatmap(all_episodes_data, ...)
        - (Cada función de plot guarda una imagen .png en results_folder).
2. **main.py**: Loguea mensaje final.