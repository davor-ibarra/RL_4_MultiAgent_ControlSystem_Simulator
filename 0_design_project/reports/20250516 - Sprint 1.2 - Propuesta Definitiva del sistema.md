# Escalamiento Definitivo
## Principios Fundamentales de Diseño para la Refactorización:

1. **Estructura de responsabilidades**: Las interfaces deben mantenerse practicamente igual, pero se debe asegurar que la creación de nuevos componentes sea declarativa a nivel de config y siendo el componente quien gestiona las variables y el uso de parámetros, por lo que el core central (main, di_container, simulation_manager, config_loader y logging_configurator) solo debe ser capaz de instanciar, resolver y orquestar el macro flujo.
2. **Composición sobre Herencia (donde sea práctico):** Los componentes principales se construirán ensamblando sub-componentes (estrategias, calculadoras específicas) o módulos (métodos privados) en lugar de tener múltiples clases heredadas para cada variación. Se espera que 
3. **Configuración Declarativa Total:** config.yaml define QUÉ componentes y sub-componentes usar y CÓMO configurarlos. El código define CÓMO funcionan e interactúan.
4. **Responsabilidad Única (SRP) Reforzada:** Cada clase o método (componente, sub-componente, factoría, helper) tiene una responsabilidad clara y bien delimitada en el sistema, por lo que cualquier lógica necesaria, contará con métodos privados para resolverlas. 
5. **Estructura interna de factorías, simulation_manager y componentes como orquestadores jerárquicos:** Se requiere que cada clase se construyan como orquestadores jerárquicos (osea que orquesten otros sub orquestadores principales de estrategias específicas con módulos o métodos privados reutilizables) para el manejo de estrategias dinámicas y componentes plug and play explícitos y directos sin necesidad de modificar el core
6. **Constructor Responsable de Validación:** Cada clase es responsable de validar los params que recibe en su constructor. Si los parámetros son inválidos para su funcionamiento, lanza un error (Fail-Fast).
7. **Logging explícitos e informativos de alto y medio nivel con alta precisión del contexto:** Los logs de las clases core deben ser capaces de identificar las fuentes y contexto para la depuración, pero en el caso de los componentes se limitarán solo a la entrada y/o salida de sus métodos claves (orquestadores) para validar la integridad de las simulaciones (entradas y salidas de datos) pero sin obstruir la lógica principal o validar tipos de datos.
8. **Manejo de Errores Estricto Durante Simulación:** Si durante la ejecución de un episodio un componente encuentra un estado de datos irrecuperable o un fallo de cálculo que compromete la integridad, el método sin gestionar un valor válido corrupto lanza una excepción solo para logear el fallo y su contexto. SimulationManager hacia arriba deben manejar esto a nivel de episodio, pero los componentes deben ser limpios, directos y explícitos, facilitando la lectura del código y con esto su extensibilidad (no se requiere validar tipos de datos). Se debe evitar el uso de excepciones que disminuyan la integridad de las simulaciones y sobrevalidar parámetros que ya son validados en los init de cada clases y en los métodos de alto nivel (se prefiere fail-fast antes que asignar números corruptos que no corresponden a la simulación).


---

## Instrucciones Técnicas (alto nivel)

Declaración de intenciones y conjunto de instrucciones direccionales para la refactorización final. Mediante los _Principios Fundamentales de Diseño_ definidos anteriormente (SRP reforzado, composición, configuración declarativa, fail-fast, etc.) y me concentro en cómo habilitar la creación plug-and-play de **nuevos algoritmos, controladores, sistemas, calculadoras y estrategias** sin tocar el núcleo (`main`, `di_container`, factorías, `simulation_manager`).

---

### 1. `simulation_manager.py`

**Objetivo operativo**  
Seguir orquestando episodios y decisiones sólo a través de **interfaces** inyectadas, sin ramificaciones por tipo concreto. Su responsabilidad final es: _“ejecutar la simulación completa con los componentes que el contenedor le entregue”_.

**Puntos de mejora para escalar**

- Eliminar cualquier `isinstance` o import directo de estrategias concretas. Sustituye la comprobación de `EchoBaselineRewardStrategy` por la lectura de un atributo estándar (`needs_virtual_simulation`) ya presente en la interfaz `RewardStrategy`.
    
- Extraer la lógica de _decision interval loop_ y _episodio_ a métodos auxiliares privados para que distintos `Environment` u otros dominios la puedan sobreescribir vía composición si fuera necesario.
    
- Simplificar la firma de métodos `_run_standard_interval_steps` y `_run_echo_baseline_interval_steps` aceptando un único objeto “runner” (inyectado) que implemente el contrato _IntervalRunner_. Así la aparición de futuras variantes (p.ej. _offline rollouts_) se limita a crear otra clase y declararla en config.
    

**Dependencias afectadas**  
`RewardStrategy` debe exponer:

`needs_virtual_simulation: bool required_auxiliary_tables: List[str]`

Las factorías siguen igual; el contenedor sólo leerá el nuevo atributo.

**Consideraciones de integración**  
Mantén la misma firma pública de `run()` para no romper `main.py`. Anota exhaustivamente en los logs cuándo se resuelve cada componente y delega cualquier validación de parámetros a las clases instanciadas.

---

### 2. `pid_qlearning_agent.py`

**Propósito**  
Ser un _orquestador jerárquico_ de tablas Q y estructuras auxiliares requerido por la estrategia de recompensa, sin conocer detalles de la misma.

**Mejoras clave**

- Sustituir la generación manual de tablas auxiliares por recorrer `reward_strategy.required_auxiliary_tables`. Así, si mañana aparece una estrategia “Advantage Buffer”, el agente las crea sin cambios de código.
    
- Extraer a estrategias la responsabilidad de _“norma de actualización”_ (por ejemplo, early-termination, penalizaciones, baseline). El agente sólo debe proporcionar métodos CRUD genéricos sobre Q, visitas y tablas auxiliares.
    
- Mover los _hyper-parameters_ (`epsilon*`, `learning_rate*`, etc.) a la sección config y leerlos en el constructor desde un diccionario.
    

**Impacto**  
`RewardStrategy` se convierte en el único actor que sabe interpretar `current_state_indices`, `actions_dict`, etc. La factoría de agentes sólo inyecta los parámetros.

**Integración**  
Asegúrate de exponer métodos públicos neutros (`get_auxiliary_table_value`, `update_auxiliary_table_value`, etc.) y documentar que cualquier nueva estrategia debe usarlos; evita tocar su firma más adelante.

---

### 3. `pendulum_environment.py`

**Propósito**  
Encapsular la interacción “sistema físico ↔ controlador ↔ reward” sin lógica de aprendizaje.

**Escalabilidad**

- Reemplaza referencias explícitas al _setpoint PID_ por lecturas a través de `controller.get_target()` (nueva interfaz mínima).
    
- Introduce una primera capa abstracta `BaseEnvironment` cuyo contrato cubra `step`, `reset`, `check_termination`, `update_reward_calculator_stats`. Los entornos futuros (por ejemplo, “QuadcopterEnv”) sólo implementan este contrato y se registran en `EnvironmentFactory`.
    
- Evita `if reset_gains` acoplado a PID: mueve ese control a `controller.reset_policy()` para que cada controlador decida.
    

**Dependencias**  
`Controller` gana los métodos `reset_policy` y `get_target`. `EnvironmentFactory` simplemente delega la creación.

---

### 4. `pendulum_virtual_simulator.py`

**Intención**  
Ejecutar _rollouts contrafactuales_ totalmente aislados del entorno real.

**Refactorización mínima**

- Convertirlo en clase genérica `VirtualSimulator` con un método `run_interval_simulation` basado en un **`DynamicSystem` clonable** y un **`Controller` clonable**.
    
- Exige que cualquier controlador que pretenda usarse en virtualizaciones implemente `clone()` o sea _deep-copy-safe_; documentarlo en la interfaz.
    

**Ajustes externos**  
`RewardStrategy` con `needs_virtual_simulation = True` fuerza al contenedor a resolver un simulador; el helper `_create_virtual_simulator` del DI ya lo respeta y sólo cambiaría el nombre de la clase.

---

### 5. `reward_factory.py`

**Objetivo**  
Ser el punto único de ensamblaje de _RewardFunction_ y _StabilityCalculator_.

**Agravios actuales y mejoras**

- Añade un registro interno (`self._registry`) para mapear `'type' → class` y permite extensiones vía un decorador `@reward_factory.register('my_new_calc')` en los módulos de terceros, evitando editar la factoría.
    
- Traslada la lógica de validación de parámetros específicos a cada clase; la factoría sólo pasa el bloque dict.
    
- Devuelve siempre instancias, nunca `None`; la decisión “no usar estabilidad” equivale a una clase `NullStabilityCalculator`.
    

**Dependencias**  
El helper del contenedor que la invoca se simplifica (no necesita `if not calculator_type`).

---

### 6. `instantaneous_reward_calculator.py` simulation_manager

**Propósito**  
Implementar una _estrategia de cálculo de recompensa_ parametrizada (gaussiana, by-stability, etc.).

**Líneas a reforzar**

- Desacoplar internamente cada método de cálculo en sub-clases o _policy objects_ (`GaussianRewardPolicy`, `StabilityBasedPolicy`) y seleccionarlas en el constructor. Así nuevas fórmulas se añaden declarativamente.
    
- Estandarizar la salida como `(reward, w_stab)` y mover el cálculo real de `w_stab` fuera si ya tenemos un `BaseStabilityCalculator`.
    

**Impacto**  
`RewardFactory` deberá pasar simplemente `"method": "gaussian"` y `"gaussian_params"`, sin tocar este archivo cuando agreguemos “quadratic”, “shaped”, etc.

---

### 7. `ira_stability_calculator.py` pid_qlearning_agentpendulum_environment

### 8. `simple_exponential_stability_calculator.py` pendulum_virtual_simula…reward_factory

_(juntos por similitud)_

**Propósito conjunto**  
Traducir un estado continuo a un escalar `w_stab ∈ [0, 1]` o a una recompensa basada en estabilidad.

**Mejoras para estandarizar**

- Definir en la interfaz `BaseStabilityCalculator` los hooks:
    
    - `calculate_instantaneous_stability(state) → float`
        
    - `calculate_stability_based_reward(state) → float`
        
    - `update_reference_stats(metrics, episode)` _(opcional)_
        
    - `get_current_adaptive_stats() → Dict`
        
    
    Ya existen, pero asegúrate de que cada calculadora declare explícitamente si es _adaptive_ (_flag_) y el episodio mínimo para actualizarse.
    
- Extraer la validación de parámetros (mu, sigma, scales, etc.) a funciones helper compartidas para evitar duplicar lógica.
    

**Dependencias**  
`InstantaneousRewardCalculator` y cualquier otra clase que consuma `BaseStabilityCalculator` sólo dependerán de esos cuatro métodos.

---

### 9. `global_reward_strategy.py` instantaneous_reward_ca…ira_stability_calculator

### 10. `shadow_baseline_reward_strategy.py` simple_exponential_stab…global_reward_strategy

### 11. `echo_baseline_reward_strategy.py` shadow_baseline_reward_…echo_baseline_reward_st…

**Rol común**  
Traducir la “información cruda” del intervalo (recompensa real, `w_stab`, tablas auxiliares, simulaciones virtuales…) a la _recompensa que el agente debe usar para actualizar Q_.

**Dirección de evolución**

|Requisito|Ajuste propuesto|
|---|---|
|Declarar necesidades de infraestructura|Cada estrategia expone `needs_virtual_simulation` y `required_auxiliary_tables`. Esto elimina toda lógica condicional en `SimulationManager` y en el agente.|
|Composición sobre herencia|Extraer el cálculo “delta B”, “R_diff”, etc. a _helper policies_ que se inyectan mediante el constructor, evitando que cada clase crezca con casos especiales.|
|Config-driven|Todos los hiper-parámetros (`beta`, `baseline_init_value`, etc.) deben leerse sin validación cruzada: la clase asume que ya fueron verificados por una función de esquema (e.g. pydantic) al cargar `config.yaml`.|
|Acceso a tablas auxiliares|Usar únicamente los getters/setters genéricos del agente para que la misma estrategia funcione con agentes futuros que no sean PID-centric.|

**Impacto**  
`PIDQLearningAgent` dejará de importar explícitamente `ShadowBaselineRewardStrategy` para inicializar `baseline_init_value`. En lugar de eso, el valor inicial se pasa desde `config.agent.params.initial_aux_values.baseline`. La estrategia sigue controlando la actualización, pero la creación la maneja el agente de forma agnóstica.

---

## Consideraciones generales (aplican a todos)

- **Configuración declarativa total**: toda variación (nuevas redes DQN, controladores LQR, sistemas “Quadcopter”, estrategias “Advantage plus Replay”) debe poder referenciarse sólo añadiendo un bloque en `config.yaml` y, como mucho, registrar la clase en su factoría mediante decorador.
    
- **Factorías como “service locators acotados”**: cada factoría mantiene un registro extensible y no contiene _if-elif_ enormes; las clases se autodetectan.
    
- **Interfaces estables y mínimas**: cuando sea necesario un nuevo método, introdúcelo en la interfaz y ofrécele una implementación por defecto (p.ej. en una `Mixin`) para no romper clases existentes.
    
- **Validación fail-fast en constructores**: todo parámetro crítico se verifica tan pronto como la clase se crea. El `SimulationManager` asume que las instancias son válidas y no re-valida.
    
- **Logging y errores**: módulos core (manager, factorías) generan _logs de alto nivel_; los componentes especializados sólo registran entrada y salida de sus métodos clave.
    
- **Composición interna**: antes de optar por herencia, preferir que cada clase tenga atributos _strategy_ o _policy_ intercambiables declarados en config (p.ej. un `PIDController` podría recibir una política de anti-windup).
    
- **Pruebas unitarias**: cada componente orquestador (factorías, manager, agentes) debe contar con tests que verifiquen que, dados mocks de dependencias declaradas, se comportan igual independientemente de la implementación concreta que se inyecte.
    

---

### Algunas Consideraciones
  
1. **Refactorizar factorías** para que usen el patrón _registry_ y eliminen los bloques `if type == '...'`.
2. **Añadir atributos en interfaces** (`needs_virtual_simulation`, `required_auxiliary_tables`, etc.) y ajustar las implementaciones existentes.
3. **Migrar lógica condicional de `SimulationManager`** a la lectura de esos atributos.
4. **Revisar `state_config`** para separar claramente _variables del sistema_ y _parámetros del controlador_; el agente sólo necesita conocer sus límites y bins.

El núcleo del sistema permanecerá inalterado mientras incorporas nuevas combinaciones de **algoritmo de RL + controlador + planta + estrategia de recompensa + calculadora de estabilidad**, simplemente escribiendo la clase y declarando su uso en `config.yaml`.