---
created: 20250408 17:04
update: 20250510-06:32
summary: 
status: 
link: []
tags:
  - content
---
# Propuesta de mejora

Integrar una recompensa diferenciada que permita medir el impacto de las acciones de cada agente sobre la recompensa global. La idea sería agregar al config.yaml dentro de "reward" despues de "type" un modo "global" para el actual, un modo ``echo_baseline`` y un modo ``shadow_baseline`` para las nuevas funcionalidades. 

## Approach A: Echo Baseline Agent

**Idea Principal:**  
Sustituir la recompensa global corriente por _recompensas diferenciales_ específicas para cada agente. Cada diferencial se obtiene comparando la recompensa global real con la recompensa **contrafactual** que resultaría si la ganancia del agente evaluado permaneciera en su valor previo, mientras que las acciones de los demás agentes se mantienen tal cual. De esta forma, cada ajuste recibe un crédito proporcional a su impacto efectivo sobre el sistema.

**Flujo de Simulación e Implementación:**

En cada episodio de Q-Learning, el entorno se reinicia con ganancias $K'_P,\ K'_I,\ K'_D$ y el péndulo evoluciona durante ventanas de decisión $v$ de cierta duración; al cerrar cada ventana se han acumulado las recompensas instantáneas para obtener la recompensa global de esa ventana:
$$R^{v}_{real} = \sum^{\vartriangle t - 1}_{t=0} r_{t}$$
Esto quiere decir; **y sin alterar el estado de la simulación principal**, se lanzan secuencialmente tres simulaciones virtuales contrafactuales: en cada una se revierte la acción de un único agente y se mantienen las de los otros dos para calcular las recompensas contrafactuales respectivas. Estas recompensas se acumulan para cada ganancia en paralelo , ya que componen el cálculo diferencial de cada recompensa que se utilizará para la actualización del `Q-value` respectivo.

1. **Evaluación y Acumulación de la Recompensa Global:**  
    Durante el intervalo de decisión se evalúan las ganancias actuales:
    $$K_P,\ K_I,\ K_D$$
    en el estado:
    $$[x,\ \dot{x}, \ \alpha,\ \dot{\alpha}]$$
    y se acumula la recompensa global:
    $$R_{\text{real}}$$
2. **Determinación de las Nuevas Acciones:**  
    Antes de proceder al proceso de aprendizaje (en la función `env.agent.learn`), se determinan las nuevas acciones para cada ganancia, obteniéndose los valores:
    $$K'_P = K_P + \Delta_P$$$$K'_I = K_I + \Delta_I$$$$K'_D = K_D + \Delta_D$$
3. **Cálculo de las Recompensas Contrafactuales:**  
    Se simula un paso de la dinámica utilizando una configuración virtual del sistema donde solo se revierte la acción de un agente individual, manteniendo las acciones de los demás tal como fueron aplicadas realmente:
    - Para el agente Proporcional:
        $$PID(K_P,\ K'_I,\ K'_D) \Rightarrow R_P$$
    - Para el agente Integral:
        $$PID(K'_P,\ K_I,\ K'_D) \Rightarrow R_I$$
    - Para el agente Derivativo:
        $$PID(K'_P,\ K'_I,\ K_D) \Rightarrow R_D$$
    Cada simulación genera una recompensa virtual contrafactual que representa el comportamiento del sistema si dicho agente no hubiese actuado, mientras los demás sí lo hacen.
4. **Cálculo de los Diferenciales de Recompensa:**  
    Se obtienen los diferenciales para cada agente, comparando la recompensa global real con la recompensa contrafactual:
    - Para Proporcional:
        $$R_{P_{dif}} = R_{\text{real}} - R_P$$
    - Para Integral:
        $$R_{I_{dif}} = R_{\text{real}} - R_I$$
    - Para Derivativo:
        $$R_{D_{dif}} = R_{\text{real}} - R_D$$
5. **Actualización de las Q‑Values:**  
    Con estos diferenciales, se actualizan las Q‑tables de cada agente usando la fórmula:
    $$Q(s, a_i) \leftarrow Q(s, a_i) + \alpha \left( R_{i_{dif}} + \gamma \cdot \max_{a'} Q(s', a') - Q(s, a_i) \right)$$
    donde: $i \in {P,\ I,\ D}$ representa a cada uno de los agentes, $s$ es el estado previo, $a_i$ es la acción del agente $i$, y $s'$ el estado posterior.

## Approach B: Shadow Baseline Agent

**Idea Principal:**  
Cada agente mantiene una estimación local del valor esperado de recompensa si hubiera ejecutado la acción `'mantener'` en lugar de modificar su ganancia. Esta estimación se representa en una tabla $B(s)$ que se actualiza mediante un promedio móvil exponencial (EMA), ponderado por una métrica de estabilidad del sistema. 

**Flujo de Simulación e Implementación**

1. **Ejecución del Intervalo de Decisión:**  
    Durante el intervalo de decisión, cada agente ejecuta su acción real  $a \in \{\text{bajar},\ \text{mantener},\ \text{subir}\}$ y se acumula la recompensa global obtenida por el sistema dinámico en dicho periodo, denotada como:
    $$ R_{real}$$
2. **Evaluación de la Estabilidad del Sistema:**  
    A partir del historial del sistema durante el intervalo de decisión, se calcula un puntaje de estabilidad $w_{stab}$ basado en el comportamiento de las variables dinámicas:
    $$ w_{stab} = \exp( - \lambda_1 \alpha^2 - \lambda_2 \dot{\alpha}^2 - \lambda_3 x^2 - \lambda_4 \dot{x}^2)$$
    Este score penaliza trayectorias alejadas del equilibrio y toma valores en $(0, 1]$.
3. **Actualización del Baseline Local $B(s)$:**  
    Si el agente ejecutó la acción `'mantener'` en el estado $s$, se actualiza su tabla de baseline $B(s)$ mediante un promedio móvil exponencial ponderado por el puntaje de estabilidad:
    $$ B(s) \leftarrow B(s) + \beta \cdot w_{stab} \cdot ( R_{real} - B(s))$$
    Donde $\beta \in (0, 1)$ es la tasa de aprendizaje del baseline.
	1. **Obtención de la Recompensa Diferencial:**  
	    Si el agente ejecutó una acción distinta de `'mantener'`, se define la recompensa diferencial como:
	    $$ R^{diff} = R_{real} - B(s)$$
4. **Actualización de la Q-Table del Agente:**  
    Finalmente, se actualiza la tabla Q del agente utilizando la recompensa diferencial calculada, según la regla estándar del Q-Learning:
    $$ Q(s,\ a) \leftarrow Q(s,\ a) + \alpha \cdot ( R^{diff} + \gamma \cdot \max_{a'} Q(s',\ a') - Q(s,\ a))$$



## Consideraciones Adicionales: 

### Métrica Generalizada de Estabilización

**Idea Principal:**  
Se define una métrica de estabilidad relativa del sistema dinámico que no depende directamente de parámetros físicos específicos, sino de desviaciones normalizadas respecto al comportamiento histórico esperado. Esta métrica puede ser utilizada como base para calcular recompensas diferenciales tanto en el enfoque _Echo Baseline Agent_ como en el _Shadow Baseline Agent_, facilitando la adaptabilidad del algoritmo a distintos entornos dinámicos y múltiples objetivos.

**Flujo de Cálculo de la Métrica:**

1. **Selección de Variables de Estado:**  
    Se consideran las variables relevantes del sistema dinámico en el intervalo de decisión:
    $$s \in \ {x,\ \dot{x},\ \alpha,\ \dot{\alpha}}$$
2. **Normalización Dinámica del Estado:**  
    Cada variable es transformada a una forma adimensional utilizando su media y desviación estándar estimadas durante condiciones de referencia estabilizadas (o mediante ventana móvil adaptativa):
    $$z_s(t) = \frac{s(t) - \mu_s}{\sigma_s + \epsilon}$$
    Donde $\mu_s$ y $\sigma_s$ corresponden a la media y desviación estándar de la variable s y $\epsilon$ es un término de regularización para evitar divisiones por cero.
3. **Cálculo de la Energía Penalizada de Desviación:**  
    Se define la _Inestabilidad Relativa Acumulada_ (IRA) como una medida de cuánto se desvía el comportamiento del sistema respecto al estado de equilibrio esperado:  
    $$IRA = \int_{t_0}^{t_1} \sum_{s} w_s \cdot z_s(t)^2 \ dt$$
    Donde $w_s$ es un peso opcional asociado a cada variable de estado $s$, permitiendo adaptar la métrica a distintos objetivos de control.
4. **Función de Recompensa Basada en Estabilidad Relativa:**  
    La recompensa general utilizada para evaluar el desempeño global del sistema se define como una función exponencial negativa de la métrica IRA:  
    $$R_{real} = \exp(- \lambda \cdot IRA)$$
    Donde $\lambda > 0$ es un hiperparámetro de sensibilidad que controla el grado de penalización ante trayectorias inestables.