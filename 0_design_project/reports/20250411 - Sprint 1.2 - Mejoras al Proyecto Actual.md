---
created: 20250411 04:04
update: 20250414-02:39
summary: 
status: 
link: 
tags: 
---
# 🧭 Plan Integral de Mejora (Versión Extendida)

---
## 🧱 1. Reestructuración Modular del Flujo Principal

### 🎯 Objetivo:

Dividir y encapsular la lógica de ejecución contenida en `main.py` en módulos especializados, desacoplando responsabilidades para facilitar mantenibilidad y reutilización.
### 📌 Afecta a:
- `main.py`
- Nuevos módulos: `simulation_manager.py`, `world_initializer.py`, `config_loader.py`, `result_handler.py`, `visualization_runner.py`
### 🔍 Modificaciones:
- **Unificar la creación de componentes** (agente, entorno, reward, etc.) en `world_initializer` .
- **Centralizar la carga y validación de configuración** en `config_loader`, con lectura YAML y validación contra esquema.
- **Orquestar simulaciones por episodios y pasos** en `simulation_manager`, incluyendo manejo de errores y logging.
- Mover guardado de resultados a `result_handler.py`.
    - Separar lógica de exportación de episodios, estado del agente, resumen y visualizaciones.
- Modularizar la visualización en `visualization_runner.py`.
    - Interpretar visualizaciones desde archivos ya generados.
---
## 🧠 2. Comentarios Estructurados y Ecuaciones Explícitas

### 🎯 Objetivo:

Facilitar la lectura y el mantenimiento del código mediante comentarios consistentes por sección y expresiones matemáticas codificadas de forma explícita.
### 📌 Afecta a:
- `main.py`
- `pid_qlearning_agent.py`
- `reward_function.py`
- `stability_calculator.py` (IRA y Exponential)
- `pendulum_environment.py`
- y otros.

### 🔍 Modificaciones:

- Incluir títulos de bloque numerados (e.g. `# [2.3] - Cálculo de estabilidad`).
    Asegurar que fórmulas como:
    `Q(s,a) ← Q(s,a) + α(R + γ max_a' Q(s',a') - Q(s,a))`
    `w_stab = exp(-Σ λ (x/scale)^2)`
    `IRA = ∫ Σ w_s z_s(t)^2 dt`
- **Asegurar que las ecuaciones estén programadas directamente** (por ejemplo, fórmulas como `exp(-Σ w * z^2)` deben estar implementadas en una línea y sin descomposición innecesaria).
- **Evitar el uso de variables ocultas o transformaciones implícitas**, manteniendo claridad y trazabilidad del cálculo.

---
## 🧩 3. Refactorización de Estrategias de Recompensa

### 🎯 Objetivo:

Separar las lógicas `global`, `shadow-baseline`, `echo-baseline` en componentes intercambiables, autónomos y testables.
### 📌 Afecta a:
- `pid_qlearning_agent.py`
- `main.py`
- `reward_factory.py` (para conectar `RewardStrategy`)
- Configuración YAML (`reward.reward_mode`)

### 🔍 Modificaciones:

- Crear interfaz `RewardStrategy` con método `compute_reward(...) → dict[str, float]`., con una interfaz común y tres implementaciones:
    - `GlobalRewardStrategy`: recompensa global acumulada.
    - `ShadowBaselineRewardStrategy`: recompensa diferencial usando tabla `B(s)`.
    - `EchoBaselineRewardStrategy`: recompensa diferencial mediante simulaciones contrafactuales.
- Refactorizar `PIDQLearningAgent.learn()` para delegar el cálculo de recompensa a la estrategia correspondiente.
- Modificar `reward_factory.py` para inicializar la estrategia y pasarla al agente.
- Actualizar YAML `reward.reward_mode` para seleccionar la estrategia.

---
## 📐 4. Consolidación del Cálculo de Estabilidad

### 🎯 Objetivo:

Unificar los métodos de evaluación de estabilidad del sistema en una jerarquía clara, extensible y fácilmente intercambiable.

### 📌 Afecta a:

- `simple_exponential_stability_calculator.py`
- `ira_stability_calculator.py`
- `reward_factory.py`
- `reward_function.py`

### 🔍 Modificaciones:

 - **Crear una superclase abstracta para calculadoras de estabilidad** `BaseStabilityCalculator`, con los métodos `calculate_instantaneous_stability()` y `calculate_stability_based_reward()`.
- **Estandarizar la selección de variables**, pesos y escalas mediante parámetros extensibles a cualquier sistema.
- **Permitir normalización adaptativa por episodio**
- Refactorizar:
    - `IRAStabilityCalculator` (incluye `adaptive_stats`)
    - `SimpleExponentialStabilityCalculator`
- Asegurar uso de parámetros `lambda_weights`, `scales`, `mu`, `sigma`, `epsilon` de forma consistente.
- Mover validación de `var_indices`, `weights`, `scales` al constructor base.
- Registrar `mu` y `sigma` en los logs por cada variable al final de cada episodio.
- Adaptar `GaussianReward` para usar `stability_calculator.calculate_stability_based_reward()` cuando esté habilitado.

---
## 📦 5. Normalización del Manejo de Estados

### 🎯 Objetivo:

Estandarizar la construcción y el acceso a las variables de estado del sistema y del agente, evitando inconsistencias semánticas y errores por índices incorrectos.
### 📌 Afecta a:
- `pid_qlearning_agent.py`
- `pendulum_environment.py`
- `rl_agent.py`
- `controller.py`
- `main.py`
### 🔍 Modificaciones:
- **Centralizar el mapeo entre nombres de variables y sus posiciones en los vectores de estado**.
- **Validar que todas las variables habilitadas en `state_config` estén correctamente nombradas** en todos los métodos, como por ejemplo `build_agent_state()` y en los logs.
- **Extender `reset()` y `step()` del entorno para propagar correctamente los valores iniciales de variables que podrían estar ausentes o incompletas.**
	- Revisar `PendulumEnvironment.reset()` para asegurar que:
	- El estado inicial y el tiempo `t = 0` estén correctamente propagados a controlador y agente.
	- `reset_episode()` se llame si `reset_gains=False`.

---
## 📊 6. Recolección Extendida de Métricas

### 🎯 Objetivo:

Capturar todos los valores relevantes de sistema, controlador, agente, estabilidad en cada timestep, resultados de cada paso de entrenamiento y el cierre del episodio.

### 📌 Afecta a:
- `simple_metrics_collector.py` → nuevo: `ExtendedMetricsCollector`
- `main.py` → integración completa del nuevo esquema de recolección
- `pid_qlearning_agent.py` → adición de métodos auxiliares para logging

### 🔍 Variables por categoría:

### 🆕 Clase Nueva: `ExtendedMetricsCollector(SimpleMetricsCollector)`
#### Métodos nuevos:
- `log_q_values(agent, agent_state_dict)`
- `log_baselines(agent, agent_state_dict)`
- `log_virtual_rewards(echo_rewards: dict)`
- `log_td_errors(td_errors: dict)`
- `log_adaptive_stats(stats_dict: dict)`
#### Modificaciones a `SimpleMetricsCollector`
- Añadir campo `episode_id` inicializado en `reset()`.
- Soporte para insertar `None` (→ se guardará como `"null"` al exportar).
- Posiblemente almacenar métricas intermedias como `dict[time_id][metric_name]` si se quiere trazabilidad temporal exacta.
#### Recolección inicial por episodio:
Registrar valores :
- `episode` (número)
#### Recolección por paso de tiempo (`log`):
Registrar lo siguiente **en cada timestep** (considerar que para el primer paso de tiempo se deben incluir los valores de inicialización de cada parámetro, en caso de que no exista o no corresponda dejar como "null"):
- Variables del sistema dinámico:
    - `time`, `pendulum_angle`, `pendulum_velocity`, `cart_position`, `cart_velocity`, `force`
- Variables del controlador:
    - `kp`, `ki`, `kd`,
    - `error` (ángulo respecto al setpoint)
    - `integral_error`
    - `derivative_error`
- Variables del agente RL:
    - `epsilon`, `learning_rate` (registrar valor repetido en cada paso intermedio)
    - `action_kp`, `action_ki`, `action_kd` (registrar en cada intervalo de decisión, "null" en pasos intermedios)
    - `q_value_kp`, `q_value_ki`, `q_value_kd` (nuevo)
    - `q_visit_count_kp`, `q_visit_count_ki`, `q_visit_count_kd` (nuevo)
    - `virtual_force_kp`, `virtual_force_ki`, `virtual_force_kd` (si está en modo `echo-baseline`)
    - `virtual_reward_kp`, `virtual_reward_ki`, `virtual_reward_kd` (si está en modo `echo-baseline`)
    - `baseline_value_kp`, `baseline_value_ki`, `baseline_value_kd` (si está en modo shadow-baseline)
    - `baseline_visit_count_kp`, `baseline_visit_count_ki`, `baseline_visit_count_kd` (si está en modo shadow-baseline)
    - `td_error_kp`, `td_error_ki`, `td_error_kd` (si está en modo shadow-baseline)
- Variables de estabilidad y recompensa:
    - `reward`, `cumulative_reward`, `stability_score`, `Adaptive Statistics` para cada parámetro su `mu` y `sigma`
- Variables del entrenamiento:
    - `id_agent_decisión` (id de cada proceso de  de decisión)
    - `learn_agent_duration_ms` (duración de cada paso de decisión)
    - `gain_step` (valor real aplicado en el episodio si `variable_step=True` de lo contrario registrar valor repetido en cada paso intermedio)
#### Recolección final por episodio:
Registrar adicionalmente:
- `episode_duration`
- `total_reward`
- `performance`
- `termination_reason`
- `avg_stability_score`
- `final_kp`, `final_ki`, `final_kd`
- `total_agent_decisions` (número de pasos de decisión)
Registrar en archivo separado:
- Json según frecuencia indicada por el usuario de:
	- `Q-Table`, `visit_count_Q-Table`, `B(s)` y `visit_count_B(s)` para shadow-baseline
- Excel con el último episodio:
	- `Q-Table`, `visit_count_Q-Table`, `B(s)` y `visit_count_B(s)` para shadow-baseline


---

## 📈 7. Archivo de Resumen Episódico (summary.xlsx)

### 🎯 Objetivo:

Centralizar los principales indicadores de cada episodio para análisis rápido, visualización y trazabilidad de evolución de desempeño.
### 📌 Afecta a:
- `utils/data_processing.py`
- `main.py`
### 🔍 Variables por episodio:
- `episode`, `termination_reason`, `episode_duration`
- `total_reward`, `performance`, `avg_stability_score`
- `final_kp`, `final_ki`, `final_kd`
- `epsilon`, `learning_rate`
- `learn_agent_duration_ms`, `total_agent_decisions`
### 📊 Agregados estadísticos:

- Media, desviación estándar, mínimo y máximo para:
    - `pendulum_angle`, `pendulum_velocity`
    - `cart_position`, `cart_velocity`
    - `kp`, `ki`, `kd`
    - `force`, `reward`, `error`, `stability_score`
    - `mu`, `sigma`
    - `q_value_kp`, `q_value_ki`, `q_value_kd`
    - `visit_count_kp`, `visit_count_ki`, `visit_count_kd
    - `virtual_force_kp`, `virtual_force_ki`, `virtual_force_kd` (si está en modo `echo-baseline`)
    - `virtual_reward_kp`, `virtual_reward_ki`, `virtual_reward_kd` (si está en modo `echo-baseline`)
    - `baseline_value_kp`, `baseline_value_ki`, `baseline_value_kd` (si está en modo shadow-baseline)
    - `baseline_visit_count_kp`, `baseline_visit_count_ki`, `baseline_visit_count_kd` (si está en modo shadow-baseline)
    - `td_error_kp`, `td_error_ki`, `td_error_kd` (si está en modo shadow-baseline)

---

## 🌡️ 8. Módulo de Preprocesamiento y Exportación de Heatmaps

### 🎯 Objetivo:

Generar automáticamente histogramas 2D (heatmaps) de cualquier par de variables seleccionadas en la configuración, exportando los resultados en un archivo Excel accesible para visualización posterior.
### 📌 Afecta a:
- Nuevo módulo `heatmap_generator.py`
- `summary.xlsx` y archivo adicional `data_heatmaps.xlsx`
- Configuración externa `sub_config_visualization.yaml`
### 🔍 Procesamiento:
- Identificar pares (x, y) de variables solicitadas.
- Filtrar episodios según criterios (`termination_reason`, etc.).
- Calcular histogramas 2D (`np.histogram2d`) con parámetros definidos (`bins`, `x/y_min/max`, `log_scale`, etc.).
- Exportar a Excel:
    - Una hoja por combinación.
    - Metadatos arriba, matriz indexada debajo.
    - Compatible con interfaz externa de visualización desacoplada.
