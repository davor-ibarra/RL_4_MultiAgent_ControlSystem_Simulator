---
created: 20250411 04:04
update: 20250414-02:39
summary: 
status: 
link: 
tags: 
---
# ğŸ§­ Plan Integral de Mejora (VersiÃ³n Extendida)

---
## ğŸ§± 1. ReestructuraciÃ³n Modular del Flujo Principal

### ğŸ¯ Objetivo:

Dividir y encapsular la lÃ³gica de ejecuciÃ³n contenida en `main.py` en mÃ³dulos especializados, desacoplando responsabilidades para facilitar mantenibilidad y reutilizaciÃ³n.
### ğŸ“Œ Afecta a:
- `main.py`
- Nuevos mÃ³dulos: `simulation_manager.py`, `world_initializer.py`, `config_loader.py`, `result_handler.py`, `visualization_runner.py`
### ğŸ” Modificaciones:
- **Unificar la creaciÃ³n de componentes** (agente, entorno, reward, etc.) en `world_initializer` .
- **Centralizar la carga y validaciÃ³n de configuraciÃ³n** en `config_loader`, con lectura YAML y validaciÃ³n contra esquema.
- **Orquestar simulaciones por episodios y pasos** en `simulation_manager`, incluyendo manejo de errores y logging.
- Mover guardado de resultados a `result_handler.py`.
    - Separar lÃ³gica de exportaciÃ³n de episodios, estado del agente, resumen y visualizaciones.
- Modularizar la visualizaciÃ³n en `visualization_runner.py`.
    - Interpretar visualizaciones desde archivos ya generados.
---
## ğŸ§  2. Comentarios Estructurados y Ecuaciones ExplÃ­citas

### ğŸ¯ Objetivo:

Facilitar la lectura y el mantenimiento del cÃ³digo mediante comentarios consistentes por secciÃ³n y expresiones matemÃ¡ticas codificadas de forma explÃ­cita.
### ğŸ“Œ Afecta a:
- `main.py`
- `pid_qlearning_agent.py`
- `reward_function.py`
- `stability_calculator.py` (IRA y Exponential)
- `pendulum_environment.py`
- y otros.

### ğŸ” Modificaciones:

- Incluir tÃ­tulos de bloque numerados (e.g. `# [2.3] - CÃ¡lculo de estabilidad`).
    Asegurar que fÃ³rmulas como:
    `Q(s,a) â† Q(s,a) + Î±(R + Î³ max_a' Q(s',a') - Q(s,a))`
    `w_stab = exp(-Î£ Î» (x/scale)^2)`
    `IRA = âˆ« Î£ w_s z_s(t)^2 dt`
- **Asegurar que las ecuaciones estÃ©n programadas directamente** (por ejemplo, fÃ³rmulas como `exp(-Î£ w * z^2)` deben estar implementadas en una lÃ­nea y sin descomposiciÃ³n innecesaria).
- **Evitar el uso de variables ocultas o transformaciones implÃ­citas**, manteniendo claridad y trazabilidad del cÃ¡lculo.

---
## ğŸ§© 3. RefactorizaciÃ³n de Estrategias de Recompensa

### ğŸ¯ Objetivo:

Separar las lÃ³gicas `global`, `shadow-baseline`, `echo-baseline` en componentes intercambiables, autÃ³nomos y testables.
### ğŸ“Œ Afecta a:
- `pid_qlearning_agent.py`
- `main.py`
- `reward_factory.py` (para conectar `RewardStrategy`)
- ConfiguraciÃ³n YAML (`reward.reward_mode`)

### ğŸ” Modificaciones:

- Crear interfaz `RewardStrategy` con mÃ©todo `compute_reward(...) â†’ dict[str, float]`., con una interfaz comÃºn y tres implementaciones:
    - `GlobalRewardStrategy`: recompensa global acumulada.
    - `ShadowBaselineRewardStrategy`: recompensa diferencial usando tabla `B(s)`.
    - `EchoBaselineRewardStrategy`: recompensa diferencial mediante simulaciones contrafactuales.
- Refactorizar `PIDQLearningAgent.learn()` para delegar el cÃ¡lculo de recompensa a la estrategia correspondiente.
- Modificar `reward_factory.py` para inicializar la estrategia y pasarla al agente.
- Actualizar YAML `reward.reward_mode` para seleccionar la estrategia.

---
## ğŸ“ 4. ConsolidaciÃ³n del CÃ¡lculo de Estabilidad

### ğŸ¯ Objetivo:

Unificar los mÃ©todos de evaluaciÃ³n de estabilidad del sistema en una jerarquÃ­a clara, extensible y fÃ¡cilmente intercambiable.

### ğŸ“Œ Afecta a:

- `simple_exponential_stability_calculator.py`
- `ira_stability_calculator.py`
- `reward_factory.py`
- `reward_function.py`

### ğŸ” Modificaciones:

 - **Crear una superclase abstracta para calculadoras de estabilidad** `BaseStabilityCalculator`, con los mÃ©todos `calculate_instantaneous_stability()` y `calculate_stability_based_reward()`.
- **Estandarizar la selecciÃ³n de variables**, pesos y escalas mediante parÃ¡metros extensibles a cualquier sistema.
- **Permitir normalizaciÃ³n adaptativa por episodio**
- Refactorizar:
    - `IRAStabilityCalculator` (incluye `adaptive_stats`)
    - `SimpleExponentialStabilityCalculator`
- Asegurar uso de parÃ¡metros `lambda_weights`, `scales`, `mu`, `sigma`, `epsilon` de forma consistente.
- Mover validaciÃ³n de `var_indices`, `weights`, `scales` al constructor base.
- Registrar `mu` y `sigma` en los logs por cada variable al final de cada episodio.
- Adaptar `GaussianReward` para usar `stability_calculator.calculate_stability_based_reward()` cuando estÃ© habilitado.

---
## ğŸ“¦ 5. NormalizaciÃ³n del Manejo de Estados

### ğŸ¯ Objetivo:

Estandarizar la construcciÃ³n y el acceso a las variables de estado del sistema y del agente, evitando inconsistencias semÃ¡nticas y errores por Ã­ndices incorrectos.
### ğŸ“Œ Afecta a:
- `pid_qlearning_agent.py`
- `pendulum_environment.py`
- `rl_agent.py`
- `controller.py`
- `main.py`
### ğŸ” Modificaciones:
- **Centralizar el mapeo entre nombres de variables y sus posiciones en los vectores de estado**.
- **Validar que todas las variables habilitadas en `state_config` estÃ©n correctamente nombradas** en todos los mÃ©todos, como por ejemplo `build_agent_state()` y en los logs.
- **Extender `reset()` y `step()` del entorno para propagar correctamente los valores iniciales de variables que podrÃ­an estar ausentes o incompletas.**
	- Revisar `PendulumEnvironment.reset()` para asegurar que:
	- El estado inicial y el tiempo `t = 0` estÃ©n correctamente propagados a controlador y agente.
	- `reset_episode()` se llame si `reset_gains=False`.

---
## ğŸ“Š 6. RecolecciÃ³n Extendida de MÃ©tricas

### ğŸ¯ Objetivo:

Capturar todos los valores relevantes de sistema, controlador, agente, estabilidad en cada timestep, resultados de cada paso de entrenamiento y el cierre del episodio.

### ğŸ“Œ Afecta a:
- `simple_metrics_collector.py` â†’ nuevo: `ExtendedMetricsCollector`
- `main.py` â†’ integraciÃ³n completa del nuevo esquema de recolecciÃ³n
- `pid_qlearning_agent.py` â†’ adiciÃ³n de mÃ©todos auxiliares para logging

### ğŸ” Variables por categorÃ­a:

### ğŸ†• Clase Nueva: `ExtendedMetricsCollector(SimpleMetricsCollector)`
#### MÃ©todos nuevos:
- `log_q_values(agent, agent_state_dict)`
- `log_baselines(agent, agent_state_dict)`
- `log_virtual_rewards(echo_rewards: dict)`
- `log_td_errors(td_errors: dict)`
- `log_adaptive_stats(stats_dict: dict)`
#### Modificaciones a `SimpleMetricsCollector`
- AÃ±adir campo `episode_id` inicializado en `reset()`.
- Soporte para insertar `None` (â†’ se guardarÃ¡ como `"null"` al exportar).
- Posiblemente almacenar mÃ©tricas intermedias como `dict[time_id][metric_name]` si se quiere trazabilidad temporal exacta.
#### RecolecciÃ³n inicial por episodio:
Registrar valores :
- `episode` (nÃºmero)
#### RecolecciÃ³n por paso de tiempo (`log`):
Registrar lo siguiente **en cada timestep** (considerar que para el primer paso de tiempo se deben incluir los valores de inicializaciÃ³n de cada parÃ¡metro, en caso de que no exista o no corresponda dejar como "null"):
- Variables del sistema dinÃ¡mico:
    - `time`, `pendulum_angle`, `pendulum_velocity`, `cart_position`, `cart_velocity`, `force`
- Variables del controlador:
    - `kp`, `ki`, `kd`,
    - `error` (Ã¡ngulo respecto al setpoint)
    - `integral_error`
    - `derivative_error`
- Variables del agente RL:
    - `epsilon`, `learning_rate` (registrar valor repetido en cada paso intermedio)
    - `action_kp`, `action_ki`, `action_kd` (registrar en cada intervalo de decisiÃ³n, "null" en pasos intermedios)
    - `q_value_kp`, `q_value_ki`, `q_value_kd` (nuevo)
    - `q_visit_count_kp`, `q_visit_count_ki`, `q_visit_count_kd` (nuevo)
    - `virtual_force_kp`, `virtual_force_ki`, `virtual_force_kd` (si estÃ¡ en modo `echo-baseline`)
    - `virtual_reward_kp`, `virtual_reward_ki`, `virtual_reward_kd` (si estÃ¡ en modo `echo-baseline`)
    - `baseline_value_kp`, `baseline_value_ki`, `baseline_value_kd` (si estÃ¡ en modo shadow-baseline)
    - `baseline_visit_count_kp`, `baseline_visit_count_ki`, `baseline_visit_count_kd` (si estÃ¡ en modo shadow-baseline)
    - `td_error_kp`, `td_error_ki`, `td_error_kd` (si estÃ¡ en modo shadow-baseline)
- Variables de estabilidad y recompensa:
    - `reward`, `cumulative_reward`, `stability_score`, `Adaptive Statistics` para cada parÃ¡metro su `mu` y `sigma`
- Variables del entrenamiento:
    - `id_agent_decisiÃ³n` (id de cada proceso de  de decisiÃ³n)
    - `learn_agent_duration_ms` (duraciÃ³n de cada paso de decisiÃ³n)
    - `gain_step` (valor real aplicado en el episodio si `variable_step=True` de lo contrario registrar valor repetido en cada paso intermedio)
#### RecolecciÃ³n final por episodio:
Registrar adicionalmente:
- `episode_duration`
- `total_reward`
- `performance`
- `termination_reason`
- `avg_stability_score`
- `final_kp`, `final_ki`, `final_kd`
- `total_agent_decisions` (nÃºmero de pasos de decisiÃ³n)
Registrar en archivo separado:
- Json segÃºn frecuencia indicada por el usuario de:
	- `Q-Table`, `visit_count_Q-Table`, `B(s)` y `visit_count_B(s)` para shadow-baseline
- Excel con el Ãºltimo episodio:
	- `Q-Table`, `visit_count_Q-Table`, `B(s)` y `visit_count_B(s)` para shadow-baseline


---

## ğŸ“ˆ 7. Archivo de Resumen EpisÃ³dico (summary.xlsx)

### ğŸ¯ Objetivo:

Centralizar los principales indicadores de cada episodio para anÃ¡lisis rÃ¡pido, visualizaciÃ³n y trazabilidad de evoluciÃ³n de desempeÃ±o.
### ğŸ“Œ Afecta a:
- `utils/data_processing.py`
- `main.py`
### ğŸ” Variables por episodio:
- `episode`, `termination_reason`, `episode_duration`
- `total_reward`, `performance`, `avg_stability_score`
- `final_kp`, `final_ki`, `final_kd`
- `epsilon`, `learning_rate`
- `learn_agent_duration_ms`, `total_agent_decisions`
### ğŸ“Š Agregados estadÃ­sticos:

- Media, desviaciÃ³n estÃ¡ndar, mÃ­nimo y mÃ¡ximo para:
    - `pendulum_angle`, `pendulum_velocity`
    - `cart_position`, `cart_velocity`
    - `kp`, `ki`, `kd`
    - `force`, `reward`, `error`, `stability_score`
    - `mu`, `sigma`
    - `q_value_kp`, `q_value_ki`, `q_value_kd`
    - `visit_count_kp`, `visit_count_ki`, `visit_count_kd
    - `virtual_force_kp`, `virtual_force_ki`, `virtual_force_kd` (si estÃ¡ en modo `echo-baseline`)
    - `virtual_reward_kp`, `virtual_reward_ki`, `virtual_reward_kd` (si estÃ¡ en modo `echo-baseline`)
    - `baseline_value_kp`, `baseline_value_ki`, `baseline_value_kd` (si estÃ¡ en modo shadow-baseline)
    - `baseline_visit_count_kp`, `baseline_visit_count_ki`, `baseline_visit_count_kd` (si estÃ¡ en modo shadow-baseline)
    - `td_error_kp`, `td_error_ki`, `td_error_kd` (si estÃ¡ en modo shadow-baseline)

---

## ğŸŒ¡ï¸ 8. MÃ³dulo de Preprocesamiento y ExportaciÃ³n de Heatmaps

### ğŸ¯ Objetivo:

Generar automÃ¡ticamente histogramas 2D (heatmaps) de cualquier par de variables seleccionadas en la configuraciÃ³n, exportando los resultados en un archivo Excel accesible para visualizaciÃ³n posterior.
### ğŸ“Œ Afecta a:
- Nuevo mÃ³dulo `heatmap_generator.py`
- `summary.xlsx` y archivo adicional `data_heatmaps.xlsx`
- ConfiguraciÃ³n externa `sub_config_visualization.yaml`
### ğŸ” Procesamiento:
- Identificar pares (x, y) de variables solicitadas.
- Filtrar episodios segÃºn criterios (`termination_reason`, etc.).
- Calcular histogramas 2D (`np.histogram2d`) con parÃ¡metros definidos (`bins`, `x/y_min/max`, `log_scale`, etc.).
- Exportar a Excel:
    - Una hoja por combinaciÃ³n.
    - Metadatos arriba, matriz indexada debajo.
    - Compatible con interfaz externa de visualizaciÃ³n desacoplada.
