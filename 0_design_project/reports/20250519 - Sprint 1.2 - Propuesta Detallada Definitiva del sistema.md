
# Escalamiento Definitivo

---

## Instrucciones Técnicas (alto nivel)

Declaración de intenciones y conjunto de instrucciones direccionales de alto nivel para la refactorización final. Mediante los _Principios Fundamentales de Diseño_ definidos anteriormente (SRP reforzado, composición, configuración declarativa, fail-fast, etc.) y en asegurar la habilitación para la creación plug-and-play de **nuevos algoritmos, controladores, sistemas, calculadoras y estrategias** sin tocar el núcleo (`main`, `di_container`, factorías, `simulation_manager`).

---

### 1. `simulation_manager.py`

**Objetivo operativo**  
Seguir orquestando episodios y decisiones pero completamente agnóstica a los componentes, es decir, sin ramificaciones por tipo concreto, permitiendo la orquestación de simulaciones de cualquier sistema dinámico, controlador y agentes declarados. Su responsabilidad final es: _“ejecutar la simulación completa con los componentes que el contenedor le entregue”_.

**Puntos de mejora para escalar**

- La inicialización de todo el sistema debe ser realizado una sola vez; instancias, estructuras, llamadas y variables de comunicación deben ser creadas al inicio y no cada vez que se toma una decisión o que se resetea el episodio, ya que la idea es que una vez corrida la simulación solo se itere sobre las métodos directos y explícitos del sistema establecido, asegurando así la velocidad de la simulación de cada episodio.
- `agent_instance_decision.build_agent_state` debe ser utilizado solo para inicializar la estructura, no para construirla en cada `_handle_decision_boundary`
- Eliminar cualquier `isinstance` o import directo de estrategias concretas. Sustituye la comprobación de `EchoBaselineRewardStrategy` por la lectura de un atributo estándar (`needs_virtual_simulation`) ya presente en la interfaz `RewardStrategy`.
- Extraer la lógica de _decision interval loop_ y _episodio_ a métodos auxiliares privados para que distintos `Environment` u otros dominios la puedan sobreescribir vía composición si fuera necesario.
- Simplificar y generalizar la firma de métodos `_run_standard_interval_steps` y `_run_echo_baseline_interval_steps` aceptando un único objeto “runner” (inyectado) que implemente el contrato _IntervalRunner_. Así la aparición de futuras variantes (p.ej. _offline rollouts_) se limita a crear otra clase y declararla en config.
- La evaluación del check termination es responsabilidad del PendulumEnvironment, por lo que solo se debería recibir el estado de terminación y el motivo, no procesar su propio check termination y menos recibir información del agente.

**Consideraciones de integración**  
Mantén la misma firma pública de `run()` para no romper `main.py`. Anota exhaustivamente en los logs cuándo se resuelve cada componente y delega cualquier validación de parámetros a las clases instanciadas.

---

### 2. `pid_qlearning_agent.py`

**Propósito**  
Ser un _orquestador jerárquico_ de tablas Q y estructuras auxiliares requerido por la estrategia de recompensa, sin conocer detalles de la misma.

**Escalabilidad**

- La clase gestiona dinámicamente la creación de tablas Q, auxiliares, la selección de acciones mediante método en función a la política establecida, y la gestión del aprendizaje mediante un orquestador directo y legible.
- `build_agent_state()` debe ser utilizado solo para inicializar la estructura, no para construirla en cada `_handle_decision_boundary`
- Capacidad del agente para gestionar su paciencia (lógica de early_termination); y en función de esta, penalizar las recompensas deben ser debidamente modularizadas como métodos privados.

**Integración**  
Asegúrate de exponer métodos públicos neutros (`get_auxiliary_table_value`, `update_auxiliary_table_value`, etc.) y documentar que cualquier nueva estrategia debe usarlos; evita tocar su firma más adelante.

---

### 3. `pendulum_environment.py`

**Propósito**  
Encapsular la interacción “sistema físico ↔ controlador ↔ reward” sin lógica de aprendizaje.

**Escalabilidad**

- Mantener capa abstracta `Environment` cuyo contrato cubra `step`, `reset`, `check_termination`, `update_reward_calculator_stats`, etc. Los entornos futuros (por ejemplo, “MountainCarEnvironment”) sólo implementan este contrato y se registran en `EnvironmentFactory`.
- Es responsabilidad del Environment gestionar todos los Check termination, por lo que debería recibir el early termination del agente, y manejar los criterios de detención.
- Es responsabilidad del Environment gestionar la conexión con `BaseStabilityCalculator` para calcular la estabilidad del sistema en cada step, así como de entregar su estado al `PendulumEnvironment`.

---

### 4. `pendulum_virtual_simulator.py`

**Intención**  
Ejecutar _rollouts contrafactuales_ totalmente aislados del entorno real.

**Sistema Actual**

- Clase genérica `VirtualSimulator` con un método `run_interval_simulation` basado en un **`DynamicSystem` clonable** y un **`Controller` clonable**.
- Exige que cualquier controlador que pretenda usarse en virtualizaciones implemente `clone()` o sea _deep-copy-safe_; documentarlo en la interfaz.

**Consideraciones**
El sistema actual asegura integridad de las simulaciones, pero se encuentra efectivamente optimizada? es necesario hacer una deepcopy en cada `run_interval_simulation()` o sería mejor solo al inicializar, y luego se le entrega la información necesaria para que realice las simulaciones virtuales?

---

### 5. `instantaneous_reward_calculator.py` 

**Propósito**  
Implementar una _estrategia de cálculo de recompensa_ parametrizada (gaussiana, by-stability, etc.).

**Escalabilidad**

- Mantener desacoplado internamente cada método de cálculo en sub-clases o _policy objects_ y seleccionarlas en el constructor. Así nuevas fórmulas se añaden declarativamente.
- Estandarizar la salida como solo `calculated_reward_value` y no como`(reward, w_stab)` ya que la responsabilidad de evaluar la estabilidad del sistema es de `PendulumEnvironment` mediante el nuevo componente `StabilityCalculator`, por lo tanto, con esta clase es que se debe proponer una conexión directa para utilizar la estabilidad del sistema si es que se ha determinado una recompensa basada en la estabilidad.
- Es responsabilidad de instantaneous_reward_calculator el calcular el valor de la recompensa instantánea (calculated_reward_value) en cada paso, basándose en el método configurado (e.g., weighted_exponential, stability_measure_based).

---

### 6. `ira_stability_calculator.py` 

### 7. `simple_exponential_stability_calculator.py` 

_(juntos por similitud)_

**Propósito conjunto**  
Traducir un estado continuo a un escalar `w_stab ∈ [0, 1]` o a una recompensa basada en estabilidad.

**Escalabilidad**

- Limpiar validaciones y mención a variables específicas del sistema del péndulo, la idea es cuente con la lógica de cálculo y que maneje dinámicamente las variables declaradas en el config (las que podrían ser para cualquier tipo de sistema o cualquier variable que venga de `PendulumEnvironment` ).

---

### 8. `global_reward_strategy.py` 

### 9. `shadow_baseline_reward_strategy.py` 

### 10. `echo_baseline_reward_strategy.py` 

**Rol común**  
Traducir la “información cruda” del intervalo (recompensa real, `w_stab`, tablas auxiliares, simulaciones virtuales…) a la _recompensa que el agente debe usar para actualizar Q_.

**Escalabilidad**

| Requisito                               | Ajuste propuesto                                                                                                                                                                                   |
| --------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Declarar necesidades de infraestructura | Cada estrategia expone `needs_virtual_simulation` y `required_auxiliary_tables`. Esto elimina toda lógica condicional en `SimulationManager` y en el agente.                                       |
| Composición sobre herencia              | Extraer el cálculo “delta B”, “R_diff”, etc. a _helper policies_ que se inyectan mediante el constructor, evitando que cada clase crezca con casos especiales.                                     |
| Config-driven                           | Todos los hiper-parámetros (`beta`, `baseline_init_value`, etc.) deben leerse sin validación cruzada: la clase asume que ya fueron verificados por una función de esquema al cargar `config.yaml`. |
| Acceso a tablas auxiliares              | Usar únicamente los getters/setters genéricos del agente para que la misma estrategia funcione con agentes futuros que no sean PID-centric o para cualquier parámetro entregado por el agente.     |

**Asegurar que**  
La estrategia sigue controlando la actualización, pero la creación la maneja el agente de forma agnóstica.

---

## Consideraciones generales (aplican a todos)

- **Configuración declarativa total**: toda variación (como por ejemplo nuevos agentes DQN, controladores LQR, sistemas “MountainCar”, estrategias “Advantage plus Replay”, etc) debe poder referenciarse sólo añadiendo un bloque en `config.yaml`
- **Factorías como “service locators acotados”**: cada factoría mantiene un registro extensible y no contiene _if-elif_ enormes; las clases se autodetectan.
- **Interfaces estables y mínimas**: cuando sea necesario un nuevo método, introdúcelo en la interfaz y ofrécele una implementación por defecto (p.ej. en una `Mixin`) para no romper clases existentes.
- **Validación fail-fast en constructores**: todo parámetro crítico se verifica tan pronto como la clase se crea. El `SimulationManager` asume que las instancias y parámetros son válidos y no son re-validados en cada llamada. Esto además permite dejar cada método y lógica específica de forma directa y explicita, y así mejorar la legibilidad del código.
- **Logging**: módulos core (manager, factorías) generan _logs de alto nivel_; los componentes especializados sólo registran logs de salida de sus parámetros y métodos claves.
- **Metric collector**: Se gestiona únicamente en el Environment y el SimulationManager , asegurando de colectar de manera eficiente en cada etapa los parámetros pertinentes, como al inicializar el episodio, finalizar un step, intervalo de decisión y episodio.
- **Gestión de Errores**: módulos core (manager, factorías) generan _logs de alto nivel_; los componentes especializados sólo contarán en sus métodos claves un solo `try-except` que permita rastrear el motivo del fallo. Por lo que, se acabaron la gestión de valores específicos y la asignación a valores corruptos que no corresponden a la simulación.
- **Composición interna**: antes de optar por herencia, preferir que cada clase tenga atributos _strategy_ o _policy_ intercambiables declarados en config (p.ej. un `PIDController` podría recibir una política de anti-windup).








---





# Plan de Refactorización Detallado

## 1. simulation_manager.py

**Declaración de intenciones**  
El objetivo de `simulation_manager.py` es convertirse en el **macro-orquestador** de todos los experimentos de simulación, **completamente agnóstico** a la naturaleza de los componentes (sistema dinámico, controlador, agente, estrategia de recompensa, simulador virtual). Debe:
- **Instanciar una sola vez**, al inicio de `run()`, todas las dependencias y estructuras (entorno, agente, controlador, reward strategy, virtual simulator, metric collector, handlers de datos y logs).
- Iterar únicamente sobre métodos **directos y explícitos** en el bucle de episodios y en el bucle de intervalos de decisión, garantizando la máxima eficiencia por episodio.
- Delegar toda la lógica específica (check de terminación, paso de dinámica, cálculo de recompensa o estabilidad, salvado de resultados) a los **sub-orquestadores** que implemente cada componente, sin validaciones ni excepciones intermedias.
- Ofrecer una interfaz única e inmutable (`run()`) para no romper integraciones existentes en `main.py` o en pipelines de despliegue.

---

### 1.1 Responsabilidad única

1. Orquestar el macro-flujo de la simulación: ejecución de episodios, bucles de decisión y pasos de simulación.
2. Ser completamente agnóstico a las implementaciones concretas de los componentes (Environment, Agent, Controller, RewardStrategy, VirtualSimulator, MetricsCollector), interactuando con ellos exclusivamente a través de sus interfaces.
3. Resolver las dependencias necesarias para la simulación a través del contenedor DI al inicio de su ejecución.
4. Gestionar la inicialización y finalización de cada episodio, incluyendo el reseteo de componentes y la recolección/agregación de métricas a alto nivel.
5. Manejar la lógica de guardado de datos por lotes y el guardado del estado del agente, basado en la configuración.
6. Invocar la lógica de simulación de intervalo (real o virtual) según lo requiera la RewardStrategy configurada.   

---

### 1.2 Lógica principal clara, lineal y legible

1. **Resolución de dependencias** (`_resolve_dependencies()`): invocar factorías DI una sola vez y obtener instancias de
    - `Environment`
    - `RLAgent`
    - `Controller`
    - `RewardStrategy`
    - `VirtualSimulator` (si `reward_strategy.needs_virtual_simulation`)
    - `MetricsCollector`, `ResultHandler`, configuración maestra, directivas de datos y carpeta de salida.
2. **Configuración de parámetros**: extraer `max_episodes`, `decision_interval`, `dt`, `episodes_per_chunk`, `agent_state_save_freq`, `log_flush_frequency` sin validaciones intermedias (fail-fast en constructores).
3. **Bucle de episodios** (`for episode_idx in range(max_episodes)`)
    - Inicializar episodio: `_initialize_episode(...)` → estado inicial + paquete de datos con acciones por defecto.
    - Bucle de simulación temporal: mientras no termine y `sim_time < episode_duration`
        - Determinar `interval_duration = min(decision_interval, rem_time)`
        - Llamar a uno de dos sub-orquestadores inyectados:
            - `_run_standard_interval_steps(...)`
            - `_run_echo_baseline_interval_steps(...)` (si `needs_virtual_simulation`)
        - Recibir: `(interval_reward, avg_stability, final_state, done, term_reason[, echo_dict])`
    - Finalizar episodio: `_finalize_episode(...)` → summary, métricas, posible guardado de estado de agente y batch de datos.
4. **Limpieza y flush de logs** en el bloque `finally`, imprimiendo “Main Simulation Loop Finished”. simulation_manager
    

---

### 1.3 Métodos y mejoras clave

> **Pauta general:** cada método público o privado debe ser **directo**, con firma **estática** y sin cambios de responsabilidades en tiempo de ejecución.

#### 1.3.1 Métodos Principales

1. `run(self) -> Tuple[List[Dict], List[Dict]]:`
    - **Función:** Orquesta la ejecución completa de todos los episodios de la simulación.
    - **Mejoras requeridas:**
        1. Refactorizar para que la inicialización de componentes y parámetros de simulación ocurra una sola vez al inicio del método, no en cada episodio o decisión. Los componentes resueltos se pasarán como argumentos a los métodos auxiliares.
		2. Eliminar cualquier lógica condicional basada en tipos de estrategias concretas (e.g., EchoBaselineRewardStrategy). En su lugar, leer atributos de la interfaz RewardStrategy (como needs_virtual_simulation) para determinar el flujo.
		3. Centralizar la lógica de resolución de dependencias en un método privado _resolve_dependencies().
		4. La lógica de manejo de lotes de datos y guardado de estado del agente se basará en parámetros de data_handling_directives_runtime.
		5. Asegurar que la firma pública se mantenga para no romper main.py.
		6. Capturar errores críticas con un único `try/except` y, en caso de fallo, delegar el salvado parcial de datos.
2. 

#### 1.3.2 Métodos Secundarios

1. `_initialize_episode(self, episode_index, environment_obj, agent_obj, controller_obj, metrics_collector_obj, sim_config) -> Tuple[np.ndarray, Dict[str, Any]]`
    - **Función:** Prepara el inicio de un nuevo episodio. Resetea componentes, inicializa el MetricsCollector para el episodio, obtiene el estado inicial y las primeras acciones.
    - **Mejoras:** asegurarse de no re-resolver dependencias ni variables de comunicación aquí; solo usar instancias ya creadas en `_resolve_dependencies()`.
2. `_run_standard_interval_steps(current_time, duration, state, env, ctrl, agent, metrics, config, actions)`
    - **Función:** Ejecuta los pasos de simulación del entorno (sistema, controlador, función de recompensa) durante un intervalo de decisión (agent_decision_period_sec), acumulando recompensas y métricas de estabilidad.
    - **Mejoras:**
        - **Generalizar firma:** en lugar de múltiples parámetros, recibir un único método `_run_interval()` → `(reward, stability, state, done, reason)` que invoque directamente a environment_obj.step() en cada paso de simulación.
        - Acumular la recompensa y las puntuaciones de estabilidad
        - Eliminar lógica de check de terminación: solo recibir `done, reason` de `env.step()`.
3. `_run_echo_baseline_interval_steps(current_time, duration, state, env, ctrl, agent, metrics, config, actions)`
    - **Función:** Extiende _run_standard_interval_steps para incluir simulaciones virtuales contrafactuales, específicas para la estrategia Echo Baseline.
    - **Mejoras:**
        - Primero llama a *_run_standard_interval_steps* para obtener los resultados del intervalo real. Luego, si es necesario, orquesta múltiples llamadas a virt_sim_obj.run_interval_simulation() para generar los escenarios contrafactuales. Calcula las recompensas diferenciales (R_diff) y las devuelve.
4. `_handle_decision_boundary(self, current_time, current_episode_state_vec, previous_interval_data, executed_interval_results, sim_configuration) -> Tuple[Dict[str, int], Optional[Dict]]:`
    - **Función:** Orquesta el proceso de aprendizaje del agente y la selección de la siguiente acción al final de un intervalo de decisión.
    - **Mejoras:**
		1. Asegurar que agent_instance_decision.build_agent_state se use solo para obtener la representación del estado S' para el aprendizaje, no para reconstruir estructuras internas del agente.
		2. Delegar a agent_instance_decision.learn() para el proceso de aprendizaje, pasándole reward_info (la información de recompensa, que puede ser escalar o diccionario), el score de estabilidad (avg_stability_measure) y el estado final del intervalo (next_agent_s_prime_dict).
		3. La información de recompensa (reward_info_for_learning) para agent.learn() se ensamblará aquí, pudiendo ser una tupla (R_real, avg_stability_score) o un diccionario de recompensas diferenciales si la estrategia lo requiere y _run_echo_baseline_interval_steps lo proveyó.
5. `_finalize_episode(self, episode_id_num, episode_metrics_data, ep_term_reason, ep_start_timestamp, ctrl_obj_finalize, env_obj_finalize, agent_obj_finalize, output_path_finalize, sim_cfg_finalize, data_handling_directives_finalize, summary_data_agg_list, current_batch_detailed_data, agent_state_snapshot_freq):`
    - **Función:** Recopila todos los datos de un episodio completo, los resume y los guarda, y activa las actualizaciones de estadísticas internas de los componentes (ej. RewardFunction).
    - **Mejoras:**
        - Utilizar las data_handling_directives_finalize (que incluyen json_history_enabled, summary_enabled, etc.) para controlar la generación de resúmenes y el guardado de datos detallados. 
        - Delegar  result_handler.save_episode_batch y result_handler.save_metadata para la persistencia. 
        - Asegurarse de que env_obj_finalize.update_reward_calculator_stats() se llama para permitir que la función de recompensa (y su calculador de estabilidad) actualicen sus estadísticas internas si son adaptativas.
#### 1.3.3 Métodos Auxiliares

1. `_resolve_dependencies(self) -> Tuple[...]:`
    - **Función:** Resuelve y retorna todas las dependencias clave del DI Container necesarias para la ejecución completa de la simulación.
    - **Mejoras:** Único punto para obtener instancias, asegurando de que resuelvan todas las instancias singleton (Environment, RLAgent, Controller, RewardStrategy, VirtualSimulator, dict (config), str (output_dir), PROCESSED_DATA_DIRECTIVES_TOKEN) y el MetricsCollector transitorio. La validación se limita a asegurar que se obtienen instancias válidas (no None) para los componentes críticos, relanzando un ValueError si alguna es None.
2. `_log_initial_metrics(actions_dict, controller)`
    - **Función:** Registra el estado inicial del entorno, controlador y agente al principio de un episodio.
    - **Mejoras:**
        - Usar directamente el metrics_collector_obj para loguear las métricas de estado, controlador y agente. No se requiere lógica compleja aquí, solo una llamada directa a los métodos de logging del MetricsCollector.

### 1.4 Consideraciones de integración

- **Firma pública inmutable**: `run() → Tuple[List[Dict], List[Dict]]` no debe cambiar, para preservar `main.py`. simulation_manager
- **Logging**: mantener logs de alto nivel en `SimulationManager`; eliminar logs de detalle de pasos de simulación.
- **Configuración declarativa**: sólo leer claves de `config['environment']['simulation']` y `config['data_handling']`, sin validaciones cruzadas..
- **Fail-Fast**: dejar la validación estricta a los constructores de cada componente; `SimulationManager` asume parámetros correctos y falla sólo en su nivel macro.


---


## 2. pendulum_environment.py

**Declaración de intenciones**  
El archivo `pendulum_environment.py` debe configurarse como la **encapsulación pura** de la dinámica del péndulo invertido (en este caso) y su interacción con el controlador y la función de recompensa, **completamente desacoplada** de cualquier lógica de aprendizaje u orquestación externa. Su API pública deberá ofrecer únicamente dos operaciones limpias (`reset()` y `step()`), manteniendo el código **directo**, explícito y **sin validaciones internas** de tipo o rango (fail-fast en constructores y métodos externos). Todos los detalles de física, terminación y cálculo de recompensa/stability se delegan a métodos privados y a componentes inyectados, asegurando máxima legibilidad y extensibilidad.

---

### 2.1 Responsabilidad única

1. Encapsular la lógica de interacción específica del "mundo físico" o sistema simulado.
2. Aplicar las acciones del Controller al DynamicSystem.
3. **Ejecutar la dinámica física del entorno** y exponer el estado del péndulo en cada paso, junto con la recompensa instantánea y la medida de estabilidad:
4. Gestionar el estado interno del sistema dinámico (DynamicSystem) y del controlador (Controller) durante un paso de simulación.
5. Calcular la recompensa y la métrica de estabilidad instantánea delegando a la RewardFunction y StabilityCalculator inyectadas.
6. Verificar todas las condiciones de terminación del episodio (límites de estado, consecución de objetivos, early termination, pero NO tiempo máximo de episodio, que lo maneja SimulationManager).
7. Resetear el DynamicSystem, el Controller y el RLAgent al inicio de un episodio.
8. Actualizar estadísticas internas de la RewardFunction y de BaseStabilityCalculator si es que fuese adaptativo al final del episodio.
9. **No debe** gestionar aprendizaje, evolución de parámetros de agente, ni orquestar secuencias de simulación.

---

### 2.2 Lógica principal clara, lineal y legible

- `__init__`: Recibe instancias de DynamicSystem, Controller, RLAgent, RewardFunction y la config completa. Almacena estas dependencias y extrae parámetros de configuración necesarios para su operación (e.g., dt_sec, controller_reset_level_on_episode_end).
- **`reset()`**
    - Restablece el entorno a sus condiciones iniciales, delegando el reset a los componentes (System, Controller, Agent).
    - Inicializa el reloj de simulación a cero.
- **`step(action)`**
    - Calcula la fuerza de control, la aplica al sistema dinámico, y obtiene el nuevo estado. Luego, calcula la recompensa y la estabilidad para este paso.
    - Comprueba terminación (`check_termination(state)`)
    - Finalmente, actualiza el estado interno del entorno y el tiempo.`
    - **Fail-fast**: asume que todos los parámetros son válidos.
- `check_termination()`:
        1. Leer los límites de estado y criterios de umbral desde self.config.
        2. Comparar self.current_episode_state contra estos límites/criterios.
        3. Recibir la solicitud de terminación temprana del RLAgent (agent.should_episode_terminate_early()).
        4. Retornar (limit_exceeded, goal_reached, agent_requested_early_termination). El agente comunica su deseo de terminar, el Environment lo incluye en su reporte de terminación, pero SimulationManager toma la decisión final de si terminar el bucle del episodio basado en esto Y el tiempo máximo.

---

### 2.3 Métodos y mejoras clave

> **Pauta general:** cada método debe ser un bloque de lógica **conciso**, sin ramas ocultas, y delegar sub-tareas a métodos privados con firmas estáticas.

#### 2.3.1 Métodos Principales

1. `reset()`
    - **Función:** devolver el estado inicial del entorno.
    - **Mejoras:**
        - Debe resetear el DynamicSystem, el Controller (usando controller.reset_policy() con el nivel configurado), y el RLAgent (usando agent.reset_agent()).
        - Asegurar que asigne atributos (`self.state`, `self.time = 0`).
2. `step(self) -> Tuple[Any, Tuple[float, float], Any]`
    - **Función:** avanzar la simulación un `dt` dado la acción aplicada.
    - **Mejoras:**
        - Recibir acciones procesadas por el agente
        - Descomponer en tres llamadas internas:
            1. `_apply_control(action_dict)`
            2. `_integrate_dynamics()`
            3. `_compute_reward()`
            4. `_compute_stability()`
        - Devolver un **DTO** (`StepResult`) que agrupe todos los campos en lugar de múltiples variables sueltas.
        - No debe contener lógica de aprendizaje del agente.
#### 2.3.2 Métodos Secundarios
1. `_apply_control(action_dict) → control_value`
    - **Función:** traducir el dict genérico de acción a la señal de control que entiende el controlador.
    - **Mejoras:**
        - Cambiar a método estático o a un objeto `ActionTranslator` inyectable para facilitar otros tipos de controladores.
2. `_integrate_dynamics(control_value)`
    - **Función:** actualizar `self.state` en base al `system.integrate(self.dt, control_value)`.
    - **Mejoras:**
        - Inyectar `self.system` ya preparado; eliminar cualquier branching sobre tipos de sistema.
3. `_compute_reward() → reward`
    - **Función:** llamar a la instancia `self.reward_calculator.calculate(self.state, …)`.
    - **Mejoras:**
        - Asegurar que sólo retorne valores primitivos, sin estructuras anidadas.
4. `_compute_stability() → stability`
    - **Función:** llamar a la instancia `self.stability_calculator.calculate(self.state)`.
    - **Mejoras:**
        - Asegurar que sólo retorne valores primitivos, sin estructuras anidadas.
5. `check_termination(state) → (done: bool, reason: str)`
    - **Función:** Verifica si el episodio debe terminar por condiciones de estado, meta. (El config_param_check_term no se usará, se usará self.config inyectada en __init__) o recibe early termination. 
    - **Mejoras:**
        - Recibir la lista de `termination_conditions` inyectada desde config; iterar linealmente y devolver al primer match.
        - Aplicar criterios configurables de fin de episodio (ángulo, posición del carro, etc). 
        - No incluir lógica numérica: las condiciones deben ser objetos con método `is_met(state, time)`.
        - Debe obtener la solicitud de terminación temprana del RLAgent (agent.should_episode_terminate_early()) e incluirla en su tupla de retorno. SimulationManager usará esta información junto con el tiempo máximo de episodio.

#### 2.3.3 Métodos Auxiliares

1. `dt(self) -> float (propiedad)`:
	- **Función:** Expone el *dt_val* del entorno.
	- **Mejoras Clave:** Mantener como está, es una forma limpia de que SimulationManager acceda al dt.
2. `update_reward_calculator_stats(self, episode_metrics_log_dict: Dict, episode_idx_completed: int)]`:
	- **Función:** Delega la actualización de estadísticas a la RewardFunction y StabilityCalculator.
	- **Mejoras Clave:** Se mantiene la delegación simple.

---

### 2.4 Consideraciones de integración

- **Interfaz pública inmutable**: conservar métodos `reset()` y `step()`, y la firma de retorno, para no invalidar `SimulationManager`.
- Cualquier nuevo Environment (e.g., MountainCarEnvironment) solo necesita implementar la interfaz Environment y ser registrado en EnvironmentFactory.
- **Inyección de dependencias**: recibir en el constructor instancias de:
    
    ```python
    def __init__(self,system: DynamicSystem, controller: Controller, reward_calculator: InstantaneousRewardCalculator, stability_calculator: BaseStabilityCalculator, termination_conditions: List[TerminationCondition], dt: float):`
    ```
- **Naming consistency**: usar `state`, `time`, `dt` y `action_dict` de forma idéntica a lo largo de todo el flujo.
- **Fail-Fast**: ninguna validación adicional; confiar en el constructor para parámetros corruptos.
- **Logging**: emitir un único log al inicio y final de cada `reset()` y de cada `step()` a nivel **alto** (“PendulumEnvironment: step completed, time=…”) y permitir al `SimulationManager` decidir el nivel de detalle global.
- Es responsable de propagar errores críticos a SimulationManager.


---


### 3. pid_qlearning_agent.py

**Declaración de intenciones**  
El PIDQLearningAgent es un **orquestador jerárquico de aprendizaje por refuerzo** especializado en la optimización de parámetros PID. Su responsabilidad principal es gestionar el estado de las tablas Q (y sus contadores de visita) para cada agente PID (kp, ki, kd), así como las tablas auxiliares requeridas por la RewardStrategy (ej. baseline). Implementa el algoritmo de Q-learning, la selección de acciones (epsilon-greedy), la gestión de sus hiperparámetros de aprendizaje (tasas de decaimiento) y la lógica de terminación temprana del episodio (Early Termination). Es agnóstico a la implementación de la RewardStrategy y del Controller, interactuando con ellos solo a través de sus interfaces.

---

### 3.1 Responsabilidad única

1. Implementar la interfaz RLAgent.
2. Gestionar la lógica de aprendizaje por reforzamiento (e.g., Q-learning).
3. Mantener y actualizar las tablas de valor (e.g., Q-tables) y cualquier tabla auxiliar requerida por la RewardStrategy inyectada (e.g., tablas de baseline).
4. Seleccionar acciones basadas en su política actual (e.g., epsilon-greedy).
5. Gestionar su estado interno (e.g., epsilon, learning rate, contadores de paciencia para early termination).
6. Construir su representación de estado (agent_state_dict) a partir del estado crudo del sistema y los parámetros configurados.
7. Comunicar a PendulumEnvironment (vía should_episode_terminate_early()) si desea terminar el episodio prematuramente.
8. **No debe** contener lógica de simulación, integración física, ni cálculos de recompensa/estabilidad: eso es responsabilidad de los componentes inyectados.

---

### 3.2 Lógica principal clara, lineal y legible

- `__init__`:
    - Recibe la RewardStrategy inyectada (con interfaz `compute_reward_for_learning`) y agent_params de la config (con Parámetros de discretización de estado (bins, rangos) y otros).
    - Inicializa hiperparámetros de Q-Learning (epsilon, alpha, discount_factor, etc.)
    - Inicializa estructuras para Q-tables y tablas de visita (visit_counts_np).
    - Basado en reward_strategy.required_auxiliary_tables, inicializa las tablas auxiliares (auxiliary_tables_data) y sus contadores de visita (auxiliary_visit_counts) con valores iniciales de aux_table_init_values (de config).
    - Inicializa la lógica de early termination (paciencia, contadores) si está habilitada en early_stopping_criteria (de config).
- `select_action(state):
    - Para cada agent_defining_var:
	    - Discretiza el `state` continuo → índices.
	    - Aplica ε-greedy sobre la Q-table correspondiente a cada variable (kp, ki, kd).
	    - Almacenar la acción seleccionada.
	- Devuelve un dict o tupla de índices accionables.
- `learn(prev_state, action, reward, next_state, done)`
    - Para cada agent_defining_var:
	    - Discretiza `prev_state` y `next_state`.
	    - Aplicar lógica de early termination: actualizar contadores de no mejora, ajustar paciencia si es adaptativa, y si se activa la penalización, modificar R_learn.
	    - Obtener la R_learn de la reward_strategy.compute_reward_for_learning(). Este método de la estrategia puede, a su vez, usar get_auxiliary_table_value y update_auxiliary_table_value del agente.
	    - Calcula el target Q:
	$$ Q_{\text{new}} = Q + \alpha \bigl(r + \gamma \max_{a'}Q(s',a') - Q(s,a)\bigr) $$
	    - Actualiza la Q-table y, opcionalmente, tablas auxiliares.
- **Reset** (`reset_agent()`)
    - Aplica decaimientos a epsilon y learning rate (si están habilitados).
    - Limpia o ajusta tablas auxiliares si procede.
    - Resetear contadores de early termination (paciencia, no mejora) y flags de solicitud de terminación según corresponda.
    - Actualizar métricas de early termination (e.g., last_episode_avg_improvement_metrics).

---

#### 3.3 Métodos y mejoras clave

> **Pauta general:** cada método debe ceñirse a su función, con firma fija y sin validaciones internas complejas (fail-fast en constructores).

#### 3.3.1 Métodos Principales

1. `__init__()`
	- **Función:** Inicializa el agente con todos sus parámetros, y construye las estructuras de datos (Q-tables, contadores de visita, tablas auxiliares).
        - **Mejoras:**
            - **Fail-Fast en Constructor:** Validar estrictamente los tipos y rangos de los parámetros críticos (ej. num_actions > 0, discount_factor en [0,1], gain_delta es numérico o dict si per_gain_delta es True). Si los parámetros son inválidos, lanzar ValueError o TypeError.
            - **Dependencia de RewardStrategy:** Confirmar que reward_strategy es una instancia de RewardStrategy. Usar reward_strategy.required_auxiliary_tables para inicializar dinámicamente las tablas auxiliares (ej. self.auxiliary_tables_data['baseline']).
            - **Inicialización Modular:** Delegar la validación y preparación de state_config a _validate_and_prepare_state_config y la inicialización de todas las tablas a _initialize_tables.
            - **Early Termination (ET) Configuración:** Inicializar todos los contadores, banderas y valores de paciencia/beta relacionados con la lógica de ET, usando la sección early_stopping_criteria de la configuración.
2. `select_action(state)`
    - **Función:** devolver la acción discreta para cada ganancia PID usando ε-greedy.
    - **Mejoras:**
        - Extraer la lógica de ε-greedy a un **`ExplorationPolicy`** inyectable, para permitir otras políticas.
        - Iterar sobre self.agent_defining_vars.
        - Simplificar la discretización: usar un utilitario genérico `_discretize(state, bins_config)` en lugar de métodos ad-hoc.
        - Si los Q-values son NaN (estado no visitado), seleccionar una acción aleatoria
        - La obtención de Q-values (`self.q_tables_np[agent_var_select][state_indices_for_q]`) debe ser directa.
        - Asumir que current_agent_state_values es un dict válido, sin try-except complejos internos.
3. `learn(prev_state, action, reward, next_state, done)`
    - **Función:** Realiza el paso de aprendizaje del algoritmo de Q-learning para actualizar las Q-tablas y otras estructuras auxiliares, y maneja la lógica de early termination..
    - **Mejoras:**
        - Invocar self.reward_strategy.compute_reward_for_learning() para obtener R_learn. Esta estrategia podrá usar los métodos genéricos get_auxiliary_table_value y update_auxiliary_table_value del agente si necesita interactuar con tablas como 'baseline'.
        - La lógica de early termination (*_update_early_termination_metrics* and *_penalize_reward*) se modularizará y llamará aquí, potencialmente modificando R_learn.
        - Identificar si el cálculo del **TD-error** se realiza mediante ecuación regular de Bellman, de lo contrario, si `done = True` actualizar solo con valor actual de Q y no con `next_Q` ya que este no existiría.
        - La actualización de Q-tables será directa y explícita. Cada actualización de Q-table para una ganancia específica debe estar encapsulada en un único try-except para loguear errores específicos (IndexError, KeyError, Exception) que puedan ocurrir durante el acceso a la tabla o cálculos intermedios. Si un Q-value no se puede actualizar (ej. valor NaN/inf), loguear una advertencia y fallar.
4. `reset_agent()`
    - **Función:** Restablece las variables del agente específicas del episodio (epsilon, learning rate, contadores de ET) al final de cada episodio.
    - **Mejoras:**
        - Aplicar el decaimiento de epsilon y learning rate. Resetear todos los contadores de no mejora (no_improvement_counters) y banderas de terminación temprana. Si la paciencia es adaptativa (patience_type_config == 'adaptive'), recalcular patience_M basándose en el rendimiento del episodio anterior.
        - Mover el decaimiento de epsilon y learning rate a un `_update_params_learning_strategy()` que aplique reglas RRULE-like definidas en config.
        - La lógica de decaimiento de epsilon y learning rate y el reseteo de contadores de early termination se mantendrán, asegurando que sea eficiente.
5. `build_agent_state(system_raw_state_vector, active_controller, agent_state_definition)`
    - **Función:** Construye el diccionario de estado (agent_state_dict) del agente a partir del estado crudo del sistema y los parámetros del controlador.
    - **Mejoras:**
        - Este método solo debe ser para inicializar la estructura del diccionario de estado que el agente observa, no para construir o reconstruir Q-tables u otras estructuras internas del agente en cada llamada. La estructura de Q-tables y auxiliares se crea una vez en __init__ (vía _initialize_tables).
        - Utilizar self.state_config para determinar qué variables del estado del sistema y del controlador deben incluirse. Realizar el mapeo de system_raw_state_vector a nombres de variables y obtener las ganancias del controlador. No validar tipos de entrada (asumir que provienen correctamente del Environment).
#### 3.3.2 Métodos Secundarios

1. `_validate_and_prepare_state_config(config_to_validate, enabled_flag_key)`
    - **Función:** Valida la estructura y el contenido de la configuración de las variables de estado (state_config), asegurando que las variables habilitadas tengan min, max y bins válidos.
    - **Mejoras:**
        - Ejecutar validaciones estrictas de tipos y valores. Lanzar ValueError/TypeError si la configuración es incorrecta.
2. `_initialize_tables(self)`
    - **Función:** (Privado) Crea e inicializa las Q-tables, tablas de visita, y todas las tablas auxiliares (según self.reward_strategy.required_auxiliary_tables) y sus contadores de visita.
    - **Mejoras:**
        - Este método se llama una vez en __init__. Es crucial para la eficiencia que estas estructuras NumPy no se recreen.
3. `_discretize_value(value_to_discretize, state_variable_name)`
    - **Función:** Convierte un valor continuo de una variable de estado en su índice de bin discreto correspondiente.
    - **Mejoras:**
        - Implementar la discretización utilizando np.clip y np.linspace para un mapeo robusto.
4. `get_discrete_state_indices_tuple(self, current_agent_state: Optional[Dict[str, Any]], agent_var_name_context: str) -> Optional[tuple]`
    - **Función:** (Privado, pero podría ser público si otras clases lo necesitan) Convierte un agent_state_dict en una tupla de índices para acceder a las tablas.
    - **Mejoras:**
        - Mantener su lógica eficiente y genérico, para compatibilidad con otros tipos de agentes.
5. `should_episode_terminate_early(self) -> bool`
    - **Función:** Indica si el agente solicita terminar el episodio.
    - **Mejoras:**
        - Simplemente devuelve el estado agregado de self._request_early_termination_flags.
6. `get_last_early_termination_metrics()`
    - **Función:** Proporciona un snapshot de las métricas internas de la lógica de terminación temprana.

#### 3.3.3 Métodos Auxiliares

1. `_get_agent_defining_vars()`
    - **Función:** Retorna una lista de los nombres de las variables (ej. 'kp', 'ki', 'kd') que este agente está aprendiendo.
2. `_get_ordered_vars_for_agent_defining_var(agent_defining_variable_name)`
    - **Función:** Determina la secuencia de variables de estado que componen el estado discreto para la Q-tabla de una ganancia específica.
    - **Mejoras:**
        - Asegurar que la variable agent_defining_variable_name siempre sea la primera, seguida por las variables de estado adicionales especificadas en la configuración.
3. `_update_early_termination_params(self, agent_var_et_logic: str, current_interval_improvement_metric: float) -> float`
    - **Función:** (Nuevo Privado) Encapsula toda la lógica de actualización de contadores de paciencia, no mejora, y actualización de parámetros
    - **Mejoras:**
        - Esta modularización mejora la legibilidad de learn(). Debe manejar los tipos de paciencia (fija/adaptativa).
4. `_update_penalize_reward_params(self, agent_var_et_logic: str, current_interval_improvement_metric: float, r_learn_original: float) -> float`
    - **Función:** (Nuevo Privado) Encapsula toda la lógica de actualización de factor beta de penalización y la actualización de parámetros
    - **Mejoras:**
        - Esta modularización mejora la legibilidad de learn(). Debe manejar si la penalización está habilitada.
5. `get_agent_state_for_saving()`
    - **Función:** Prepara el estado interno completo del agente (tablas Q, contadores de visita, tablas auxiliares y sus contadores) en un formato serializable (ej. lista de diccionarios) para su guardado.
    - **Mejoras:**
        - Iterar sobre todas las Q-tables y las tablas auxiliares definidas por reward_strategy.required_auxiliary_tables. Convertir los arrays de NumPy a listas de Python y los valores a tipos nativos (float, int) para compatibilidad JSON.
6. `update_auxiliary_table_value(self, aux_table_id_name_update: str, gain_context_update: str, state_idx_tuple_update: tuple, new_value_update: float)`
    - **Función:** Actualiza un valor en una tabla auxiliar genérica y su contador de visitas.
    - **Mejoras:**
        - Actualización directa en self.auxiliary_tables_data y self.auxiliary_visit_counts.
7. `get_auxiliary_table_names(self) -> List[str]`
    - **Función:** Devuelve los nombres de las tablas auxiliares que el agente gestiona.
    - **Mejoras:**
        - Directamente retorna list(self.auxiliary_tables_data.keys()).

---

### 3.4 Consideraciones de integración

- El resto de métodos auxiliares y complementarios deberían mantenerse o ajustarse para cumplir con la lógica propuesta.
- El agente no debe tener conocimiento directo de estrategias específicas; su __init__ usa reward_strategy.required_auxiliary_tables para saber qué tablas auxiliares crear.
- Las RewardStrategy dependerán de los métodos genéricos get_auxiliary_table_value, update_auxiliary_table_value, y get_auxiliary_table_names para interactuar con el agente de forma agnóstica.
- Interactúa con la interfaz Controller para obtener las ganancias actuales al construir el estado.
- **API Pública Inmutable**: conservar exactamente las firmas de `select_action`, `learn` y `reset_agent` para no romper la llamada desde `SimulationManager`.
- **Naming Consistency**: mantener y estandarizar nombres `state`, `action`, `reward`, `next_state`, `done` en toda la cadena de llamadas cuando corresponda.
- **Logging**: registrar sólo en nivel _info_ la selección de acción y la actualización de episodios completos; eliminar logs de cada paso de tabla.
- **Fail-Fast**: confiar en las factorías y en los constructores para validación de tamaños de tablas y rangos de discretización; no revalidar en métodos de lógica.
- Sus métricas internas son expuestas para MetricsCollector.


---


## 4. pendulum_virtual_simulator.py (Representante de cualquier VirtualSimulator)

**Declaración de intenciones**  
El módulo `pendulum_virtual_simulator.py` debe erigirse como un **sub-orquestador especializado** que ejecute simulaciones contrafactuales o rollouts (en este caso, de un péndulo invertido) **totalmente aislado** del entorno real, para calcular recompensas “echo” sin introducir validaciones de dominio en cada iteración. Debe:
- **Clonar** una única vez las plantillas de sistema, controlador y función de recompensa al inicio de cada intervalo, garantizando que las instancias originales queden inalteradas.
- Iterar de forma **lineal y explícita** un número determinado de pasos, invocar en cada paso al controlador clonado, avanzar la dinámica clonada y acumular recompensa y obtener promedio de la estabilidad.
- Reportar únicamente dos valores al terminar el intervalo: la **recompensa total** y la **estabilidad promedio**, sin excepciones intermedias (salvo un `try/except` de muy alto nivel que retorne valores y contexto del error).

---

### 4.1 Responsabilidad única

1. **Ejecutar simulaciones virtuales** en intervalos de decisión, usando componentes clonados para calcular una recompensa contrafactual y un score de estabilidad.
2. **No debe** gestionar lógica de aprendizaje, métricas externas, ni control de flujo de episodios; eso lo hace `SimulationManager`.
3. Implementar la interfaz VirtualSimulator.
4. Ejecutar simulaciones de intervalos (rollouts) de forma aislada y contrafactual, sin afectar el estado del entorno real.
5. Utilizar copias profundas (o instancias clonadas) del DynamicSystem, Controller y RewardFunction para cada simulación virtual.
6. Configurar el Controller virtual con un conjunto fijo de ganancias para la duración del rollout.
7. Acumular la recompensa total y el promedio de la métrica de estabilidad durante el rollout virtual.

---

### 4.2 Lógica principal clara, lineal y legible

1. `__init__`: 
	- Recibe instancias de DynamicSystem, Controller, RLAgent, RewardFunction y la config completa. Almacena estas dependencias y extrae parámetros de configuración necesarios para su operación (e.g., dt_sec, controller_reset_level_on_episode_end).
	- Validar e inyectar plantillas de `DynamicSystem`, `Controller` y `RewardFunction`.
	- Verificar `dt` positivo y finito; almacenar en `self.dt_virtual`.
	- Comprobar que el controlador plantilla implemente `reset_internal_state`, `update_params` y `compute_action`.
2. `run_interval_simulation(initial_state, start_time, duration, gains_dict)`
    - Prepara el entorno virtual creando copias profundas del sistema, controlador y función de recompensa.
    - Resetear el estado interno del Controller virtual.
    - Configura el controlador virtual con las ganancias fijas proporcionadas.
    - Ejecuta un bucle de simulación paso a paso para la duración del intervalo, aplicando acciones y acumulando recompensas y estabilidad.
    - Retorna la recompensa total y la estabilidad promedio del intervalo virtual.


---

### 4.3 Métodos y mejoras clave

> **Pauta general:** cada método privado debe tener una firma estática y delegar tareas al clon; no debe repetirse lógica de validación en cada paso.

#### 4.3.1 Métodos Principales

1. `__init__(system_template_instance, controller_template_instance, reward_function_template_instance, dt_sec_value)`
    - **Función:** Inicializa el simulador virtual con instancias plantilla de los componentes principales y el dt_sec.
    - **Mejoras requeridas:**
		1. **Fail-Fast Validation:** Validar estrictamente que las instancias plantilla (system_template_instance, controller_template_instance, reward_function_template_instance) son del tipo de interfaz correcto. Validar dt_sec_value (debe ser numérico, positivo y finito).
		2. **Compatibilidad de Componentes:** Asegurar que el controller_template_instance tenga los métodos necesarios para ser funcional en el simulador virtual (reset_internal_state, update_params, compute_action).
2. `run_interval_simulation(initial_state_vector_input, interval_start_time_sec, interval_total_duration_sec, fixed_controller_gains_map) → Tuple[float,float]`
    - **Función:** ejecutar el rollout completo, devolviendo recompensa y estabilidad promedio.
    - **Mejoras requeridas:**
        1. Extraer la **clonación** en un método `_clone_components()` para evitar duplicación y permitir optimizaciones futuras (p.ej. clonar una sola vez y resetear internamente)

#### 4.3.2 Métodos Secundarios

No hay

#### 4.3.3 Métodos Auxiliares

No hay

### 4.4 Consideraciones de integración

- **Firma pública inmutable**: `run_interval_simulation` debe seguir recibiendo `(initial_state, start_time, duration, gains_dict)` y devolviendo `(float, float)`.
- **Inyección de dependencias**: recibir en el constructor sólo las **plantillas** y `dt`, sin lógica adicional.
- **Configuración declarativa**: `SimulationManager` decide cuándo y con qué parámetros invocar este simulador; aquí no debe haber lógica de branching según estrategia.
- **Fail-Fast & Logging**: todas las validaciones críticas en constructor; en `run_interval_simulation` capturar sólo excepciones de ejecución para retornar valores por defecto y contexto para registrar el error.


---


## 5. instantaneous_reward_calculator.py (Representante de RewardFunction) & stability_calculator.py (Nuevo componente)

**Declaración de intenciones**  
El módulo `instantaneous_reward_calculator.py` y `|.py` actuarán como los **únicos responsables** de la computación de la recompensa instantánea y la medida de estabilidad en cada paso de simulación, **totalmente desacoplado** de la lógica de aprendizaje o del entorno físico. Su API pública ofrecerá únicamente un método `calculate(state, action, next_state, time) → (reward) o  (stability_score)`, delegando toda la lógica de fórmulas específicas a componentes configurables y manteniendo el código **directo**, **explicativo** y sin validaciones internas de tipos o rangos (fail-fast en constructores).

---

### 5.1 Responsabilidad única

1. Implementar la interfaz RewardFunction y BaseStabilityCalculator.
2. Es responsabilidad de instantaneous_reward_calculator el calcular el valor de la recompensa instantánea (calculated_reward_value) en cada paso, basándose en el método configurado (e.g., weighted_exponential, stability_measure_based).
3. Es responsabilidad de stability_calculator el calcular siempre una métrica de estabilidad (current_stability_score, anteriormente w_stab) delegando a la instancia de BaseStabilityCalculator inyectada.
4. **No debe** orquestar simulaciones, actualizar agentes, ni gestionar flujos de datos o excepciones intermedias.

---

### 5.2 Lógica principal clara, lineal y legible

1. `__init__`: 
	- Recibe:
		- `calculation_config` (tipo de reward: `"weighted_exponential"` o `"stability_based"`, y parámetros de pesos/coeficientes)
		- Instancia de `BaseStabilityCalculator` implementa opcionalmente `calculate_stability_based_reward`.
	- Asigna parámetros directos a atributos y **no** valida internamente (fail-fast si algo falta en config).
2. `calculate(state, action, next_state, time)`
    - Retorna la recompensa instantáneo y el score de estabilidad de forma independiente.
    - En caso de que la recompensa se base en stability_calculator, llamar a los métodos privados necesarios para componer la `calculate_stability_based_reward`
3. Otros auxiliares que correspondan

---

### 5.4 Consideraciones de integración

- **API pública inmutable**: el método `calculate(state, action, next_state, time)` debe conservar su firma pero separando ahora claramente las responsabilidades de cada componente.
- **Inyección de Dependencias**: recibir en el constructor únicamente el `calculation_config` y la instancia de `BaseStabilityCalculator` si es que corresponde.
- **Configuración Declarativa**: no leer directamente `config.yaml`; asumir que `calculation_config` ya proviene de la factoría `RewardFactory` y `Stability_Calculator` en caso de que sea necesario.
- **Naming Consistency**: usar nombre de variables de`state`, `action`, `next_state`, `time`, `reward` y `stability` de forma uniforme en todo el proyecto.
- **Fail-Fast & Logging**: no capturar excepciones de dominio; cualquier fallo debe propagarse al `SimulationManager`. Solo un log al inicio del cálculo y, opcionalmente, al final con nivel `DEBUG`.



---


## 6. ira_stability_calculator.py & 7. simple_exponential_stability_calculator.py

**Declaración de intenciones**  
Cada una de estas clases implementa un **algoritmo específico para el cálculo de estabilidad** que gestionará el nuevo componente `stability_calculator.py`. Su responsabilidad única es traducir un estado del sistema (vector de valores continuos) en una puntuación numérica de estabilidad (w_stab en [0, 1]) y, si se solicita, en una recompensa basada exclusivamente en la estabilidad pero que deberá gestionar externamente la encargada de calcular la recompensa instantánea. También gestionan sus propias estadísticas de referencia internas (ej. mu/sigma para IRA) y las actualizan si son calculadores adaptativos. No tienen conocimiento de la RewardFunction ni del RLAgent; solo proporcionan métricas de estabilidad.

El módulo `ira_stability_calculator.py` será el **único** encargado de convertir un vector de estado continuo en un **score de estabilidad** basado en la métrica IRA (Z-score adaptativo), **completamente desacoplado** de cualquier otro componente. Debe:
- Normalizar cada característica del estado usando estadísticas de referencia adaptativas.
- Calcular un único valor escalar de estabilidad mediante la suma cuadrática de Z-scores ponderados y la aplicación de una función de decaimiento exponencial.
- Ofrecer además un método para actualizar dichas estadísticas de referencia al finalizar cada episodio.
- Exponer una API mínima y estática (`calculate_instantaneous_stability`, `update_reference_stats`, `get_current_adaptive_stats`), sin validaciones internas de tipos o rangos (fail-fast en constructor) y con un único try/except de nivel medio para logging de errores críticos.

El módulo `simple_exponential_stability_calculator.py` debe erigirse como el **componente ligero** encargado de convertir un vector de estado continuo en un **score de estabilidad** mediante una suma ponderada de características escaladas y una función de decaimiento exponencial, **totalmente desacoplado** del resto del sistema. Su API pública ofrecerá solo un método `calculate_instantaneous_stability(state: np.ndarray) → float`, manteniendo el código **conciso**, **explícito** y sin validaciones internas de tipo o rango (fail-fast en el constructor).

---

### 4.1 Responsabilidad única

1. Implementar la interfaz BaseStabilityCalculator.
2. Traducir un estado de sistema continuo a un escalar de estabilidad stability_score ∈ [0, 1] (anteriormente w_stab).
3. Manejar dinámicamente las variables de estado y sus ponderaciones/escalas/parámetros de decaimiento definidos en la configuración, sin hardcodear nombres de variables específicas del péndulo.
4. (Solo IRAStabilityCalculator): Gestionar estadísticas de referencia adaptativas (mu, sigma) si está configurado.

---

### 4.4 Consideraciones de integración

- Son instanciados e inyectados por StabilityCalculator.
- La configuración (exp_decay_metric_params o ira_zscore_metric_params) define completamente su comportamiento.
- El system_feature_indices debe ser generalizado para el manejo de cualquier tipo de sistema.
- **API pública inmutable**: conservar los nombres y firmas de los cuatro métodos principales para no romper la integración con `instantaneous_reward_calculator` o `SimulationManager`.
- **Inyección de dependencias**: el constructor solo recibe `ira_zscore_metric_params`;  y `scales` y `decay_coeffs` respectivamente, y **no** acceder al config global. Todos los cálculos posteriores usan datos internos.
- **Naming consistency**: usar `state`, `history`, `current_episode_num`, `mu`, `sigma` de forma uniforme.
- **Fail-Fast**: asumir que `ira_zscore_metric_params` es válido; los errores de estadística se capturan únicamente en `update_reference_stats`.
- **Logging**: emitir solo logs de nivel `INFO` en inicialización y cambios de estadística, y `ERROR` en fallos de cálculo; delegar a `SimulationManager` el nivel de detalle global.



---

## 8. global_reward_strategy.py & 9. shadow_baseline_reward_strategy.py & 10. echo_baseline_reward_strategy.py

(juntos por similitud, todos implementan RewardStrategy)

**Declaración de intenciones**  
Cada una de estas clases implementa una **estrategia de recompensa específica**. Su responsabilidad única es definir cómo la información de recompensa "cruda" de un intervalo (recompensa real del entorno, puntuación de estabilidad, posibles recompensas contrafactuales) **se transforma en la recompensa de aprendizaje (R_learn) que el RLAgent debe utilizar para actualizar sus valores Q**. Sirven como una capa de abstracción entre las señales de recompensa del entorno y el mecanismo de aprendizaje del agente. Además, declaran sus necesidades de infraestructura (needs_virtual_simulation, required_auxiliary_tables) al SimulationManager y al RLAgent.

El módulo `global_reward_strategy.py` será el **componente más simple** de las estrategias de recompensa, encargado **únicamente** de devolver la recompensa real acumulada en un intervalo de decisión, sin necesidad de simulaciones virtuales ni tablas auxiliares. Su API pública se limitará a un único método `compute_reward_for_learning(real_interval_reward, …) → float`, garantizando un comportamiento **predecible**, **performante** y **aglutinador** de los datos de recompensa calculados por el entorno.

El módulo `shadow_baseline_reward_strategy.py` debe comportarse como una **estrategia de recompensa basada en baseline**, calculando la señal de aprendizaje como la **diferencia** entre la recompensa real y un valor de baseline almacenado en una tabla auxiliar y **actualizando** dicho baseline a medida que cambian las ganancias del controlador. Debe ser **ligero**, **determinista** y **plug-and-play**, sin simulaciones virtuales ni lógica de aprendizaje extraña al cálculo de la diferencia, pero lo suficientemente general para fácilmente integrar métodos basados en este enfoque.

El módulo `echo_baseline_reward_strategy.py` define una **estrategia “echo”** que utiliza el resultado de una **simulación virtual** para calcular la recompensa de aprendizaje como la **diferencia** entre la recompensa real y la contraparte contrafactual —todo ello **externo** al agente— entregando únicamente esa señal sin lógica adicional. Debe ser **plug-and-play**, **eficiente** y **completamente desacoplado** de cualquier tabla auxiliar.

---

### 4.1 Responsabilidad única

1. Implementar la interfaz RewardStrategy.
2. Traducir la "información cruda" del intervalo (recompensa real, avg_interval_stability_score, tablas auxiliares del agente, resultados de simulaciones virtuales) a la recompensa final (R_learn) que el RLAgent debe usar para actualizar su Q-table para una ganancia específica.
3. Declarar sus necesidades de infraestructura a través de los atributos de clase needs_virtual_simulation y required_auxiliary_tables.
4. (Para ShadowBaseline y potencialmente otras): Interactuar con el RLAgent (a través de agent.get_auxiliary_table_value y agent.update_auxiliary_table_value) para leer/escribir en tablas auxiliares (e.g., la tabla de baseline B(S)).


---

### 4.4 Consideraciones de integración

- Estas estrategias son instanciadas por RewardFactory según la configuración en environment.reward_setup.reward_strategy.
- Nuevas estrategias solo necesitan implementar la interfaz RewardStrategy, definir sus atributos declarativos, y ser registradas en RewardFactory y config.yaml.
- El RLAgent es responsable de crear las tablas auxiliares que la estrategia declara necesitar; la estrategia solo las usa a través de los métodos genéricos del agente.
- **Firma pública inmutable:** conservar exactamente `compute_reward_for_learning(real_interval_reward, **kwargs)` para que `SimulationManager` y `Agent` sigan funcionando sin cambios.
- **Configuración declarativa:** registrar en la factoría `RewardFactory` con parámetros `needs_virtual_simulation` y `required_auxiliary_tables` según correspondan, garantizando que el gestor de simulaciones gestione la creación de `VirtualSimulator` y solicite tablas auxiliares cuando sea necesario.
- **Plug-and-play:** gracias a su sencillez, puede reemplazarse por cualquier otra Reward_Strategy sin tocar el core.
- **Naming consistency**: usar `real_interval_reward`, `state_key`, `auxiliary_tables` uniformemente, al igual que `differential_rewards_map`, `episode_idx`, `interval_idx` según corresponda.
- **Logging:** no requiere logs internos de valor de recompensa; en caso de necesitar trazabilidad, `SimulationManager` podrá interceptar el valor antes o después de la llamada.
- **Fail-Fast:** Se asume que `real_interval_reward` es un número válido, confiar en que `auxiliary_tables` contiene la tabla correcta; cualquier error de tipo o valor provendrá del entorno.

