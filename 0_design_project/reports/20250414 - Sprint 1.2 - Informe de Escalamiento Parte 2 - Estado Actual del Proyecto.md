---
created: 20250414 06:04
update: 20250422-19:01
summary: 
status: done
link: 
tags:
  - content
---
## Documento del Estado Actual del Proyecto (v3.0.0)

**1. Objetivo del Proyecto:**

- El objetivo central sigue siendo entrenar un agente de Aprendizaje por Refuerzo (RL) para adaptar dinámicamente las ganancias (Kp, Ki, Kd) de un controlador PID que estabiliza un péndulo invertido sobre un carro.
- El agente, basado en Q-learning (implementado en PIDQLearningAgent), aprende a ajustar las ganancias PID para maximizar una función de recompensa acumulada.
- **Actualización Clave:** Se han introducido estrategias de recompensa diferenciadas (global, shadow-baseline, echo-baseline) como componentes intercambiables para evaluar el impacto de cada ajuste de ganancia de forma más precisa, utilizando un RewardStrategy inyectado en el agente.
- **Actualización Clave:** El cálculo de la estabilidad del sistema (usado para la recompensa o métricas) se ha abstraído en una interfaz BaseStabilityCalculator con implementaciones concretas (IRA, Exponencial Simple), permitiendo flexibilidad y comparación.

**2. Arquitectura y Componentes:**

```
RL-Agent-Factory/
├── config.yaml                     
├── sub_config_visualization.yaml   
├── main.py                         
├── config_loader.py                
├── world_initializer.py            
├── simulation_manager.py           
├── result_handler.py               
├── visualization_runner.py         
├── heatmap_generator.py            
│
├── components/
│   ├── agents/
│   │   └── pid_qlearning_agent.py
│   ├── analysis/
│   │   ├── extended_metrics_collector.py
│   │   ├── ira_stability_calculator.py
│   │   ├──simple_exponential_stability_calculator.py
│   │   └── pendulum_metrics_analyzer.py
│   ├── controllers/
│   │   └── pid_controller.py
│   ├── environments/
│   │   └── pendulum_environment.py
│   ├── rewards/
│   │   └── gaussian_reward.py
│   ├── reward_strategies/
│   │   ├── global_reward_strategy.py
│   │   ├── shadow_baseline_reward_strategy.py
│   │   └── echo_baseline_reward_strategy.py
│   └── simulators/
│       └── pendulum_virtual_simulator.py
│
├── factories/
│   ├── environment_factory.py        # Requiere modificación para inyectar reward_function y strategy (ver nota abajo)
│   ├── reward_factory.py             # Requiere modificación para crear stability_calculator (ver nota abajo)
│   └── agent_factory.py
│   ├── controller_factory.py 
│   └── system_factory.py 
│
├── interfaces/
│   ├── reward_strategy.py
│   ├── stability_calculator.py
│   ├──controller.py
│   ├── dynamic_system.py
│   ├── environment.py
│   ├── metrics_analyzer.py
│   ├── metrics_collector.py
│   ├── rl_agent.py
│   ├── reward_function.py
│   └── virtual_simulator.py
│
├── utils/
│   ├── data_processing.py            
│   ├── visualization.py              
│   ├── numpy_encoder.py              
│   └── episode_saver.py              
│
├── data/
│   └── (configs/, logs/ - generados)
│
├── results_history/
│   └── (results...)
│
├── tests/
│   └── (...)                       # No modificado en este plan
│
└── requirements.txt                # Asegurar que incluye 'pyyaml', 'numpy', 'pandas', 'openpyxl'
```


- **Diseño Modular Mejorado:** El proyecto ahora sigue una arquitectura significativamente más modular y desacoplada. La lógica monolítica de main.py ha sido distribuida en módulos especializados:
    
    - main.py: Punto de entrada y orquestador de alto nivel.
    - config_loader.py: Carga y valida la configuración (config.yaml, sub_config_visualization.yaml).
    - world_initializer.py: Centraliza la creación de todos los componentes principales (entorno, agente, controlador, reward, stability, strategy, collector, simulator) utilizando las fábricas correspondientes y gestionando la inyección de dependencias.
    - simulation_manager.py: Contiene y ejecuta el bucle principal de simulación (episodios y pasos), manejando la lógica de tiempo, llamadas a env.step, agent.learn, env.reset, y la recolección detallada de métricas en cada paso.
    - result_handler.py: Centraliza todas las operaciones de guardado de resultados (lotes de episodios, metadatos, estado del agente JSON/Excel, tabla de resumen, y disparo de la generación de datos para heatmaps).
    - visualization_runner.py: Dispara la generación de gráficos basada en la configuración y los datos recopilados.
    - heatmap_generator.py: Nuevo módulo independiente (ejecutable por línea de comandos) que procesa los datos detallados guardados (simulation_data_*.json) y genera un archivo Excel (data_heatmaps.xlsx) con los datos de histogramas 2D precalculados, desacoplando la generación de datos de la visualización.
        
- **Interfaces (ABCs):** Se mantienen y se han añadido nuevas (RewardStrategy, BaseStabilityCalculator), reforzando la modularidad y extensibilidad.
- **Fábricas:** Las fábricas (EnvironmentFactory, RewardFactory, AgentFactory, etc.) ahora manejan la creación de componentes con sus dependencias actualizadas (e.g., RewardFactory crea StabilityCalculator, AgentFactory recibe RewardStrategy).
- **Componentes Principales (Actualizados):**
    - PendulumEnvironment: Coordina la interacción, recibe reward_function pre-creada.
    - InvertedPendulumSystem, PIDController: Sin cambios funcionales mayores (se añadió get_params a PID).
    - PIDQLearningAgent:
        - Recibe RewardStrategy inyectada en el constructor.
        - La lógica de learn() ahora delega el cálculo de la recompensa efectiva a la RewardStrategy.
        - Mantiene el uso interno de arrays NumPy para Q/Visits/Baselines para eficiencia.
        - get_agent_state_for_saving() sigue transformando NumPy a dict JSON (incluyendo tablas Baseline).
        - Se añadieron métodos helper (get_q_values_for_state, get_visit_counts_for_state, get_baseline_value_for_state, get_last_td_errors) para facilitar el logging detallado desde simulation_manager.
        - La representación del estado sigue siendo configurable vía state_config.
    - GaussianReward: Recibe BaseStabilityCalculator inyectado. Utiliza use_stability_based_reward para decidir si calcular recompensa Gaussiana o delegar al calculador.
    - IRAStabilityCalculator, SimpleExponentialStabilityCalculator: Implementan BaseStabilityCalculator. IRA maneja estadísticas adaptativas (update_reference_stats, get_current_adaptive_stats).
    - RewardStrategy (Implementaciones: GlobalRewardStrategy, ShadowBaselineRewardStrategy, EchoBaselineRewardStrategy): Encapsulan la lógica específica para determinar la recompensa usada en la actualización Q-learning de cada ganancia. ShadowBaseline actualiza internamente B(s).
    - ExtendedMetricsCollector: (Reemplaza SimpleMetricsCollector). Recolecta un conjunto mucho más amplio de métricas en cada paso de tiempo y decisión (estado del sistema, estado del controlador, estado interno del agente - Q/Visits/Baselines/TD Errors, recompensas virtuales, parámetros de RL, métricas de estabilidad, estadísticas adaptativas, duración de cómputo).
- **Utilidades:**
    - data_processing.py: summarize_episode ahora procesa todas las métricas nuevas y calcula estadísticas agregadas. save_summary_table reside ahora en result_handler.
    - visualization.py: Sin cambios funcionales mayores, consume datos de summary_df y all_episodes_data.
    - episode_saver.py, agent_state_manager.py: Funcionalidad movida a result_handler.py (marcados como deprecados).
    - numpy_encoder.py: Sin cambios.
- **Configuración:** config.yaml y sub_config_visualization.yaml se mantienen sin cambios estructurales, pero su contenido ahora impulsa la selección de estrategias y calculadores, además de la generación de datos para heatmaps.

**3. Estado Actual y Funcionalidad:**

- El código ha sido **refactorizado exitosamente** según el plan del Sprint 1.2 (v3.0.0).
- La ejecución está ahora **altamente modularizada**.
- Las estrategias de recompensa (RewardStrategy) y los cálculos de estabilidad (BaseStabilityCalculator) son **componentes intercambiables** configurables vía config.yaml.
- Se implementa la **recolección exhaustiva de métricas** a través de ExtendedMetricsCollector y simulation_manager.
- El **resumen por episodio** (summary.xlsx) es completo, incluyendo estadísticas agregadas de todas las métricas relevantes.
- La **generación de datos para heatmaps** (data_heatmaps.xlsx) está funcional y desacoplada del proceso principal de visualización.
- Se mantiene el guardado periódico del **estado completo del agente** (Q, Visits, Baselines) en JSON y su conversión final a Excel.
- El sistema es funcional para los modos global, shadow-baseline y echo-baseline.

**4. Puntos Clave y Consideraciones (v3.0.0):**

- **Representación del Estado del Agente:** Sigue siendo el punto de diseño más crítico. La config.yaml actual no incluye el estado físico (ángulo/velocidad) en state_config, lo que limita la capacidad del agente para adaptar las ganancias contextualmente. Habilitar estas variables aumentará drásticamente el tamaño de las tablas Q/Visit/Baseline.
- **Abstracción y Testabilidad:** La inyección de dependencias (Strategy, Calculator) mejora significativamente la testabilidad y flexibilidad del sistema. Nuevas estrategias o calculadores pueden añadirse implementando las interfaces correspondientes.
- **Riqueza de Métricas:** La gran cantidad de métricas recolectadas permite un análisis muy profundo del comportamiento del sistema y del agente. Sin embargo, los archivos simulation_data_*.json pueden volverse muy grandes.
- **Heatmaps Desacoplados:** data_heatmaps.xlsx permite que la visualización (potencialmente con herramientas externas como Tableau, PowerBI o scripts Python separados) no dependa de recargar y procesar los grandes archivos simulation_data_*.json.
- **Rendimiento:** El logging detallado en cada dt podría introducir una sobrecarga. El modo echo-baseline sigue siendo computacionalmente intensivo debido a las simulaciones virtuales.
- **Ajuste Fino:** La necesidad de ajustar hiperparámetros (pesos/escalas de recompensa, parámetros RL, beta para Shadow, lambda/pesos/stats para IRA) persiste y es crucial para un buen rendimiento.
- **Claridad del Código:** La refactorización y los comentarios/fórmulas explícitas mejoran la mantenibilidad.

**5. Próximos Pasos / Posibles Mejoras:**

- **Evaluar la Representación del Estado:** Prioridad alta. Experimentar incluyendo angle y angular_velocity en state_config y analizar el impacto en el aprendizaje y el tamaño de las tablas.
- **Ajuste Fino:** Ajustar sistemáticamente los hiperparámetros y la función de recompensa.
- **Análisis Avanzado:** Desarrollar scripts o notebooks para analizar en profundidad los datos en summary.xlsx, data_heatmaps.xlsx y agent_state_tables.xlsx (e.g., visualizar convergencia de Q-values, correlacionar TD errors con cambios de ganancia, analizar impacto de beta o estadísticas adaptativas).
- **Pruebas de Robustez:** Evaluar el rendimiento con diferentes condiciones iniciales, ruido, o parámetros del sistema físico.
- **Implementar Más Componentes:** Añadir nuevos agentes (e.g., DQN, Actor-Critic), estrategias de recompensa, o calculadores de estabilidad para aprovechar la modularidad.
- **Optimizar Rendimiento:** Si es necesario, optimizar el logging detallado o la ejecución del simulation_manager.

## Flujo Detallado de Datos Estructurado (v3.0.0)

**Fase 1: Inicialización (orquestado por main.py)**

1. **main.py**: Llama a config_loader.load_and_validate_config() -> retorna config, vis_config (dicts).
2. **main.py**: Llama a result_handler.setup_results_folder(config) -> crea carpeta, retorna results_folder (string).
3. **main.py**: Llama a world_initializer.initialize_simulation_components(config):
    - **WorldInitializer**: Crea ExtendedMetricsCollector.
    - **WorldInitializer -> RewardFactory.create_stability_calculator(reward_cfg)** -> retorna stability_calculator (objeto BaseStabilityCalculator o None).
    - **WorldInitializer -> RewardFactory.create_reward_function(reward_cfg, stability_calculator)** -> retorna reward_function (objeto RewardFunction).
    - **WorldInitializer**: Crea reward_strategy (objeto RewardStrategy) basado en `reward_cfg['reward_mode']`.
    - **WorldInitializer**: Modifica `config['environment']['agent']['params']` inyectando reward_strategy_instance (y shadow_baseline_params si aplica).
    - **WorldInitializer -> EnvironmentFactory.create_environment(config, reward_function_instance)`**:
        - **EnvironmentFactory**: Llama a SystemFactory, ControllerFactory.
        - **EnvironmentFactory -> AgentFactory.create_agent(agent_type, agent_params)`**:
            - **AgentFactory**: Extrae reward_strategy_instance de agent_params. Crea PIDQLearningAgent, pasándole reward_strategy, state_config, etc. Retorna agent.
        - **EnvironmentFactory**: Crea PendulumEnvironment, pasándole system, controller, agent, reward_function, dt, etc. Retorna env.
    - **WorldInitializer**: Limpia las modificaciones temporales de config.
    - **WorldInitializer**: Crea PendulumVirtualSimulator si reward_mode es echo-baseline.
    - **WorldInitializer**: Retorna components (dict con env, metrics_collector, agent, controller, reward_function, stability_calculator, reward_strategy, virtual_simulator).
4. **main.py**: Llama a result_handler.save_metadata(metadata_dict, results_folder).

**Fase 2: Ejecución de la Simulación (orquestado por main.py, ejecutado por simulation_manager.py)**

1. **main.py**: Llama a simulation_manager.run_simulation(components, config, results_folder).
2. **SimulationManager**: Extrae componentes y parámetros del config.
3. **SimulationManager (Bucle de Episodios)**: Para cada episode:
    - Llama a env.reset(initial_state_vector) -> retorna state_vector. (Internamente resetea system, controller, agent).
    - Llama a metrics_collector.reset(episode_id).
    - Loguea estado inicial T=0 (metrics_collector.log(...) para estado físico, Kp/Ki/Kd, errores PID, epsilon/lr, acciones NaN, etc. Llama a log_q_values, log_baselines, etc., con estado inicial).
    - Selección de acción inicial: agent.build_agent_state(), agent.select_action(), metrics_collector.log(action_...).
    - **SimulationManager (Bucle de Pasos dt)**: Para cada t_step:
        - Calcula current_time. Guarda state_vector si es decision_boundary.
        - Llama a env.step(actions) -> retorna next_state_vector, (reward, stability_score), force.
        - Loguea métricas del paso (metrics_collector.log para estado físico, Kp/Ki/Kd, error PID, int_err, deriv_err, epsilon/lr, reward, cum_reward, force, stability, gain_step).
        - Llama a env.check_termination(config) -> determina done, termination_reason.
        - **Bloque de Decisión/Aprendizaje (if current_time >= next_decision_time or done)**:
            - Incrementa agent_decision_count. Loguea id_agent_decision.
            - Calcula reward_for_agent_info (dict, tuple, o float). Si es echo, llama a virtual_simulator.run_interval_simulation y loguea metrics_collector.log_virtual_rewards.
            - Loguea estado del agente antes de aprender: metrics_collector.log_q_values, metrics_collector.log_q_visit_counts, metrics_collector.log_baselines.
            - Llama a agent.learn(..., reward_for_agent_info, ...) (usa reward_strategy interna).
            - Loguea estado del agente después de aprender: metrics_collector.log_td_errors(agent.get_last_td_errors()).
            - Si no done, selecciona nuevas actions (agent.select_action).
            - Loguea action_kp/ki/kd (nuevas si no done, NaN si done).
            - Resetea interval_reward, interval_stability_scores. Actualiza current_agent_state_dict, next_decision_time.
            - Loguea learn_select_duration_ms.
        - **Bloque NO Decisión**: Loguea action_kp/ki/kd (las activas actualmente). Loguea NaN para métricas de decisión (duración, ID, Q/Baseline/TD/Virtual).
        - Si done, break del bucle de pasos.
        - Actualiza state_vector = next_state_vector.
    - **SimulationManager (Fin de Episodio)**:
        - Loguea episode_duration_s, total_agent_decisions, final_kp/ki/kd.
        - Obtiene episode_data = metrics_collector.get_metrics(). Añade termination_reason, avg_stability_score.
        - Llama a env.update_reward_calculator_stats(episode_data, episode) (interno llama a reward_function.update_calculator_stats -> stability_calculator.update_reference_stats).
        - Loguea estadísticas adaptativas: stats = stability_calc.get_current_adaptive_stats(), metrics_collector.log_adaptive_stats(stats).
        - Obtiene episode_data final (con stats).
        - Appendea a episode_batch, all_episodes_data.
        - Llama a data_processing.summarize_episode(episode_data) -> retorna summary (dict). Appendea a summary_data.
        - Llama a result_handler.save_episode_batch(...) y result_handler.save_agent_state(...) periódicamente.
4. **SimulationManager**: Retorna all_episodes_data, summary_data a main.py.

**Fase 3: Finalización y Salidas (orquestado por main.py, ejecutado por result_handler.py)**

1. **main.py**: Llama a result_handler.finalize_results(config, summary_data, all_episodes_data, agent, results_folder).
2. **ResultHandler.finalize_results**:
    - Llama a save_agent_state(agent, final_episode, results_folder) (usa agent.get_agent_state_for_saving() que transforma NumPy a dict; guarda JSON). Actualiza _last_saved_agent_json_path.
    - Llama a convert_json_agent_state_to_excel(_last_saved_agent_json_path, ...) (lee JSON, crea DataFrames, guarda agent_state_tables.xlsx).
    - Llama a save_summary_table(summary_data, ...) (crea DataFrame, guarda summary.xlsx).
    - Encuentra el archivo simulation_data_*.json más reciente.
    - Llama a heatmap_generator.generate_heatmap_data(latest_sim_data_path, heatmap_configs, ...) (lee JSON detallado, calcula histogramas 2D, guarda data_heatmaps.xlsx).

**Fase 4: Visualización (orquestado por main.py, ejecutado por visualization_runner.py)**

1. **main.py**: Llama a visualization_runner.run_visualizations(vis_config, summary_data, all_episodes_data, results_folder).
2. **VisualizationRunner**: Crea summary_df = pd.DataFrame(summary_data).
3. **VisualizationRunner**: Llama a `utils.visualization.generate_plots(vis_config['plots']`, summary_df, all_episodes_data, results_folder).
4. **generate_plots**: Itera sobre plot_configs. Llama a las funciones correspondientes (plot_generic_bar, plot_generic_line, plot_generic_heatmap) pasando summary_df o all_episodes_data. Cada función de plot guarda una imagen .png.
