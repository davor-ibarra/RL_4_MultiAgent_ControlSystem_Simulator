---
created: 20250305 10:03
update: 20250416-02:37
summary: Release and Sprint control
status: 
link: 
tags:
  - High-Level-Note
---
# WIP

**Tablero Scrum:**

[[]]

# Check List
## Product Backlog
- [~] PBI1: MVP Funcional de Sintonizaci√≥n PID P√©ndulo
- [~] PBI2: Control Avanzado de P√©ndulo y Adaptabilidad
- [ ] PBI3: Implementaci√≥n del N√∫cleo del Framework Extensible
- [ ] PBI4: Demostraci√≥n de Extensibilidad del Framework
## Stage Check
- [ ] **Release 1: MVP y Control Avanzado del P√©ndulo**¬†(En Progreso)
	- [~] **Sprint 1.1: Establecimiento de L√≠nea Base y Validaci√≥n MVP Inicial**¬†(Completado)
		- **Objetivo:**¬†Validar implementaci√≥n actual (post-NumPy/estado) y establecer l√≠nea base fiable.
		- **Resultado Esperado:**¬†C√≥digo base validado, estado del agente guardable/cargable, m√©tricas consistentes.
		- **Outcome:**¬†Se genera informe de estado actual detallando la funcionalidad, flujo de datos y validaci√≥n de la implementaci√≥n con tablas NumPy y guardado de estado. C√≥digo base estable para iniciar Sprint 1.2.
	- [~] **Sprint 1.2: Recompensas Diferenciales y Evaluaci√≥n del Impacto de Acciones
		- **Objetivo:**¬†Reemplazar el modelo de recompensa global por un mecanismo que permita medir el impacto individual de las acciones de cada agente (Kp, Ki, Kd) en la estabilidad del sistema, utilizando simulaciones virtuales y baselines por estado.
		- **Resultado Esperado:**¬†Sistema entrenando con recompensas diferenciadas, permitiendo evaluar qu√© tan beneficiosa fue la acci√≥n de cada agente. L√≥gica de entrenamiento m√°s explicativa y trazable para futuras mejoras.
		- **Outcome:** Los nuevos modos `echo-baseline` y `shadow-baseline` fueron integrados exitosamente, permitiendo descomponer el efecto de cada ganancia del PID sobre la recompensa global. Esto allana el camino para entrenamientos m√°s estables y agentes con roles diferenciados. El c√≥digo fue adaptado con m√≠nima refactorizaci√≥n, preservando la modularidad.
- [ ] **Release 2: Framework Extensible Completo**¬†(Planificado)
	- [ ] **Sprint 2.1: **Construcci√≥n del N√∫cleo del Framework**¬†(Planificado)
		- **Objetivo:**¬†Implementar¬†core¬†(Orchestrator, Registry, etc.) y refactorizar componentes existentes a nuevas interfaces.
		- **Resultado Esperado:**¬†Simulaci√≥n del p√©ndulo ejecutada a trav√©s del nuevo framework central.
		- **Outcome:**¬†
	- [ ] **Sprint 2.2: Demostraci√≥n de Extensibilidad**¬†(Planificado)
		- **Objetivo:**¬†A√±adir nuevo sistema y agente v√≠a configuraci√≥n para validar flexibilidad.
		- **Resultado Esperado:**¬†Simulaci√≥n de nuevo sistema/agente funcionando bajo el framework sin cambios en el¬†core.
		- **Outcome:**

---
## Release 1

---
### Sprint 1.1

**Objetivo:** Validar que la implementaci√≥n actual produce resultados consistentes y esperados para la sintonizaci√≥n del PID del √°ngulo del p√©ndulo. Establecer una l√≠nea base fiable.

**Backlog:**
- [x] Implementaci√≥n de¬†PIDQLearningAgent¬†con tablas Q y contadores de visita basados en NumPy.
- [x] Creaci√≥n de¬†agent_state_manager¬†y l√≥gica de transformaci√≥n en el agente para guardar estado en JSON.
- [x] Refactorizaci√≥n de¬†main.py¬†para mejorar flujo, manejo de errores y guardado peri√≥dico.
- [x] Validaci√≥n de utilidades de procesamiento de datos
	- [x] summarize_episode
	- [x] save_summary_table
	- [x] visualization.
- [x] Informe de Estado Actual del Proyecto, validando la funcionalidad y el flujo de datos.
- [ ] Ejecuci√≥n de simulaciones e Informe de Resultados


**Historial:**
Dev:
[[20250324 - Sprint 1.1 - Escalamiento Tecnol√≥gico a MVP - Parte 1]]
[[20250401 - Sprint 1.1 - Escalamiento Tecnol√≥gico a MVP - Parte 2]]
Test:
[[20250325 - Validaci√≥n del Escalamiento tecnol√≥gico ]]
Prod:
[[20250326 - Informe de resultados]]  (en lienzo)


![[Sprint 1.1 - Actual System State.png]]

**Retrospectiva:**

- ‚úÖ**Qu√© fue bien:**¬†
	- La estructura general es comprensible y flexible.
	- La implementaci√≥n de NumPy mejor√≥ la eficiencia. 
	- El guardado de estados y de datos ha sido validado. 
	- La visualizaci√≥n de datos configurable junto con la simulaci√≥n.
- üõ†Ô∏è**Qu√© podr√≠a mejorar:**¬†
	- La separaci√≥n de responsabilidades entre¬†
		- main.py,¬†
		- PendulumEnvironment
		- PIDQLearningAgent.
- üß≠**Acciones:**¬†
	- Implementar controlador dual. (realizado pero se recomienda revalidar)
	- Inicializaci√≥n Aleatoria de Condiciones iniciales
	- Terminar deuda t√©cnica
		- Nombres de variables
		- Separar algunas responsabilidades
		- Extracci√≥n de Q-tables a utils
		- Optimizar y homologar gr√°ficos en app
	- Planificar refactorizaci√≥n expl√≠cita en Sprint 1.2. 
	- Ideas:
		- [[2 - 1 - 1 - Sprint 1.1 - Retrospective Results]]
		- Limitar al agente a solo las acciones disponibles
		- Dise√±o de la Recompensa
			- Funci√≥n de Costo con Ponderaci√≥n Din√°mica
			- Recompensa Intermedia por Sub-objetivos
				- Reducir el error de √°ngulo por debajo de un umbral.
				- Mantenerse dentro de una franja estable durante cierto n√∫mero de pasos.
		- Abordar el entrenamiento guiado mediante estrategias de exploraci√≥n/explotaci√≥n adaptativas.
			- Agente con modo explorador o explotador
			- Exploraci√≥n
				- Upper Confidence Bound
				- Exploraci√≥n Dirigida por la Recompensa
			- Explotaci√≥n
				- Œµ Decay adaptativa en funci√≥n al rendimiento del agente
			- Entrenamiento individualizado, combinado, finalizando con uno conjunto
		- Modos de agentes para el ajuste fino o grueso
		- Memoria de las pol√≠ticas (aunque ser√≠a optimizar la pol√≠tica)
			- Almacenar trayectorias exitosas y amplificar recompensas de las acciones que llevaron a un episodio exitoso

---
### Sprint 1.2

**Objetivo:** ...

**Backlog:**
- [x] Implementar modo `echo-baseline` con simulaciones contrafactuales y diferenciales por ganancia.
- [x] Implementar modo `shadow-baseline` con baseline local `B(s)` y m√©trica de estabilidad `w_stab`.
- [x] Refactorizar `PIDQLearningAgent` para aceptar recompensas por agente.
- [x] A√±adir soporte de `reward_mode` en `config.yaml` y l√≥gica correspondiente en `main.py`.
- [x] Validar `PendulumVirtualSimulator` para simulaciones de recompensa virtual.
- [x] Loggear `w_stab`, recompensas diferenciales y baseline para trazabilidad.

**Historial:**
Dev:
[[20250408 - Sprint 1.2 - Propuesta de Control de P√©ndulo Avanzado]]
[[20250410 - Sprint 1.2 - Informe de Escalamiento Parte 1 - Control de P√©ndulo Avanzado]]
[[20250411 - Sprint 1.2 - Mejoras al Proyecto Actual]]
[[20250414 - Sprint 1.2 - Informe de Escalamiento Parte 2 - Estado Actual del Proyecto]]
Test:
[[20250425 - Sprint 1.2 - Plataforma para la Evaluaci√≥n de Resultados]]
Prod:
[[20250410 - Informe Implementaci√≥n y Evaluaci√≥n Recompensas Diferenciadas]]




**Retrospectiva:**

- ‚úÖ **Qu√© fue bien:**
	- **Integraci√≥n exitosa de l√≥gica avanzada de recompensa** (`echo-baseline` y `shadow-baseline`), sin romper compatibilidad con el modo `global`.
		- Se adiciona un calculador de reward del tipo `gaussian` o del tipo `stability_calculator`
	- **PendulumVirtualSimulator** implementado correctamente, reutilizando el controlador y sistema sin afectar el entorno real.
	- **Separaci√≥n clara de responsabilidades** entre `main.py`, `PendulumEnvironment`, `PIDQLearningAgent` y `GaussianReward`, facilitando la trazabilidad de cada flujo de decisi√≥n y aprendizaje.
	- **Sistema robusto de m√©tricas y logging**, permitiendo an√°lisis detallado del comportamiento del agente y del sistema.
	- **Flexibilidad total desde el `config.yaml`**, lo que permitir√° experimentar f√°cilmente con distintos modos y par√°metros de recompensa.
- üõ†Ô∏è **Qu√© podr√≠a mejorar:**
	- **Costo computacional elevado en modo `echo-baseline`**
	- **Dificultad para interpretar el impacto individual de cada ganancia sin herramientas de visualizaci√≥n espec√≠ficas.**
	- **Dependencia fuerte de la configuraci√≥n correcta**
	- **Complejidad t√©cnica creciente**: la integraci√≥n de m√∫ltiples agentes y modos de recompensa complica la mantenibilidad sin una refactorizaci√≥n del n√∫cleo orientada a control multiagente.
- üß≠ **Acciones para los pr√≥ximos sprints:**
	- Desarrollar un **modo de entrenamiento dual (√°ngulo + posici√≥n)** con separaci√≥n clara de agentes.
	- Incorporar **herramientas de an√°lisis visual del impacto de cada acci√≥n** en la estabilidad.
	- **Optimizar el rendimiento del modo `echo-baseline`**, por ejemplo, paralelizando simulaciones virtuales o limitando su frecuencia.
	- Estudiar la integraci√≥n de una **estrategia de entrenamiento jer√°rquico o cooperativo entre agentes**.
	- Planificar una **refactorizaci√≥n hacia arquitectura orientada a componentes multi-agente**, manteniendo los principios actuales.



## Release 2

### Sprint 2.1

**Objetivo:** ...

**Backlog:**
- 

**Historial:**
Dev:

Test:

Prod:

### Sprint 2.2

**Objetivo:** ...

**Backlog:**
- 

**Historial:**
Dev:

Test:

Prod: